[ { "title": "RL Model Test - Deep Fake", "url": "/posts/Deep-Fake-Test/", "categories": "RL Model Test, Artificial Intelligence", "tags": "reinforcementlearning, deepfake, rl test", "date": "2022-03-23 20:00:00 +0900", "snippet": "Reference :Just record for test of a Model from Nicks VideoMy test clip :" }, { "title": "ML Model Test - Movenet Multiperson", "url": "/posts/Movenet-Test/", "categories": "ML Model Test, Artificial Intelligence", "tags": "reinforcementlearning, movenet, rl test", "date": "2022-02-22 16:00:00 +0900", "snippet": "Reference :Just record for test of a Model directly loaded from HubMy test clip :" }, { "title": "Review - Article Alphago Zero", "url": "/posts/Review-Alphago-Zero/", "categories": "Review, Artificial Intelligence", "tags": "reinforcementlearning, review, alphago", "date": "2022-02-07 16:00:00 +0900", "snippet": "Link :일단 읽으면서 생각 흐름대로 끄적이기부터Alphago이세돌을 이긴 첫번째 alphgo는 두 개의 deep neural network를 사용 Policy network : Outputs move probabilities Value network : predict the winner of games played by the policy networkTrain 후에는 Monte Carlo Tree Saerch (MCTS) 로 통합, 가능성이 높은 수들을 위주로 예측하고 이 예측된 수들의 승리가능성을 value network로 판단Alphago ZeroAlphago Zero는 각각 판후이와 이세돌을 이긴 Alphago Fan 과 Alphago Lee 와는 중요한 점에서 다름 Self-play reinforcement learning 으로만 학습하여 지도를 완전히 배제함 Domain 지식 등이 들어가는 feature 변수를 완전히 제외하였고 오로지 바둑판과 돌 위치로만 학습시킴 두개의 network를 사용하지 않고 하나의 nearal network만 사용함 MC rollout 없이 single neural network로만 수를 계산하는 더 간단한 tree search를 사용함 - Algorithm that incorporates lookahead search inside the training loopReinforcement learningDeep neural network $f_\\theta$ with parameters $\\theta$ Input : raw board representation $s$ of the position and its history Output : probabilities and a value $(p ,v) = f_\\theta(s)$ vector of move probabilities $p$ represents the probability of selecting each move $a$,$p_a = Pr(a\\vert s)$ neural network consists of residual blocks of convolutional layers with batch normalization and rectifier nonlinearities Train from self-play game In each position $s$, an MCTS search is executed, guided by neural network $f_\\theta$ MCTS search output probabilities $\\pi$ of playing each move. neural network parameters $\\theta$ are updated to maximize the similarity of the policy vector $p_t$ to the search probabilities $\\pi_t$ and to minimize the error between the predicted winner $v_t$ and the game winner $z$" }, { "title": "RLcourse note - Lecture 9 Exploration and Exploitation", "url": "/posts/RL-course-Note-9/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2022-01-14 19:00:00 +0900", "snippet": "Video Link :9강 소감Exploration과 Exploitation을 밸런스를 맞추면서 학습하도록하는 몇가지 기법에 대한 내용인데 별로 중요하진 않은것같다..? 솔직히 몇가지 배웠어도 큰 성능차이가 나는 부분도 아니고 몇 가지 환경적 상황 변수를 파악하고 거기에 맞게 설계한다면 이런 문제는 훨씬 효율적이고 적합하게 해결될 수 있다. 몇개 내용설명이 미흡하여 따로 공부해볼까했지만 중요하지 않아보여서 패스IntroductionExploration vs. Exploitation Dilemma Inline decision-making invloves a fundamental choice: Exploitation : Make the best decision given current information Exploration : Gather more information The best long-term strategy may involve short-term sacrifices Gather enough information to make the best overall decisionsPrinciples Naive Exploration Add noise to greedy policy (e.g. $\\epsilon$-greedy) Optimistic Initialisation Assume the best until proven otherwise Optimism in the Face of Uncertainty Prefer actions with uncertain values Probability Matching Select actions according to probability they are best Infromation State Search Lookahead search incorporating value of information Multi-Armed Bandits A multi-armed bandit is tuple $\\langle\\mathcal{A,R}\\rangle$ $\\mathcal{A}$ is a known set of m actions (or “arms”) $\\mathsf{\\mathcal{R}^a(r) = \\mathbb{P}[r\\vert a]}$ is an unknown probability distribution over rewards At each step t the agent selects an action $a_t \\in \\mathcal{A}$ The environment generates a reward $r_t \\sim \\mathcal{R}^{a_t}$ The goal is to maximise cumulative reward $\\sum^t_{\\tau = 1} r_\\tau$tuple에서 P를 빼놓고 내용에서는 정작 사용하고 있는 아이러니, state transition probability와 reward probability가 다를 수는 있지만 state에서 정의될 수 있는 부분이기도 하다. reward varation을 state에서 정의하지 못할 만한 경우가 있을까?Regret The action-value is the mean reward for action a,\\[\\mathsf{ Q(a) = \\mathbb{E} [r \\vert a] }\\] The optimal value $\\mathsf{V}^*$ is\\[\\mathsf{ V^* = Q(a^*) = \\underset{a\\in\\mathcal{A}}{max}\\; Q(a) }\\] The regret is the opportunity loss for one step\\[\\mathsf{ l_t = \\mathbb{E} [V^* - Q(a_t)] }\\] The total regret is the total opportunity loss\\[\\mathsf{ L_t = \\mathbb{E} \\left[ \\displaystyle\\sum^t_{\\tau = 1 } V^* - Q(a_\\tau) \\right] }\\] Maximise cumulative reward $\\equiv$ minimise total regretOptimal도 그냥 Q 쓰면되지 난데없이 왜 V야. mean action value는 따로 표시하면 되지 왜 똑같이 q를 쓰려고 하는가. 여기서 regret은 경우에 따라 minus인 경우도 생길 것이다. 근데 regret은 왜 Expected value인가. one step 이라면서 왜 expected야 짜증나게. Summation 해놓고 E 붙이는것도 마찬가지고Counting Regret The count $\\mathsf{N_t(a)}$ is expected number of elections for action a The gap $\\Delta _a$ is the difference in value between action a and optimal action $\\mathsf{ a^, \\Delta_a = V^ - Q(a) }$ Regret is a function of gaps and the counts\\[\\begin{aligned}\\mathsf{L_t}&amp;amp;= \\mathsf{ \\mathbb{E} \\left[ \\displaystyle\\sum^t_{\\tau=1} V^* - Q(a_\\tau) \\right] } \\\\&amp;amp;= \\mathsf{ \\displaystyle\\sum_{a\\in\\mathcal{A}} \\mathbb{E} [N_t(a)] (V^* - Q(a)) } \\\\&amp;amp;= \\mathsf{ \\displaystyle\\sum_{a\\in\\mathcal{A}} \\mathbb{E} [N_t(a)] \\Delta_a }\\end{aligned}\\] A good algorithm ensures small counts for large gaps Problem: gaps are not known!이미 Summation을 했는데 $\\mathbb{E}$는 왜 붙어있는건가…Linear or Sublinear Regret If an algorithm forever explores it will have linear total regret If an algorithm never explores it will have linear total regret Is it possible to achieve sublinear total regret?Greedy and $\\epsilon$-greedy algorithmsGreedy Algorithm We consider algorithms that estimate $\\mathsf{\\hat{Q}_t(a)\\approx Q(a)}$ Estimate the value of each action by Monte-Carlo evaluation\\[\\mathsf{ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\displaystyle\\sum^T_{t=1} r_t 1(a_t=a) }\\] The greedy algorithm selects action with highest value\\[\\mathsf{ a^*_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; \\hat{Q}_t(a) }\\] Greedy can lock onto a suboptimal action forever $\\Rightarrow$ Greedy has linear total regret$\\epsilon$-greedy Algorithm The $\\epsilon$-greedy algorithm continues to explore forever With probability $1-\\epsilon$ select $\\mathsf{a=\\underset{a\\in\\mathcal{A}}{argmax}\\;\\hat{Q}(a)}$ With probability $\\epsilon$ select a random action Constant $\\epsilon$ ensures minimum regret\\[\\mathsf{ l_t \\geq \\frac{\\epsilon}{\\mathcal{A}} \\displaystyle\\sum_{a\\in\\mathcal{A}}\\Delta_a }\\] $\\Rightarrow$ $\\epsilon$-greedy has linear total regretOptimistic Initialisation Simple and practical idea: initialise $\\mathsf{Q(a)}$ to high value Update action value by incremental Monte-Carlo evaluation Starting with $\\mathsf{N(a) &amp;gt; 0}$\\[\\mathsf{ \\hat{Q}_t(a_t) = \\hat{Q}_{t-1} + \\frac{1}{N_t(a_t)}(r_t - \\hat{Q}_{t-1}) }\\] Encourages systematic exploration early on But can still lock onto suboptimal action $\\Rightarrow$ greedy + optimistic initialisation has linear total regret $\\Rightarrow$ $\\epsilon$-greedy + optimistic initialisation has linear total regret초기 Value를 높게 설정해두고 시작함으로써 Exploration이 저절로 일어나지만 여전히 suboptimal 수렴은 가능하다. 간단하지만 꽤 괜찮은 접근법Decating $\\epsilon_t$-Greedy Algorithm Pick a decay schedule for $\\epsilon_1, \\epsilon_2, \\dots$ Consider the following schedule\\[\\begin{aligned}\\mathsf{ c } &amp;amp;&amp;gt; \\mathsf{ 0 } \\\\\\mathsf{ d } &amp;amp;= \\mathsf{ \\underset{a\\vert\\Delta_a&amp;gt;0}{min}\\; \\Delta_i } \\\\\\mathsf{ \\epsilon_t } &amp;amp;= \\mathsf{ min \\lbrace 1, \\frac{c\\vert \\mathcal{A}\\vert}{d^2t} \\rbrace }\\end{aligned}\\] Decaying $\\epsilon_t$-greedy has logarithmic asymptotic total regret! Unfortunately, schedule requires advance knowledge of gaps Goal: find an algorithm with sublinear regret for any multi-armed bandit (without knowledge of $\\mathcal{R}$)gap notation 또 엉망이다 action 끼리의 regret 차이라고 한다. 그리고 regret은 실제로 알수 없는 값이므로 이론적인 내용일뿐 아직 의미는 없다. 그냥 exploration이 decay해서 regret이 logarithmic 해지고 기회비용이 줄었을 뿐Lower Bound The performance of any algorithm is determined by similarity between optimal arm and other arms Hard problems have similar-looking arms with different means This is described formally by the gap $\\Delta_a$ and the similarity in distributions $\\mathsf{KL(\\mathcal{R}^a\\Vert\\mathcal{R}^a *)}$ Theorem (Lai and Robbins Asymptotic total regret is at least logarithmic in number of steps$$ \\mathsf{ \\underset{t\\rightarrow\\infty}{\\lim}\\; L_t \\leq \\log\\, t \\displaystyle\\sum_{a\\vert\\Delta_a&amp;gt;0} \\frac{\\Delta_a}{KL(\\mathcal{R}^a\\Vert \\mathcal{R}^{a^*})} } $$ 흠 상황과 모델을 만들기에 따라 극복 가능할것 같은 느낌도 있지만 굳이. 근데 이걸 왜 하는지 나오려나?Upper Confidence BoundOptimism in the Face of Uncertainty Which action should we pick? The more uncertain we are about an action-value The more important it is to eplore that action It could turn out to be the best action After picking blue action We are less uncertain about the value And more likely to pick another action Until we home in on best action흠 충분히 data를 수집한 뒤에도 1번 그래프가 유지된다면 그래도 1번을 고를 것인가Upper Confidence Bounds Estimate an upper confidence $\\mathsf{\\hat{U}_t(a)}$ for each action value Such that $\\mathsf{Q(a)\\leq\\hat{Q}_t(a)+\\hat{U}_t(a)}$ with high probability This depends on the number of times $\\mathsf{N(a)}$ has been selected Small $\\mathsf{N_t(a) \\Rightarrow}$ large $\\mathsf{\\hat{U}_t(a)}$ (estimated value is uncertain) Large $\\mathsf{N_t(a) \\Rightarrow}$ small $\\mathsf{\\hat{U}_t(a)}$ (estimated value is accurate) Select action maximising Upper Confidence Bound (UCB)\\[\\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; \\hat{Q}_t(a) + \\hat{U}_t(a) }\\]평균적인 reward 기대값이 큰 action이 아니라불확실성이 있는 action을 선택할 가능성을 높이기 위한 방법으로써 U를 사용하는 것.Hoeffding’s Inequality Theorem (Hoeffding’s Inequality) Let \\(\\mathsf{X_1,\\dots ,X_t}\\) be i.i.d. random variables in [0,1], and let \\(\\mathsf{ \\bar{X}_t = \\frac{1}{t}} \\sum^t_{\\tau=1} X_\\tau }\\) be the sample mean. Then$$\\mathsf{ \\mathbb{P}[\\mathbb{E}[X]&amp;gt;\\bar{X}_t+u]\\leq e^{-2tu^2} }$$ We will apply Hoeffding’s Inequality to rewards of the bandit conditioned on selecting action a\\[\\mathsf{ \\mathbb{P} \\left[ Q(a) &amp;gt; \\hat{Q}_t(a) + U_t(a) \\right] \\leq e^{-2N_t(a)U_t(a)^2} }\\]Calculating Upper Confidence Bounds Pick a probability p that true value exceeds UCB Now solve for $\\mathsf{U_t(a)}$\\[\\begin{aligned}\\mathsf{ e^{-2N_t(a)U_t(a)^2} } &amp;amp;= \\mathsf{ p } \\\\\\mathsf{ U_t(a) } &amp;amp;= \\mathsf{ \\sqrt{\\frac{-\\log p}{2N_t(a)}} }\\end{aligned}\\] Reduce p as we observe more rewards, e.g. $\\mathsf{p=t^{-4}}$ Ensures we select optimal action as $\\mathsf{t\\rightarrow \\infty}$\\[\\mathsf{ U_t(a) = \\sqrt{\\frac{-\\log p}{2N_t(a)}} }\\]UCB1 This leads to the UCB1 algorithm\\[\\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(a) + \\sqrt{\\frac{2\\log t}{N_t(a)}} }\\] Theorem The UCB algorithm achieves logarithmic asymtotic total regret$$\\mathsf{ \\displaystyle\\lim_{t\\rightarrow\\infty} L_t \\leq 8\\log t \\displaystyle\\sum_{a\\vert\\Delta_a&amp;gt;0} \\Delta_a }$$ UCB중 확률이 decay하는 \\(t^{-4}\\)를 사용하는 알고리즘Bayesisan Bandits So far we have made no assumptions about the reward distribution $\\mathcal{R}$ Except bounds on rewards Bayesian bandits exploit prior knowledge of rewards, $\\mathsf{p}[\\mathcal{R}]$ They compute posterior distribution of reward $\\mathsf{p[\\mathcal{R}\\vert h_t]}$ where $\\mathsf{h_t=a_1,r_1,\\dots a_{t-1},r_{t-1}}$ is the history Use posterior to guide exploration Upper confidence bounds (Bayesian UCB) Probability matching (Thompson sampling) Better performance if prior knowledge is accurateBayesian UCB Example: Independent Gaussians Assume reward distribution is Gaussian, $\\mathsf{\\mathcal{R}_a(r) = \\mathcal{N}(r;\\mu_a,\\sigma^2_a)}$ Compute Gaussian posterior over $\\mu_a$ and $\\sigma_a^2$ (by Bayes law)\\[\\mathsf{ p[\\mu_a, \\sigma^2_a \\vert h_t] \\propto p[\\mu_a,\\sigma^2_a] \\displaystyle\\prod_{t\\vert a_t=a} \\mathcal{N}(r_t; \\mu_a,\\sigma^2_a) }\\] Pick action that maximises standard deviation of $\\mathsf{Q(a)}$\\[\\mathsf{ a_t = argmax \\;\\mu_a + c\\sigma_a/\\sqrt{N(a)} }\\]Probability Matching Probability matching selects action a according to probability that a is the optimal action\\[\\mathsf{ \\pi(a\\vert h_t) = \\mathbb{P} [Q(a) &amp;gt; Q(a&#39;), \\forall a&#39; \\neq a \\vert h_t] }\\] Probability matching is optimistic in the face of uncertainty Uncertain actions have higher probability of being max Can be difficult to compute anlytically from posteriorThompson Sampling Thompson sampling implements probability matching\\[\\begin{aligned}\\mathsf{ \\pi(a\\vert h_t) } &amp;amp;= \\mathsf{ \\mathbb{P} [ Q(a) &amp;gt; Q(a&#39;), \\forall a&#39; \\neq a \\vert h_t ] } \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\mathcal{R}\\vert h_t} \\left[ 1(a=\\underset{a\\in\\mathcal{A}}{argmax}\\; Q(a))\\right] }\\end{aligned}\\] Use Bayes law to compute posterior distribution $\\mathsf{p[\\mathcal{R}\\vert h_t]}$ Sample a reward distribution $\\mathcal{R}$ from posterior Compute action-value function $\\mathsf{Q(a) = \\mathbb{E}[\\mathcal{R}_a]}$ Select action maximising value on ssample, $\\mathsf{a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(a)}$ Thompson sampling achieves Lai and Robbins lower bound!Value of Information Exploration is useful because it gains information Can we quantify the value of information? How much reward a decision-maker would be prepared to pay in order to have that information, prior to making a decision Long-term reward after getting information - immediate reward Information gain is higher in uncertain situations Therefor it makes sense to explore uncertain situations more If we know value of information, we can trade-off exploration and exploitation optimallyexploration은 거의 random하게 시행되지만 좀더 smart, logically efficient한 방법이 있지않을까, 여기서는 uncertatinty만 쫓아서 모든 tree를 검증하는 방향으로만 하고 있다.Information State Search We have viewed bandits as one-step decision-making problems Can also view as sequential decision-making problems At each step there is an information state $\\mathsf{\\tilde{s}}$ $\\mathsf{\\tilde{s}}$ is a statistic of the history, $\\mathsf{\\tilde{s_t} = f(h_t)}$ summarising all information accumulated so far Each action a causes a transition to a new information state $\\mathsf{\\tilde{s}’}$ (by adding information), with probability $\\mathsf{\\mathcal{\\tilde{P}}^a_{\\tilde{s},\\tilde{s}’}}$ This defines MDP $\\mathcal{\\tilde{M}}$ in augmented information state space\\[\\mathcal{ \\tilde{M} = \\langle \\tilde{S}, A, \\tilde{P}, R, \\gamma\\rangle }\\]Example: Bernoulli Bandits Consider a Bernoulli bandit, such that $\\mathsf{\\mathcal{R}^a = \\mathcal{B}(\\mu_a)}$ e.g. Win or lose a game with probability $\\mu_a$ Want to find which arm has the highest $\\mu_a$ The information state is $\\mathsf{\\tilde{s}} = \\langle\\alpha,\\beta\\rangle$ $\\alpha_a$ counts the pulls of arm a where reward was 0 $\\beta_a$ counts the pulls of arm a where reward was 1 Solving Information State Space Bandits We now have an infinite MDP over information states This MDP can be solved by reinforcement learning Model-free reinforcement learning e.g. Q-learning (Duff, 1994) Bayesian model-based reinforcement learning e.g. Gittins indices (Gittins, 1979) This approach is known as Bayes-adaptive RL Finds Bayes-optimal exploration/exploitation trade-off with respect to prior distribution Bayes-Adaptive Bernoulli Bandits Start with Beta$(\\alpha_a, \\beta_a)$ prior over reward function $\\mathcal{R}^a$ Each time a is selected, update posterior for \\(\\mathcal{R}^a\\) Beta($\\alpha_a + 1 ,\\beta_a$) if $r= 0$ Beta(\\(\\alpha_a,\\beta_a+1\\)) if \\(r=1\\) This defines transition function \\(\\mathcal{\\tilde{P}}\\) for the Bayes-adaptive MDP Information state \\(\\langle\\alpha,\\beta\\rangle\\) corresponds to reward model Beta(\\(\\alpha,\\beta\\)) Each state transition corresponds to a Bayesian model updateGittins Indices for Bercoulli Bandits Bayes-adaptive MDP can be solved by dynamic programming The solution is known as the Gittins index Exact solution to Bayes-adaptive MDP is typically intractable Information state space is too large Recent idea: apply simulation-based search (Guez et al. 2012) Forward search in information state space Using simulations from current information state Contextual Bandits A contextual bandit is a tuple \\(\\langle\\mathcal{A,S,R}\\rangle\\) \\(\\mathcal{A}\\) is a known set of actions (or “arms”) \\(\\mathcal{S= \\mathbb{P}}\\mathsf{[s]}\\) is an unknown distribution over states (or “contexts”) \\(\\mathsf{\\mathcal{R}^a_s(r) = \\mathbb{P}[r\\vert s,a]}\\) is an unknown probability distribution over rewards At each step t Environmnet generates state \\(s_t\\sim \\mathcal{S}\\) Agent selects action \\(\\mathsf{a_t}\\in\\mathcal{A}\\) Environment generates reward \\(\\mathsf{r_t}\\sim\\mathcal{R}^{a_t}_{s_t}\\) Goal is to maximise cumulative reward \\(\\mathsf{\\sum^t_{\\tau=1} r_\\tau}\\)Linear UCBLinear Regression Action-value function is expected reward for state s and action a\\[\\mathsf{Q(s,a) = \\mathbb{E}[r\\vert s,a]}\\] Estimate value function with a linear function approximator\\[\\mathsf{ Q_\\theta(s,a) = \\phi (s,a)^\\top \\theta \\approx Q(s,a) }\\] Estimate parameters by least squares regression\\[\\begin{aligned}\\mathsf{ A_t } &amp;amp;= \\mathsf{ \\displaystyle\\sum^t_{\\tau=1} \\phi(s_\\tau , a_\\tau) \\phi(s_\\tau, a_\\tau)^\\top } \\\\\\mathsf{ b_t } &amp;amp;= \\mathsf{ \\displaystyle\\sum^t_{\\tau=1} \\phi(s_\\tau,a_\\tau)r_\\tau } \\\\\\mathsf{ \\theta_t } &amp;amp;= \\mathsf{ A_t^{-1} b_t }\\end{aligned}\\]Linear Upper Confidence Bounds Least squares regression estimates the mean action-value \\(\\mathsf{Q_\\theta(s,a)}\\) But it can also estimate the variance of the action-value \\(\\mathsf{\\sigma^2_\\theta(s,a)}\\) i.e. the uncertainty due to parameter estimation error Add on a bonus for uncertainty, \\(\\mathsf{U_\\theta(s,a) = c\\sigma}\\) i.e. define UCB to be c standard deviations above the meanGeometric Interpretation Define confidence ellipsoid \\(\\mathcal{E}_t\\) around parameters \\(\\theta_t\\) Such that \\(\\mathcal{E}_t\\) includes true parameters \\(\\theta^*\\) with high probability Use this ellipsoid to estimate the uncertainty of action values Pick parameters within ellipsoid that maximise action value\\[\\mathsf{\\underset{\\theta\\in\\mathcal{E}}{argmax}\\; Q_\\theta(s,a)}\\]Calculating Linear Upper Confdence Bounds For least quares regression, parameter covariance is \\(\\mathsf{A^{-1}}\\) Action-value is linear in features, \\(\\mathsf{Q_\\theta(s,a) = \\phi (s,a)^\\top \\theta}\\) So action-value variance is quadratic, \\(\\mathsf{\\sigma^2_\\theta(s,a) = \\phi(s,a)^\\top A^{-1} \\phi(s,a)}\\) Upper confidence bound is \\(\\mathsf{Q_\\theta(s_t,a) + c\\sqrt{\\phi(s,a)^\\top A^{-1} \\phi(s,a)}}\\) Select action maximising upper confidence bound\\[\\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q_\\theta(s_t,a)+ c\\sqrt{\\phi(s_t,a)^\\top A^{-1}_t \\phi(s_t,a)}}\\]MDPsExploration/Exploitation Principles to MDPSThe same principles for exploration/exploitation apply to MDPs Naive Exploration Optimistic Initialisation Optimism in the Face of Uncertainty Probability Matching Information State SearchOptimistic InitialisationsOptimistic Initialisation: Model-Free RL Initialise action-value function Q(s,a) to \\(\\frac{r_{max}}{1-\\gamma}\\) Run favourite model-free RL algorithm Monte-Carlo control Sarsa Q-learning … Encourages systematic exploration of states and actionsOptimistic Initialisation: Model-Based RL Construct an optimistic model of the MDP Initialise transitions to go to heaven (i.e. transition to terminal state with \\(r_{max}\\) reward) Solve optimistic MDP by favourite planning algorithm policy iteration value iteration tree search … Encourages systematic exploration of states and actions e.g. RMax algorithm (Brafman and Tennenholtz)Optimism in the Face of UncertaintyUpper Confidence Bounds: Model-Free RL Maximise UCB on action-value function \\(\\mathsf{Q^\\pi (s,a)}\\)$$\\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(s_t,a) + U(s_t,a) }$$ Estimate uncertainty in policy evaluation (easy) Ignores uncertainty from policy improvement Maximise UCB on optimal action-value function \\(\\mathsf{Q^*(s,a)}\\)$$\\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(s_t,a) + U_1(s_t,a) + U_2(s_t,a) }$$ Estimate uncertainty in policy evaluation (easy) plus uncertainty from policy improvement (hard) Bayesian Model-Based RL Maintain posterior distribution over MDP models Estimate both transitions and rewards, \\(\\mathsf{p[\\mathcal{P,R}\\vert h_t]}\\) where \\(\\mathsf{h_t = s_1,a_1,r_2,\\dots ,s_t}\\) is the history Use posterior to guide exploration Upper confidence bounds (Bayesian UCB) Probability matching (Thompson sampling) Probability MatchingThompson Sampling: Model-Based RL Thompson sampling implements probability matching\\[\\begin{aligned}\\mathsf{ \\pi(s,a,\\vert h_t)} &amp;amp;= \\mathsf{ \\mathbb{P} [Q^*(s,a) &amp;gt; Q^*(s,a&#39;), \\forall a&#39; \\neq a \\vert h_t] } \\\\ &amp;amp;= \\mathsf{ \\mathbb{E}_{\\mathcal{P,R}\\vert h_t} \\left[ 1(a=\\underset{a\\in\\mathcal{A}}{argmax}\\; Q^*(s,a))\\right] }\\end{aligned}\\] Use Bayes law to compute posterior distribution \\(\\mathsf{p[\\mathcal{P,R}\\vert h_t]}\\) Sample an MDP \\(\\mathcal{P,R}\\) from posterior Solve MDP using favourite planning algorithm to get \\(\\mathsf{Q^*(s,a)}\\) Select optimal action for sample MDP, \\(\\mathsf{a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q^*(s_t,a)}\\)Information State SearchInformation State Search in MDPs MDPs can be augmented to include infromation state Now the augmented state is \\(\\langle s,\\tilde{s}\\rangle\\) where s is original state within MDP and \\(\\tilde{s}\\) is a statistic of the history (accumulated information) Each action a causes a transition to a new state s’ with probability \\(\\mathcal{P}^a_{s,s&#39;}\\) to a new information state \\(\\tilde{s}&#39;\\) Defines MDP \\(\\mathcal{\\tilde{M}}\\) in augmented information state space\\[\\mathcal{\\tilde{M} = \\langle \\tilde{S},A,\\tilde{P},R,\\gamma\\rangle}\\]Bayes Adaptive MDPs Posterior distribution over MDP model is an information state\\[\\mathsf{\\tilde{s}_t = \\mathbb{P}[\\mathcal{P,R}\\vert h_t]}\\] Augmented MDP over \\(\\mathsf{\\langle s, \\tilde{s}\\rangle}\\) is called Bayes-adaptive MDP Solve this MDP to find optimal exploration/exploitation trade-off (with respect to prior) However, Bayes-adaptive MDP is typically enormous Simulation-based search has proven effective (Guez et al.)Conclusion Have covered several principles for exploration/exploitation Naive methods such as $\\epsilon$-greedy Optimistic initialisation Upper confidence bounds Probability matching Information state search Each principle was developed in bandit setting But same principles also apply to MDP setting" }, { "title": "RLcourse note - Lecture 8 Integrating Learning and Planning", "url": "/posts/RL-course-Note-8/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2022-01-04 18:00:00 +0900", "snippet": "Video Link :8강 소감Tree Search 의 장점은 알 수 없는 Model을 가정하여 거기에서 현재까지 학습된 policy에 따라 여러 시나리오들을 진행해보고 그를 바탕으로 Value를 빠르게 계산해볼 수 있다는 것이다. 강의에서는 Model로부터 Sample Experience를 확보하여 빠른 학습이 가능하다는 것을 중점으로 두고 설명하고 있으나, 사실 이 부분은 논리적으로 빠른 학습은 어려워 보인다. 특히 모델의 오차를 더 강화할 수도 있다는 부분에서 그렇다. 그러나 Tree Search 과정은 상당히 유의미한데, 이 방법을 통해 실제 Policy를 기반으로 유의미한 미래 예측과 그 Return 예측을 통해 Action을 선택할 수 있게 함으로써 무가치한 미래 예측에 할당될 자원을 절약할 수 있고 보다 빠르게 실시간으로 Model을 완벽히 모르는 상태에서도 최선의 선택을 할 수 있다는 점이다. 이 구조는 실제 인간 등이 선택을 하는 사고 과정과도 유사하지 않을까?IntroductionLearn model directly from experience, and use planning to construct a value function or policy. Integrate learning and planning into a single architecturemodel을 경험으로부터 학습한다고 하는데, 이게 제일 중요한 부분인거 같은데 잘 다뤄져있지 않다. 이 실제 environment에 근접하는 model을 결정하는 부분이야말로 RL의 학습을 결정하는 가장 중요한 부분이 아닐까?Model-Based and Model-Free RL Model-Free RL No model Learn value function (and/or policy) from experience Model-Based RL Learn a model from experience Plan value function (and/or policy) from model Model-Based Reinforcement LearningAdvantages: Can efficiently learn model by supervised learning methods Can reason about model uncertaintyDisadvantages: First learn a model, then construct a value function$\\Rightarrow$ two sources of approximation errorModel A model $\\mathcal{M}$ is a representation of an MDP $\\langle\\mathcal{S,A,P,R}\\rangle$ parametrized by $\\eta$ We will assume state space $\\mathcal{S}$ and action space $\\mathcal{A}$ are known So a model $\\mathcal{M=\\langle P_\\eta ,R_\\eta \\rangle}$ represents state transitions $\\mathcal{P_\\eta \\approx P}$ and rewards $\\mathcal{R_\\eta \\approx R}$\\[\\begin{aligned}\\mathsf{ S_{t+1} } &amp;amp; \\sim \\mathsf{ \\mathcal{P}_\\eta(S_{t+1}\\;\\vert\\;S_t, A_t) } \\\\\\mathsf{ R_{t+1} } &amp;amp;= \\mathsf{ \\mathcal{R}_\\eta(R_{t+1}\\;\\vert\\;S_t, A_t) }\\end{aligned}\\] Typically assume conditional independence between state transitions and rewards\\[\\mathsf{ \\mathbb{P}[S_{t+1}, R_{t+1}\\;\\vert\\;S_t, A_t] = \\mathbb{P}[S_{t+1}\\;\\vert\\;S_t, A_t]\\mathbb{P}[R_{t+1}\\;\\vert\\;S_t, A_t] }\\]경험으로부터 Model의 P, R을 기록하고 이를 통해 비슷한 결과를 내는 모델을 만든다. 즉 모델을 확률적인 state 변화와 Reward로 준다는 것이다. 이보다 중요한 구성 요소가 충분히 있을 것 같은데 전부 필드 개발자의 직관에 맡기겠다고 말하는 것 같다. State와 Action, Probability와 Reward를 잘 구성하기 어려울 것 같은데. 단순한 문제 또는 Model을 완벽하게 알고 있는 문제가 아니고서야.또한 Action의 결과로 어느 State로 전이되는지를 은근슬쩍 무시하기 시작했다. 어쩐지 지난 강의에서 Reward중 next state 부분을 없애버리더라. 이렇게 은근슬쩍 하나씩 맘대로 하는게 쌓이고 쌓인다. 물론 그 모든 조건을 만족하도록 Model을 만들어낼 수 있을지도 모른다. AI가 점점 어려워지겠지Model Learning Goal: estimate model $\\mathcal{M}_\\eta$ from experience $\\lbrace \\mathsf{ S_1, A_1, R_2, \\dots, S_T } \\rbrace $ This is a supervised learning problem\\[\\begin{aligned}\\mathsf{ S_1, A_1 } &amp;amp;\\rightarrow \\mathsf{ R_2,S_2 } \\\\\\mathsf{ S_2, A_2 } &amp;amp;\\rightarrow \\mathsf{ R_3,S_3 } \\\\&amp;amp;\\vdots\\\\\\mathsf{ S_{T-1}, A_{T-1} } &amp;amp;\\rightarrow \\mathsf{ R_T,S_T }\\end{aligned}\\] Learning $\\mathsf{s,a,\\rightarrow r}$ is a regression problem Learning $\\mathsf{s,a,\\rightarrow s’}$ is a density estimation problem Pick loss function, e.g. mean-squared error, KL divergence, $\\dots$ Find parameters $\\eta$ that minimise empirical lossReward가 항상 Regression으로 결정될 수 있는 건 아닐 것 같은데, 전이 되는 state 에 따라 Reward가 바뀐다면 s,a 만 가지고는 결정할 수 없다.Examples of Models Table Lookup Model Linear Expectation Model Linear Gaussian Model Gaussian Process Model Deep Belief Network Model …Table Lookup Model Model is an explicit MDP, $\\mathcal{\\hat{P}, \\hat{R}}$ Count visits $\\mathsf{N(s,a)}$ to each state action pair\\[\\begin{aligned}\\mathsf{ \\mathcal{\\hat{P}}^a_{s,s&#39;} } &amp;amp;= \\mathsf{ \\frac{1}{N(s,a)} \\displaystyle\\sum^T_{t=1} 1(S_t,A_t,S_{t+1} = s,a,s&#39;) } \\\\\\mathsf{ \\mathcal{\\hat{R}}^a_s } &amp;amp;= \\mathsf{ \\frac{1}{N(s,a)} \\displaystyle\\sum^T_{t=1} 1(S_t,A_t = s,a)R_t }\\end{aligned}\\] Alternatively At each time-step t, record experience tuple\\(\\mathsf{ \\langle S_t, A_t, R_{t+1}, S_{t+1} \\rangle }\\) To sample, model, randomly pick tuple matching $ \\mathsf{ \\langle s,a, \\cdot, \\cdot \\rangle } $ Planning with a Model Given a model $\\mathcal{M_\\eta = \\langle P_\\eta, R_\\eta \\rangle}$ Solve the MDP $\\langle \\mathcal{S,A,P_\\eta, R_\\eta} \\rangle$ Using favourite planning algorithm Value iteration Policy iteration Tree search … Sample-Based Planning A simple but powerful approach to planning Use the model only to generate samples Sample experience from model\\[\\begin{aligned}\\mathsf{ S_{t+1} } &amp;amp; \\sim \\mathsf{ \\mathcal{P}_\\eta(S_{t+1}\\;\\vert\\;S_t, A_t) } \\\\\\mathsf{ R_{t+1} } &amp;amp;= \\mathsf{ \\mathcal{R}_\\eta(R_{t+1}\\;\\vert\\;S_t, A_t) }\\end{aligned}\\] Apply model-free RL to samples, e.g.: Monte-Carlo control Sarsa Q-learning Sample-based planning methods are often more efficientPlanning with an Inaccurate Model Given an imperfect model $\\langle \\mathcal{P_\\eta, R_\\eta} \\rangle \\neq \\langle \\mathcal{P, R} \\rangle$ Performance of model-based RL is limited to optimal policy for approximate MDP $\\langle \\mathcal{S,A,P_\\eta, R_\\eta} \\rangle$ i.e. Model-based RL is only as good as the estimated model When the model is inaccurate, planning process will compute a suboptimal policy Solution 1: when model is wrong, use model-free RL Solution 2: reason explicitly about model uncertainty이런 사례가 많이 생길것 같은데 그 이유는 여태까지 쌓아온 가정이 너무 많으면서 굳이 필요없어보이는 부분까지도 가정하기 때문이라는 느낌Integrated ArchitecturesDynaReal and Simulated ExperienceWe consider two sources of experienceReal experience - Sample from envrionment (True MDP)\\[\\begin{aligned}\\mathsf{ S&#39; } &amp;amp; \\sim \\mathsf{ \\mathcal{P}^a_{s,s&#39;} } \\\\\\mathsf{ R } &amp;amp;= \\mathsf{ \\mathcal{R}^a_s }\\end{aligned}\\]Simulated experience - Sampled from model (approximate MDP)\\[\\begin{aligned}\\mathsf{ S&#39; } &amp;amp; \\sim \\mathsf{ \\mathcal{P}_\\eta(S&#39;\\;\\vert\\;S, A) } \\\\\\mathsf{ R } &amp;amp;= \\mathsf{ \\mathcal{R}_\\eta(R\\;\\vert\\;S, A) }\\end{aligned}\\]Integrating Learning and Planning Model-Free RL No model Learn value funciton (and/or policy) from real experience Model-Based RL (using Sample-Based Planning) Learn a model from real experience Plan value function (and/or policy) from simulated experience Dyna Learn a model from real experience Learn and plan value function (and/or policy) from real and simlated experience Dyna-Q Algorithm Initialize Q(s,a) and Model(s,a) for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$Do forever:$\\quad$ (a) $S\\leftarrow$ current (nonterminal) state$\\quad$ (b) $A\\leftarrow\\epsilon$-greedy (S,A) $\\quad$ (c) Execute action A; observe resultant reward, R, and state, S’$\\quad$ (d) $Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma\\; max_a Q(S’,a)-Q(S,A)]$$\\quad$ (e) $Model(S,A)\\leftarrow R,S’$ (assuming deterministic environment)$\\quad$ (f) Repeat n times: $\\quad\\quad$ $S\\leftarrow$ random previously observed state $\\quad\\quad$ $A\\leftarrow$ random action previously taken in S $\\quad\\quad$ $R,S’\\leftarrow Model(S,A)$ $\\quad\\quad$ $Q(S,A)\\leftarrow Q(S,A) + \\alpha [R + \\gamma max_a Q(S’,a)-Q(S,A)]$ Model에서 추가적인 exeprience를 얻어 Value를 학습시키는 것은 적은 episode로도 빠르게 Value를 학습시킬 수 있을 지 모르나, Model의 정확성은 Real Experience가 충분히 많은 수로 확보되고 Model이 실제 Environment에 가까울 때에만 믿을 수 있다. 때로 초반에 잘못된 Model을 구성하도록 유도할 수도 있으며 장기적으로도 크게 유의미할것 같지는 않다.Simulation-Based SearchForward Search Forward search algorithms select the best action by lookahead They build a search tree with the current state $s_t$ at the root Using a model of the MDP to look ahead No need to solve whole MDP, just sub-MDP starting from nowSimulation-Based Search Forward search paradigm using sample-based planning Simulate episodes of experience from now with the model Apply model-free RL to simulated episodes Simulate episodes of experience from now with the model\\[\\mathsf{ \\lbrace s^k_t, A^k_t, R^k_{t+1}, \\dots, S^k_T \\rbrace ^K_{k=1} \\sim \\mathcal{M}_v }\\] Apply model-free RL to simulated episodes Monte-Carlo control $\\rightarrow$ Monte-Carlo search Sarsa $\\rightarrow$ TD search Monte-Carlo SearchSimple Monter-Carlo Search Given a model $\\mathcal{M}_v$ and a simulation policy $\\pi$ For each action $\\mathsf{a}\\in\\mathcal{A}$ Simulate K episodes from current (real) state $\\mathsf{s_t}$$$ \\mathsf{ \\lbrace s_t, a, R^k_{t+1}, S^k_{t+1}, A^k_{t+1} \\dots, S^k_T \\rbrace ^K_{k=1} \\sim \\mathcal{M}_v,\\pi } $$ Evaluate actions by mean return (Monte-Carlo evaluation)$$ \\mathsf{ Q(s_t,a) = \\frac{1}{K} \\displaystyle\\sum^K_{k=1} G_t \\overset{P}{\\rightarrow} q_\\pi(s_t,a) } $$ Select current (real) action with maximum value$$ \\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(s_t,a) } $$Monte-Carlo Tree Search (Evaluation) Given a model $\\mathcal{M}_v$ Simulate K episodes from current state $\\mathsf{s_t}$ using current simulation policy $\\pi$$$ \\mathsf{ \\lbrace s_t, A^k_{t+1}, R^k_{t+1}, S^k_{t+1}, \\dots, S^k_T \\rbrace ^K_{k=1} \\sim \\mathcal{M}_v,\\pi } $$ Build a search tree containing visited states and actions Evaluate states Q(s,a) by mean return of episodes from s, a$$ \\mathsf{ Q(s,a) = \\frac{1}{N(s,a)} \\displaystyle\\sum^K_{k=1}\\sum^T_{u=t} 1(S_u, A_u = s,a)G_u \\overset{P}{\\rightarrow} q_\\pi(s_t,a) } $$ After search is finished, select current (real) action with maximum value in search tree$$ \\mathsf{ a_t = \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(s_t,a) } $$Monte-Carlo Tree Search (Simulation) MCTS, the simulation policy $\\pi$ improves Each simulation consists of two phases (in-tree, out-of-tree) Tree policy (improves): pick actions to maximise Q(S,A) Default policy (fixed): pick actions randomly Repeat (each simulation) Evaluate states Q(S,A) by Monte-Carlo evaluation Improve tree policy, e.g. by $\\epsilon$-greedy(Q) Monte-Carlo control applied to simulated experience Converges on the optimal search tree, $\\mathsf{ Q(S,A) \\rightarrow q_*(S,A)}$Position Evaluation in Go How good is a position s? Reward function (undiscounted):\\[\\begin{aligned}\\mathsf{ R_t } &amp;amp;= \\mathsf{ 0 \\text{ for all non-terminal steps } t&amp;lt;T } \\\\\\mathsf{ R_T } &amp;amp;= \\begin{cases}1 &amp;amp; \\text{ if Balck wins } \\\\0 &amp;amp; \\text{ if White wins }\\end{cases}\\end{aligned}\\] Policy $\\mathsf{\\pi = \\langle\\pi_B , \\pi_W\\rangle}$ selects moves for both players Value function (how good is position s):\\[\\begin{aligned}\\mathsf{v_\\pi(s)} &amp;amp;= \\mathsf{ \\mathbb{E}_\\pi [R_T \\vert S=s] = \\mathbb{P} [\\text{Black wins } \\vert S=s] } \\\\\\mathsf{v_*(s)} &amp;amp;= \\mathsf{ \\underset{\\pi_B}{max}\\; \\underset{\\pi_W}{min}\\; v_\\pi(s) }\\end{aligned}\\]Advantages of MC Tree Search Highly selective best-first search Evaluates states dynamically (unlike e.g. DP) Uses sampling to break curse of dimensionality Works for “black-box” models (only requires samples) Computationally efficient, anytime, parallelisableTemporal-Difference Search Simulation-based search Using TD instead of MC (bootstrapping) MC tree search applies MC control to sub-MDP from now TD search applies Sarsa to sub-MDP from nowMC vs. TD search For model-free reinforcement learning, bootstrapping is helpful TD learning reduces variance but increases bias TD learning is usually more efficient than MC TD($\\lambda$) can be much more efficient than MC For simulation-based search, bootstrapping is also helpful TD search reduces variance but increase bias TD search is usually more efficient than MC search TD($\\lambda$) search can be much more efficient than MC search TD Search Simulate episodes from the current (real) state $\\mathsf{s_t}$ Estimate action-value function $\\mathsf{Q(s,a)}$ For each step of simulation, update action-values by Sarsa\\[\\mathsf{ \\Delta Q(S,A) = \\alpha (R+ \\gamma Q(S&#39;,A&#39;) - Q(S,A)) }\\] Select actions based on action-values $\\mathsf{Q(s,a)} e.g. $\\epsilon$-greedy May also use function approximation for QDyna-2 In Dyna-2, the agent stores two sets of feature weights Long-term memory Short-term (working) memory Long-term memory is updated from real experience using TD learning General domain knowledgd that applies to any episod Short-term memory is updated from simulated experience using TD search Specific local knowledge about the currrent situation Over value function is sum of long and short-term memories" }, { "title": "RLcourse note - Lecture 7 Policy Gradient", "url": "/posts/RL-course-Note-7/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-12-24 01:00:00 +0900", "snippet": "Video Link :Introduction of Policy Gradient7강 소감마지막으로 제일 어려운 부분이라고 하는데 사실 진짜 어렵다기보다는 논리적으로 중요한 연결점들을 다 생략해버리고 이게 결국 이렇게 된다라는 결과적인 내용만 정리한 데다가 notation 은 이상하게 작성되고 시도때도없이 변하니 이해하기 어려운 게 당연하다. 그래도 컨셉은 거의 다 이해했지만 베이스가 되는 논리가 많이 빠져있으니 제대로 기억해낼지는 모르겠다. 나머지 강의에서도 크게 보충이 안될것 같고 결국엔 무언가 해보던지 좀더 심화 강의를 들어보던지 책을 독학하던지 해야 한다. 물론 이정도 지식이면 실제 활용에도 문제 없을 정도는 될것같지만 그래도 찝찝한게 싫은게 나니까 어쩔수없다.Policy-Based Reinforcement Learning In the last lecture we approximated the value or action-value function using parameters $\\theta$ 이번엔 $\\theta$ notation이 변수로 들어가지 않고 underbar로 들어갔다. 의미는 똑같다\\[\\begin{aligned}\\mathsf{V_\\theta (s)} &amp;amp;\\approx \\mathsf{V^\\pi (s)} \\\\\\mathsf{Q_\\theta (s,a)} &amp;amp;\\approx \\mathsf{Q^\\pi (s,a)}\\end{aligned}\\] A policy was generated directly from the value function e.g. using $\\epsilon$-greedy In this lecture we will directly parametrise the policy\\[\\mathsf{ \\pi_\\theta (s,a) = \\mathbb{P} [a\\vert s, \\theta] }\\] We will focus again on model-free reinforcement learningValue-Based and Policy-Based RL Value Based Learnt Value Function Implicit policy (e.g. $\\epsilon$-greedy) Policy Based No Value Function Learnt Policy Actor-Critic Learnt Value Function Learnt Policy Advantages of Policy-Based RLAdvantages: Better convergence properties Effective in high-dimensional or continuous action spaces Can learn stochastic policies 왜 Value based는 Stochastic이 안된다고 하는걸까 걍 하면되지Disadvantages: Typically converge to a local rather than global optimum Evaluating a policy is typically inefficient and high variance 이거에 대한 부분은 차후에 고민해보는걸로Policy SearchPolicy Objective Functions Goal : given policy $\\mathsf{\\pi_\\theta (s,a)}$ with parameters $\\theta$, find best $\\theta$ But how do we measure the quality of a policy $\\pi_\\theta$? In episodic environments we can use the start value\\[\\mathsf{ J_1 (\\theta) = V^{\\pi_\\theta}(s_1) = \\mathbb{E}_{\\pi_\\theta} [v_1] }\\] In continuing envrionments we can use the average value\\[\\mathsf{ J_{avV} (\\theta) = \\displaystyle\\sum_s d^{\\pi_\\theta} (s) V^{\\pi_\\theta} (s) }\\] Or the average reward per time-step\\[\\mathsf{ J_{avR} (\\theta) = \\displaystyle\\sum_s d^{\\pi_\\theta} (s) \\displaystyle\\sum_a \\pi_\\theta (s,a) \\mathcal{R}^a_s }\\] where $\\mathsf{d^{\\pi_\\theta} (s) }$ is stationary distribution of Markov chain for $\\pi_\\theta$ Value 안쓴다더니 Value Function만 안쓰고 Value는 쓰는건가 approximation 만 안하는거네, 근데 state Value는 빼고 Action reward 만 가져다 쓰는Policy Optimisation Policy based reinforcement learning is an optimisation problem Find $\\theta$ that maximises $\\mathsf{J(\\theta)}$ Some approaches do not use gradient Hill climbing Simplex / amoeba / Nelder Mead Genetic algorithms Greater efficiency often possible using gradient Gradient descent Conjugate gradient Quasi-newton We focus on gradient descent, many extensions possible And on methods that exploit sequential structureFinite Difference Policy GradientPolicy Gradient Let $\\mathsf{J(\\theta)}$ be a policy objective function Policy gradient algorithms search for a local maximum in $\\mathsf{J(\\theta)}$ by ascending the gradient of the policy, w.r.t parameters $\\theta$\\[\\Delta \\theta = \\alpha \\nabla _\\theta \\mathsf{J}(\\theta)\\] Where $\\nabla _\\theta \\mathsf{J}(\\theta)$ is the policy gradient사실 이 방법의 local maximise 방법은 적절한 알파값을 찾는게 쉽지 않고 그래프 형태에 따라 달라지기 때문에 사실 그렇게 좋은 방법이라고 하긴 어렵다. 물론 최대한 작은 숫자를 넣어주면 안정적이기는 하지만 그런 만큼 느려지기도 한다. 최대한 간단한 방법의 접근이긴한데 나중에 보충하려나\\[\\mathsf{ \\nabla _\\theta J(\\theta) } = \\left( \\begin{array} \\; \\mathsf{ \\frac{\\partial J(\\theta)}{\\partial \\theta _1} } \\\\ \\vdots \\\\ \\mathsf{ \\frac{\\partial J(\\theta)}{\\partial \\theta _n} } \\end{array} \\right)\\] and $\\alpha$ is a step-size parameter조금 더 좋은 수치해석 접근방법이 있었던거 같은데 수치해석 다시 찾아볼까..Computing Gradients By Finite Differences To evaluate policy gradient of $\\mathsf{\\pi_\\theta (s,a)}$ For each dimension $\\mathsf{k\\in [1,n]}$ Estimate kth partial derivative of objective function w.r.t. $\\theta$ By perturbing $\\theta$ by small amount $\\epsilon$ in kth dimension$$ \\mathsf{ \\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + \\epsilon u_k) - J(\\theta)}{ \\epsilon} } $$ where $\\mathsf{u_k}$ is unit vector with 1 in kth component, 0 elsewhere Uses n evaluations to compute policy gradient in n dimensions Simple, noisy, inefficient - but sometimes effective Works for arbitrary policies, even if policy is not differentiable하나씩 조금씩 바꿔본다는 그런 방법Monte-Carlo Policy GradientLikelihood RatiosScore Function We now compute the policy gradient analytically Assume policy $\\pi_\\theta$ is differentiable whenever it is non-zero and we know the gradient $\\nabla_\\theta \\pi_\\theta \\mathsf{(s,a)}$ Likelihood ratios exploit the following identity\\[\\begin{aligned}\\nabla_\\theta \\pi_\\theta \\mathsf{(s,a)} &amp;amp;= \\mathsf{ \\pi_\\theta (s,a) \\frac{ \\nabla_\\theta \\pi_\\theta (s,a) }{\\pi_\\theta (s,a)} } \\\\&amp;amp;= \\mathsf{ \\pi_\\theta (s,a) \\nabla_\\theta \\log \\pi_\\theta (s,a) }\\end{aligned}\\] The score function is $\\nabla_\\theta \\log \\pi_\\theta \\mathsf{(s,a)}$여기서 Policy를 approximation, Score function 이라는 이름의 의미는 이 값이 policy의 각 확률에 대해서 gradient 보정 방향마다의 보정 크기 비율을 곱해주는 값이기 때문인 듯 Gradient = policy * scoreSoftmax Policy We will use a softmax policy as a running example Weight actions using linear combination of features $ \\phi \\mathsf{(s,a)^T} \\theta $ Probability of action is proportional to exponentiated weight\\[\\mathsf{ \\pi_\\theta (s,a) \\propto e^{\\phi (s,a)^T \\theta} }\\] The score function is\\[\\mathsf{ \\nabla_\\theta \\log \\pi_\\theta (s,a) = \\phi (s,a) - \\mathbb{E}_{\\pi_\\theta}[\\phi(s,\\cdot)] }\\]과정이 안적혀있어서 나중에 찾아봐야Gaussian Policy In continuous action spaces, a Gaussian policy is natural Mean is a linear combination of state features $\\mathsf{\\mu(s) = \\phi(s)^T\\theta}$ Variance may be fixed $\\rho^2$, or can also parametrised Policy is Gaussian, $\\mathsf{a \\sim \\mathcal{N}(\\mu(s),\\rho^2)}$ The scroe function is\\[\\mathsf{ \\nabla_\\theta \\log \\pi_\\theta (s,a) = \\frac{ (a-\\mu(s))\\phi(s) }{ \\rho^2 } }\\]Continuous한 action을 선택해야하여 한 방법으로 Gaussain 형태를 사용한다Policy Gradient TheoremOne-Step MDPs Consider a simple class of one-step MDPs Starting in state $\\mathsf{s\\sim d(s)}$ Terminating after on time-step with reward $\\mathsf{r=\\mathcal{R}_{s,a}}$ Use likelihood ratios to compute the policy gradient\\[\\begin{aligned}\\mathsf{J(\\theta)} &amp;amp;= \\mathbb{E}_{\\pi_\\theta} [\\mathsf{r}] \\\\&amp;amp;= \\mathsf{\\displaystyle\\sum_{s\\in \\mathcal{S}} d(s) \\displaystyle\\sum_{a\\in\\mathcal{A}} \\pi_\\theta (s,a) \\mathcal{R}_{s,a} } \\\\\\mathsf{\\nabla_\\theta J(\\theta)} &amp;amp;= \\mathsf{\\displaystyle\\sum_{s\\in \\mathcal{S}} d(s) \\displaystyle\\sum_{a\\in\\mathcal{A}} \\pi_\\theta (s,a) \\nabla_\\theta \\log \\pi_\\theta (s,a) \\mathcal{R}_{s,a} } \\\\&amp;amp;= \\mathsf{\\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta (s,a) r]}\\end{aligned}\\]이거 그냥 Value Function 아니냐고근데 왜 one step return에 next step state value는 은근슬쩍 빠지고 action reward만 남아있는가 자꾸 이랬다 저랬다 할거야? Terminateing state value가 0이라고 써주던지.. 만약에 state value 포함하면 sigma도 한개 더 들어가야함Policy Gradient Theorem - The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs Replaces instantaneous reward r with long-term value $\\mathsf{Q^\\pi(s,a)}$ - r은 action reward 였는데 멋대로 Q action value로 바꾸기 있냐 Policy gradient theorem applies to start state objective, average reward and average value objective Theorem For any differentiable policy $\\mathsf{\\pi_\\theta(s,a)}$, for any of the policy objective functions $\\mathsf{J = J_1, J_{avR} \\text{, or} \\frac{1}{1-\\gamma}J_{avR}}$, the policy gradient is$$ \\mathsf{ \\nabla_\\theta J(\\theta) = {\\color{Red} \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a)]} } $$ 하여튼 score와 action value 곱의 기대값이다. 증명은 생략되어 있으므로 나중에 책을 찾아본다, 사실상 sarsa나 Q-learning을 parameterise한 것인듯Monte-Carlo Policy Gradient (REINFORCE) Update parameters by stochastic gradient ascent Using policy gradient theorem Using return $\\mathsf{v_t}$ as an unbiased sample of $\\mathsf{Q^{\\pi_\\theta} (s_t,a_t)}$\\[\\mathsf{\\Delta \\theta_t = \\alpha \\nabla_\\theta \\log \\pi_\\theta (s_t,a_t) v_t }\\] function REINFORCE $\\quad$ Initialise $\\theta$ arbitrarily $\\quad$ for each episode $\\mathsf{ \\lbrace s_q,a_q,r_2, \\dots, s_{T-1}, a_{T-1}, r_T \\rbrace \\sim \\pi_\\theta }$ do $\\quad\\quad$ for $\\mathsf{t=1}$ to $\\mathsf{T-1}$ do $\\quad\\quad\\quad$ $\\mathsf{ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (s_t,a_t) v_t }$ $\\quad\\quad$ end for $\\quad$ end for $\\quad$ return $\\theta$ end function 에피소드 중 각 step에 대해서 score 곱하기 return으로 parameter를 업데이트한다. 오래전에 나온 알고리즘이라 딱 이 알고리즘을 Reinforce 알고리즘이라고 한다. 알파고1 에 쓰인 알고리즘improvement가 부드럽지만 variance가 커서 느리다Actor-Critic Policy GradientReducing Variance Using a Critic Monte-Carlo policy gradient still has high variance We use a critic to estimate the action-value function,\\[\\mathsf{ Q_w (s,a) \\approx Q^{\\pi_\\theta} (s,a)}\\] Actor-critic algorithms maintain two sets of parameters Critic - Updates action-value function parameters w Actor - Updates policy parameters $\\theta$, in direction suggested by critic Actor-critic algorithms follow an approximate policy gradient\\[\\begin{aligned}\\mathsf{\\nabla_\\theta J(\\theta)} &amp;amp;\\approx \\mathsf{\\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta (s,a) Q_w(s,a)]} \\\\\\Delta \\theta &amp;amp;= \\mathsf{ \\alpha \\nabla_\\theta \\log \\pi_\\theta (s,a) Q_w(s,a) }\\end{aligned}\\]J 는 average return for time step인데 왜 return 값의 sample로 parameter를 업데이트 하는 부분 이거 자꾸 신경쓰이긴 한데.. alpha값 꽤 중요하겠다.Estimating the Action-Value Function The critic is solving a familiar probelm: policy evaluation How good is policy $\\pi_\\theta$ for current parameters $\\theta$? This problem was explored in previous two lectures, e.g. Monte-Carlo policy evaluation Temporal-Difference learning TD($\\lambda$) Could also use e.g. least-squares policy evaluationAction-Value Actor-Critic Simple actor-critic algorithm based on action-value critic Using linear value fn approx. $\\mathsf{Q_w(s,a) =\\phi(s,a)^T w}$ Critic - Update w by linear TD(0) Actor - Updates $\\theta$ by policy gradient function QAC $\\quad$ Initialise $\\mathsf{s}, \\theta$ $\\quad$ Sample $\\mathsf{ a \\sim \\pi_\\theta }$ $\\quad$ For each step do $\\quad\\quad$ Sample reward $\\mathsf{r=\\mathcal{R}^a_s}$; sample transition $\\mathsf{s’ \\sim \\mathcal{P}^a_s}$ $\\quad\\quad$ Sample action $\\mathsf{a’ \\sim \\pi_\\theta (s’, a’)}$ $\\quad\\quad$ $\\mathsf{ \\delta = r + \\gamma Q_w (s’,a’) - Q_w (s,a)}$ $\\quad\\quad$ $\\mathsf{ \\theta = \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (s,a) Q_w(s,a)}$ $\\quad\\quad$ $\\mathsf{ w \\leftarrow w + \\beta \\delta \\phi (s,a) }$ $\\quad\\quad$ $\\mathsf{ a \\leftarrow a’, s \\leftarrow s’ }$ $\\quad$ end for end function policy와 Value를 둘다 동시에 학습한다는 건데 결국 하나씩 하나씩 조립한 것. TD error를 구하고 policy 업데이트하고 linear model에 error 곱해서 Q parameter weight 업데이트한다.Compatible Function ApproximationBias in Actor-Critic Algorithms Approximatin the policy gradient introduces bias A biased policy gradient may not find the right solution e.g. if $\\mathsf{Q_w(s,a)}$ uses aliased features, can we solve gridworld example? Luckily, if we choose value function approximation carefully Then we can avoid introducing any bias i.e. We can still follow the exact policy gradientCompatible Function Approximation Theorem (Compatible Function Approximation Theorem) If the following two conditions are satisfied: 1. Value function approximator is compatible to the policy$$ \\mathsf{ \\nabla _w Q_w(s,a) = \\nabla_\\theta \\log \\pi_\\theta (s,a) } $$2. Value function parameters w minimise the mean-squared error $$ \\mathsf{ \\epsilon = \\mathbb{E}_{\\pi_\\theta} [(Q^{\\pi_\\theta}(s,a)-Q_w(s,a))^2] } $$ Then the policy gradient is exact,$$\\mathsf{ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log_\\theta \\pi_\\theta(s,a) Q_w (s,a)]} $$ 왜 에러가 parameterised policy의 action Value와 parameterised Action Value의 차이가 되어있을까. 설명이 없어도 너무 없다. improved Value 와 이전 단계 Value의 에러가 되어야 하는 거 아닌가근데 이부분 강의에서 안다루고 대충넘어감Proof of Compatible Function Approximation TheroremIf w is chosen to minimise mean-squared error, gradient of $\\epsilon$ w.r.t. w must be zero,\\[\\begin{aligned}\\mathsf{ \\nabla_w \\epsilon } &amp;amp;= 0 \\\\\\mathsf{ \\mathbb{E}_{\\pi_\\theta} [(Q^{\\theta}(s,a)-Q_w(s,a))\\nabla_w Q_w (s,a)] } &amp;amp;= 0 \\\\\\mathsf{ \\mathbb{E}_{\\pi_\\theta} [(Q^{\\theta}(s,a)-Q_w(s,a)) \\nabla_\\theta \\log \\pi_\\theta (s,a)] } &amp;amp;= 0 \\\\\\mathsf{ \\mathbb{E}_{\\pi_\\theta} [Q^{\\theta}(s,a) \\nabla_\\theta \\log \\pi_\\theta (s,a)] } &amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} [Q_w(s,a) \\nabla_\\theta \\log \\pi_\\theta (s,a)] }\\end{aligned}\\]So $\\mathsf{Q_w(s,a)}$ can be substituted directly into policy gradient,\\[\\mathsf{ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log_\\theta \\pi_\\theta(s,a) Q_w (s,a)] }\\]Advantage Function CriticReducing Variance Using a Baseline We subtract a baseline function B(s) from the policy gradient This can reduce variance, without changing expectation\\[\\begin{aligned}\\mathsf{ \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log_\\theta \\pi_\\theta(s,a) B(s)] } &amp;amp;= \\mathsf{ \\displaystyle\\sum_{s\\in\\mathcal{S}} d^{\\pi_\\theta}(s) \\displaystyle\\sum_{a} \\nabla_\\theta \\pi_\\theta(s,a)B(s) } \\\\&amp;amp;= \\mathsf{ \\displaystyle\\sum_{s\\in\\mathcal{S}} d^{\\pi_\\theta}B(s) \\nabla_\\theta \\displaystyle\\sum_{a\\in\\mathcal{A}} \\pi_\\theta(s,a) } \\\\&amp;amp;= 0\\end{aligned}\\] A good baseline is the state value function $\\mathsf{B(s) = V^{\\pi_\\theta}(s)}$ So we can rewrite the policy gradient using the advantage function $\\mathsf{A^{\\pi_\\theta}(s,a)}$\\[\\begin{aligned}\\mathsf{A^{\\pi_\\theta}(s,a)} &amp;amp;= \\mathsf{ Q^{\\pi_\\theta}(s,a)-V^{\\pi_\\theta}(s) } \\\\\\mathsf{ \\nabla_\\theta J(\\theta)} &amp;amp;= {\\color{Red} \\mathsf{ \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(s,a) A^{\\pi_\\theta}(s,a)] }}\\end{aligned}\\]그냥 action과 관계없는 state value 를 빼면 평균적으로 영향이 0에 수렴한다는 뜻이라서 충분한 수를 sample할 때는 괜찮다는 것Estimating the Advantage Function The advantage function can significantly reduce variance of policy gradient So the critic should really estimate the advantage function For example, by estimating both $\\mathsf{V^{\\pi_\\theta}(s)}$ and $\\mathsf{Q^{\\pi_\\theta}(s,a)}$ Using two function approximators and two parameter vectors,\\[\\begin{aligned}\\mathsf{ V_v(s) } &amp;amp;\\approx \\mathsf{ V^{\\pi_\\theta}(s) } \\\\\\mathsf{ Q_w(s,a) } &amp;amp;\\approx \\mathsf{ Q^{\\pi_\\theta}(s,a) } \\\\\\mathsf{ A(s,a) } &amp;amp;= \\mathsf{ Q_w(s,a)-V_v(s) }\\end{aligned}\\] And updating both value functions by e.g. TD learning For the true value function $\\mathsf{V^{\\pi_\\theta}(s)}$, the TD error $\\delta^{\\pi_\\theta}$\\[\\mathsf{ \\delta^{\\pi_\\theta} = r+\\gamma V^{\\pi_\\theta}(s&#39;) - V^{\\pi_\\theta}(s) }\\] is an unbiased estimate of the advantage function\\[\\begin{aligned}\\mathsf{ \\mathbb{E}_{\\pi_\\theta}[\\delta^{\\pi_\\theta} \\vert s,a ] } &amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta}[ r+\\gamma V^{\\pi_\\theta}(s&#39;) \\vert s,a ] - V^{\\pi_\\theta}(s) } \\\\&amp;amp;= \\mathsf{ Q^{\\pi_\\theta}(s,a) - V^{\\pi_\\theta}(s) } \\\\&amp;amp;= \\mathsf{ A^{\\pi_\\theta}(s,a) }\\end{aligned}\\] So we can use the TD error to compute the policy gradient\\[\\mathsf{ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\nabla_\\theta \\log \\pi_\\theta(s,a) \\delta^{\\pi_\\theta} ] }\\] In practice we can use an approximate TD error\\[\\mathsf{ \\delta_v = r+\\gamma V_v(s&#39;) - V_v(s) }\\] This approach only requires one set of critic parameters v여기까지 하면 State value와 Action Value, Policy까지 전부 parameterise하는 것이었지만. Advantage는 TD error와 같고 이걸로 policy gradient를 계산할 수 있다. 근데 Q가 아니라 V를 parameterise 한다는 건 dimension이 하나 줄어든다는 건데 이거에 관해서 전에 model free가 불가능하다고 했던거 같은데 이거 한번 다시 맞춰봐야겠다.Eligibility TracesCritics at Different Time-Scales Critic can estimate value function $\\mathsf{V_\\theta(s)}$ from many targets at different time-scales For MC, the target is the return $\\mathsf{v_t}$$$ \\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} v_t} - V_\\theta(s))\\phi(s) } $$ For TD(0), the target is the TD target $\\mathsf{ r+\\gamma V(s’) }$$$ \\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} r+\\gamma V(s&#39;) } - V_\\theta(s))\\phi(s) } $$ For forward-view TD(\\lambda), the target is the $\\lambda$-return $\\mathsf{v^\\lambda_t}$$$ \\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} v^\\lambda_t} - V_\\theta(s))\\phi(s) } $$ For backward-view TD, we use eligibility traces$$\\begin{aligned}\\mathsf{ \\delta_t } &amp;amp;= \\mathsf{ r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) } \\\\\\mathsf{ e_t } &amp;amp;= \\mathsf{ \\gamma\\lambda e_{t-1} + \\phi(s_t) } \\\\\\mathsf{ \\Delta \\theta } &amp;amp;= \\mathsf{ \\alpha\\delta_t e_t }\\end{aligned}$$ Actors at Different Time-Scales The policy gradient can also be estimated at many time-scales\\[\\mathsf{ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\nabla_\\theta \\log \\pi_\\theta(s,a) {\\color{Red} A^{\\pi_\\theta}(s,a) } ] }\\] Monte-Carlo policy gradient uses error from complete return\\[\\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} v_t} - V_v(s_t)) \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) }\\] Actor-critic policy gradient uses the one-step TD error\\[\\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} r+\\gamma V_v(s_{t+1})} - V_v(s_t)) \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) }\\]Policy Gradient with Eligibility Traces Just like forward-view TD($\\lambda$), we can mix over time-scales\\[\\mathsf{ \\Delta\\theta = \\alpha({\\color{Red} v^\\lambda_t} - V_v(s_t)) \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t) }\\] where $\\mathsf{ v^\\lambda_t - V_v(s_t)}$ is a biased estimate of advantage fn Like backward-view TD($\\lambda$), we can also use eligibility traces By equivalence with TD($\\lambda$), substituting $\\mathsf{\\phi(s) = \\nabla_\\theta \\log \\pi_\\theta(s,a)}$ \\[\\begin{aligned}\\mathsf{ \\delta } &amp;amp;= \\mathsf{ r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t) } \\\\\\mathsf{ e_{t+1} } &amp;amp;= \\mathsf{ \\lambda e_t + \\nabla_\\theta \\log \\pi_\\theta(s,a) } \\\\\\mathsf{ \\Delta \\theta } &amp;amp;= \\mathsf{ \\alpha\\delta_t e_t }\\end{aligned}\\] This update can be applied online, to incomplete sequencesNatural Policy GradientAlternative Policy Gradient Directions Gradient ascent algorithms can follow any ascent direction A good ascent direction can significantly speed convergence Also, a policy can often be reparametrised without changing action probabilities For example, increasing score of all actions in a softmax policy The vanilla gradient is sensitive to these reparametrisationsNatural Policy Gradient The natural policy gradient is parametrisation independent It finds ascent direction that is closest to vanilla gradient, when changing policy by a small, fixed amount\\[\\mathsf{ \\nabla^{nat}_\\theta \\pi_\\theta (s,a) = G^{-1}_\\theta \\nabla_\\theta \\pi_\\theta(s,a)}\\] where $\\mathsf{G_\\theta}$ is the Fisher information matrix\\[\\mathsf{ G_\\theta = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a)\\nabla_\\theta \\log \\pi_\\theta(s,a)^T \\right] }\\]Natural Actor-Critic Using compatible function approximation,\\[\\mathsf{ \\nabla_w A_w (s,a) = \\nabla_\\theta\\log\\pi_\\theta(s,a) }\\] So the natural policy gradient simplifies,\\[\\begin{aligned}\\mathsf{ \\nabla_\\theta J(\\theta) } &amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a) A^{\\pi_\\theta}(s,a) \\right] } \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a)\\nabla_\\theta \\log \\pi_\\theta(s,a)^T w \\right] } \\\\&amp;amp;= \\mathsf{ G_\\theta w } \\\\\\mathsf{ \\nabla^{nat}_\\theta J(\\theta) } &amp;amp;= \\mathsf{ w } \\\\\\end{aligned}\\] i.e. update actor parameters in direction of critic parametersSummary of Policy Gradient Algorithms The policy gradient has many equivalent forms\\[\\begin{alignat*}{3}\\mathsf{ \\nabla_\\theta J(\\theta) } &amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a)\\; {\\color{Red} v_t } \\right] } \\quad &amp;amp; \\text{REINFORCE} \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a) \\;{\\color{Red} Q^w(s,a) } \\right] } \\quad &amp;amp; \\text{Q Actor-Critic} \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a) \\; {\\color{Red} A^w(s,a) } \\right] } \\quad &amp;amp; \\text{Advantage Actor-Critic} \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a) \\;{\\color{Red} \\delta } \\right] } \\quad &amp;amp; \\text{TD Actor-Critic} \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(s,a) \\;{\\color{Red} \\delta e } \\right] } \\quad &amp;amp; \\text{TD($\\lambda$) Actor-Critic} \\\\\\mathsf{ G^{-1}_\\theta\\nabla_\\theta J(\\theta) } &amp;amp;= \\mathsf{ w } &amp;amp; \\text{Natural Actor-Critic}\\end{alignat*}\\] Each leads a stochastic gradient ascent algorithm Critic uses policy evaluation (e.g. MC or TD learning) to estimate $\\mathsf{Q^\\pi (s,a), A^\\pi(s,a) \\text{ or } V^\\pi(s)}$" }, { "title": "RLcourse note - Lecture 6 Value Function Approximation", "url": "/posts/RL-course-Note-6/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-12-20 17:00:00 +0900", "snippet": "Video Link :Introduction of Value Function ApproximationReinforcement learning can be used to solve large problems, e.g.6강 소감6강은 어느정도 이해했지만 좀 찜찜한 부분들도 있다 슬슬 실전에 사용되는 내용에 많아지는 만큼 세심한 수식들이 많은데 나중에 다시한번 봐야할듯Value Function Approximation So far we gave represented value function by a lookup table Every state s has an entry $\\mathsf{V(s)}$ Or every state-action pair s, a has an entry $\\mathsf{Q(s,a)}$ Probelm with large MDPs: There are too many states and/or actions to store in memory It is too slow to learn the value of each state individually Solution for large MDPs: Estimate value function with function approximation$$ \\begin{aligned}\\mathsf{\\hat{v}(s,w)} &amp;amp;\\approx \\mathsf{v_\\pi(s)} \\\\\\mathsf{or \\;\\hat{q}(s,a,w)} &amp;amp;\\approx \\mathsf{q_\\pi(s,a)}\\end{aligned}$$ Generalise from seen states to unseen states Update parameter w using MC or TD learning Function Approximators Linear combinations of features Neural network Decision tree Nearest neighbour Fourier / wavelet bases …We require a training method that is suitable for non-stationary, non-iid dataIncremental MethodsGradient Descent Let $\\mathsf{J(w)}$ be a differentiable function of parameter vector w Define the gradient of $\\mathsf{J(w)}$ to be$$ \\mathsf{\\nabla _w J(w)} = \\left( \\begin{array} \\mathsf{ \\frac{\\partial J(w)}{\\partial w_1} } \\\\ \\vdots \\\\ \\mathsf{ \\frac{\\partial J(w)}{\\partial w_n} } \\end{array} \\right) $$ To find a local minimum of $\\mathsf{J(w)}$ Adjust w in direction of -ve gradient $$ \\mathsf{ \\Delta w = -\\frac{1}{2} \\alpha \\nabla _w J(w) } $$where $\\alpha$ is a step-size parameterGradient의 minus값으로 향하는 vector이므로 local minimum을 찾게 됨Value Funcion Approx. By Stochastic Gradient Descent Goal: find parameter vector w minimising mean-squared error between approximate value fn $\\mathsf{\\hat{v}(s,w)}$ and true value fn $\\mathsf{v_\\pi(s)}$\\[\\mathsf{ J(w) = \\mathbb{E}_\\pi [(v_\\pi(S) - \\hat{v} (S,w))^2] }\\] Gradient descent finds a local minimum\\[\\begin{aligned}\\Delta \\mathsf{w} &amp;amp;= \\mathsf{-\\frac{1}{2}\\alpha \\nabla _w J(w) } \\\\&amp;amp;= \\mathsf{ \\alpha \\mathbb{E}_\\pi [(v_\\pi(S) - \\hat{v}(S,w))\\nabla _w \\hat{v} (S,w)] }\\end{aligned}\\] Stochastic gradient descent samples the gradientError 의 Gradient descent를 취할 때 Target 은 fixed여야 한다\\[\\mathsf{ \\Delta w = \\alpha (v_\\pi(S) - \\hat{v}(S,w))\\nabla _w \\hat{v} (S,w) }\\] Expected update is equal to full gradient updateLinear Function ApproximationFeature Vectors Represent state by a feature vector\\[\\mathsf{ x(S)} = \\left( \\begin{array}\\, \\mathsf{x_1(S)} \\\\ \\vdots \\\\ \\mathsf{x_n(S)} \\end{array} \\right)\\] For example: Distance of robot from landmarks Trends in the stock market Piece and pawn configurations in chess information의 집합으로 결국 가장 중요한 input임Linear value Function Approximation Represent value function by a linear combination of feature\\[\\mathsf{ \\hat{v}(S,w) = x(S)^T w=\\displaystyle\\sum^n_{j=1}x_j(S)w_j }\\] Objective function is quadratic in parameters $\\mathsf{w}$\\[\\mathsf{ J(w) = \\mathbb{E}_\\pi \\left[ (v_\\pi(S)-x(S)^T w)^2\\right] }\\] Stochastic gradient descent converges on global optimum Update rule is particularly simple\\[\\begin{aligned}\\mathsf{\\nabla _w \\hat{v}(S,w)} &amp;amp;= \\mathsf{x(S)} \\\\\\mathsf{\\Delta w} &amp;amp;= \\mathsf{ \\alpha (v_\\pi(S) - \\hat{v}(S,w))x(S) }\\end{aligned}\\]Update - step-size $\\times$ prediction error $\\times$ feature valueLinear한 경우를 굳이 보여주는 이유는 단순한 문제들에 대하여 문제를 Linear하게 설정해내서 해결해내는 경우가 많기 때문일듯Table Lookup Features Table lookup is a special case of linear value function approximation Using table lookup features\\[\\mathsf{ x^{table}(S)} = \\left( \\begin{array}\\, \\mathsf{1(S=s_1)} \\\\ \\vdots \\\\ \\mathsf{1(S=s_n)} \\end{array} \\right)\\] Parameter vector $\\mathsf{w}$ gives value of each individual state\\[\\mathsf{\\hat{v}(S,w)} = \\left( \\begin{array}\\, \\mathsf{1(S=s_1)} \\\\ \\vdots \\\\ \\mathsf{1(S=s_n)} \\end{array} \\right) \\cdot \\left( \\begin{array}\\, \\mathsf{w_1} \\\\ \\vdots \\\\ \\mathsf{w_n} \\end{array} \\right)\\]이전 강의까지 하던 table lookup 방식의 연장선이라는 것Incremental Prediction Algorithm Have assumed true value function $\\mathsf{v_\\pi(s)}$ given by supervisor But in RL there is no supervisor, only rewards In practice, we substitute a target for $\\mathsf{v_\\pi(s)}$ For MC, the target is the return $\\mathsf{G_t}$$$ \\mathsf{ \\Delta w = \\alpha({\\color{red}G_t} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } $$ For TD(0), the target is the TD target $\\mathsf{R_{t+1} + \\gamma\\hat{v}(S_{t+1},w)}$$$ \\mathsf{ \\Delta w = \\alpha({\\color{red}R_{t+1} + \\gamma\\hat{v}(S_{t+1},w)} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } $$ For TD($\\lambda$), the target is the $\\lambda$-return $\\mathsf{G^\\lambda_t}$$$ \\mathsf{ \\Delta w = \\alpha({\\color{red}G^\\lambda_t} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } $$ target을 고르는 prediction 방법들E가 왜 feature를 더하는지도 생각해봐야Monte-Carlo with Value Function Approximation Return $\\mathsf{G_t}$ is an unbiased, noisy sample of true value $\\mathsf{v_\\pi(S_t)}$ Can therefore apply supervised learning to “training data”:\\[\\mathsf{ \\langle S_1, G_1\\rangle,\\langle S_2,G_2\\rangle, \\dots , \\langle S_T,G_T\\rangle }\\] For example, using linear Monte-Carlo policy evaluation\\[\\begin{aligned}\\mathsf{\\Delta w} &amp;amp;= \\mathsf{ \\alpha({\\color{red}G_t} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } \\\\&amp;amp;= \\mathsf{ \\alpha(G_t - \\hat{v}(S_t,w))x(S_t) }\\end{aligned}\\] Monte-Carlo evaluation converges to a local optimum Even when using non-linear value function approximationTD Learning with Value Function Approximation The TD-target $\\mathsf{R_{t+1} + \\gamma \\hat{v}(S_{t+1},w)}$ is biased sample of true value $\\mathsf{v_\\pi(S_t)}$ Can still apply supervised learning to “training data”:\\[\\mathsf{ \\langle S_1, R_2 + \\gamma\\hat{v}(S_2,w) \\rangle,\\langle S_2,R_3 + \\gamma\\hat{v}(S_3,w) \\rangle, \\dots , \\langle S_T,R_T\\rangle }\\] For example, using linear TD(0)\\[\\begin{aligned}\\mathsf{\\Delta w} &amp;amp;= \\mathsf{ \\alpha({\\color{red}R + \\gamma\\hat{v}(S&#39;,w)} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } \\\\&amp;amp;= \\mathsf{ \\alpha\\delta x(S) }\\end{aligned}\\] Linear TD(0) convergs (close) to global optimumTD($\\lambda$) with Value Function Approximation The $\\lambda$-return $\\mathsf{G^\\lambda_t}$ is also a biased sample of true value $\\mathsf{v_\\pi(s)}$ Can again apply supervised learning to “training data”:\\[\\mathsf{ \\langle S_1, G^\\lambda_1\\rangle,\\langle S_2,G^\\lambda_2\\rangle, \\dots , \\langle S_{T-1},G^\\lambda_{T-1}\\rangle }\\] Forward view linear TD($\\lambda$)\\[\\begin{aligned}\\mathsf{\\Delta w} &amp;amp;= \\mathsf{ \\alpha({\\color{red}G^\\gamma_t} - \\hat{v}(S_t,w))\\nabla _w\\hat{v}(S_t,w) } \\\\&amp;amp;= \\mathsf{ \\alpha({\\color{red}G^\\gamma_t} - \\hat{v}(S_t,w))x(S_t) }\\end{aligned}\\] Backward view linear TD($\\lambda$)\\[\\begin{aligned}\\mathsf{\\delta _t} &amp;amp;= \\mathsf{ R_{t+1} +\\gamma hat{v}(S_{t+1},w) - hat{v}(S_t,w) } \\\\\\mathsf{E_t } &amp;amp;= \\mathsf{\\gamma\\lambda E_{t-1} +x(S_t) } \\\\\\mathsf{\\Delta w} &amp;amp;= \\alpha\\delta _t E_t\\end{aligned}\\]이거 E는 모든 x에 대해서 정의되는거같은데 또는 &amp;lt;S,A&amp;gt; 로target은 왜 Gradient에 포함하지 않는가에 대해서 Time-reversal 이라고 설명하는데 이건 좀 고민을 해봐야겠다. 실제로 해봐도 그렇게 하면 결과가 나오지 않는다고 하는데.... Target 도 gradient를 취할 경우 %\\lambda$가 크면 값이 아주 작아지거나 뒤바뀌기도 하는데 그러면 아예 의미가 달라져서 잘못된 접근이 되긴 한다.Forward view and backward view linear TD($\\lambda$) are equivalentIncremental Control AlgorithmControl with Value Function ApproximationPolicy evaluation - Approximate policy evaluation $\\mathsf{\\hat{q}(\\cdot, \\cdot, w) \\approx q_\\pi}$Policy improvement $\\epsilon$-greedy policy improvementAction-Value Function Approximation Approximate the action-value function\\[\\mathsf{ \\hat{q}(S,A,w) \\approx q_\\pi(S,A) }\\] Minimise mean-squred error between approximate action-value fn $\\mathsf{\\hat{q}(S,A,w)}$ and true action-value fn $\\mathsf{q_\\pi (S,A)}$\\[\\mathsf{ J(w) = \\mathbb{E}_\\pi [(q_\\pi(S,A) - \\hat{q}(S,A,w))^2] }\\] Use stochastic gradient descent to find a local minimum\\[\\begin{aligned}\\mathsf{-\\frac{1}{2}\\nabla _w J(w)} &amp;amp;= \\mathsf{(q_\\pi(S,A)-\\hat{q}(S,A,w)) \\Delta _w \\hat{q}(S,A,w)} \\\\\\mathsf{\\Delta w} &amp;amp;= \\mathsf{\\alpha(q_\\pi(S,A)-\\hat{q}(S,A,w)) \\Delta _w \\hat{q}(S,A,w)}\\end{aligned}\\]Linear Action-Value Function Approximation Represent state and action by a feature vector\\[\\mathsf{x(S,A)} = \\left( \\begin{array} \\; \\mathsf{x_1(S,A)} \\\\ \\vdots \\\\ \\mathsf{x_n(S,A)} \\end{array} \\right)\\] Represent action-value fn by linear combination of features\\[\\mathsf{ \\hat{q}(S,A,w) = x(S,A)^T w = \\displaystyle\\sum^n_{j=1} x_j (S,A)w_j }\\] Stochastic gradient descent update\\[\\begin{aligned}\\mathsf{ \\nabla _w \\hat{q}(S,A,w) } &amp;amp;= \\mathsf{ x(S,A)} \\\\\\mathsf{\\Delta w} &amp;amp;= \\mathsf{\\alpha(q_\\pi(S,A)-\\hat{q}(S,A,w))x(S,A)}\\end{aligned}\\]Incremental Control Algorithms Like prediction, we must substitute a target for $\\mathsf{q_\\pi(S,A)}$ For MC, the target is the return $\\mathsf{G_t}$$$ \\mathsf{ \\Delta w = \\alpha ( {\\color{red}G_t} - \\hat{q}(S_t,A_t,w))\\nabla _w \\hat{q}(S_t,A_t,w) } $$ For TD(0), the target is the TD target $\\mathsf{R_{t+1} + \\gamma Q(S_{t+1}A_{t+1})}$$$ \\mathsf{ \\Delta w = \\alpha ( {\\color{red}R_{t+1} + \\gamma Q(S_{t+1}A_{t+1})} - \\hat{q}(S_t,A_t,w))\\nabla _w \\hat{q}(S_t,A_t,w) } $$ For forward-view TD($\\lambda$), target is the action-value $\\lambda$-return$$ \\mathsf{ \\Delta w = \\alpha ( {\\color{red}q^\\lambda_t} - \\hat{q}(S_t,A_t,w))\\nabla _w \\hat{q}(S_t,A_t,w) } $$ For backward-view TD($\\lambda$), equialent update is$$ \\begin{aligned}\\mathsf{\\delta _t} &amp;amp;= \\mathsf{ R_{t+1} +\\gamma hat{v}(S_{t+1},A_{t+1},w) - hat{v}(S_t,A_t,w) } \\\\\\mathsf{E_t } &amp;amp;= \\mathsf{\\gamma\\lambda E_{t-1} +\\nabla _w \\hat{v}(S_t,A_t,w) } \\\\\\mathsf{\\Delta w} &amp;amp;= \\alpha\\delta _t E_t\\end{aligned} $$ ConvergenceConvergence of Prodiction Algorithms On/Off-Policy Algorithm Table Lookup Linear Non-Linear On-Policy MC TD(0) TD($\\lambda$) $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ X X Off-Policy MC TD(0) TD($\\lambda$) $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ X X $\\checkmark$ X X Gradient Temporal-Difference Learning TD does not follow the gradient of any objective function This is why TD can diverge when off-policy or using non-linear function approximation Gradient TD follows true gradient of projected Bellman error On/Off-Policy Algorithm Table Lookup Linear Non-Linear On-Policy MC TD(0) Gradient TD $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ X $\\checkmark$ Off-Policy MC TD(0) Gradient TD $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ X $\\checkmark$ $\\checkmark$ X $\\checkmark$ Gradient TD 가 뭔진 알아서 찾아보라는 건가Convergence of Control Algorithms Algorithm Table Lookup Linear Non-Linear Monte-Carlo Control Sarsa Q-learning Gradient Q-learning $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ ($\\checkmark$) ($\\checkmark$) X $\\checkmark$ X X X X $(\\checkmark) =$ chatters around near-optimal value functionBatch MethodsBatch Reinforcement Learning Gradient descent is simple and appealing But it is not sample efficient Batch methods seek to find the best fitting value function Given the agent’s experience (“training data”)Least Squares Prediction Given value function approximation $\\mathsf{\\hat{v}(s,w) \\approx v_\\pi(s)}$ And experience $\\mathcal{D}$ consisting of &amp;lt;state, value&amp;gt; pairs\\[\\mathsf{ {\\cal D} = \\lbrace \\langle s_1,v_1^\\pi \\rangle, \\langle s_2,v_2^\\pi \\rangle , \\dots, \\langle s_T,v_T^\\pi \\rangle \\rbrace }\\] Which parameters $\\mathsf{w}$ give the best fitting value fn $\\mathsf{\\hat{v}(s,w)}$? Least squares algorithms find parameter vector w minimising sum-squared error between $\\mathsf{\\hat{v}(s_t,w)}$ and target values $\\mathsf{v^\\pi_t}$\\[\\begin{aligned} \\mathsf{ LS(w) } &amp;amp;= \\mathsf{ \\displaystyle\\sum^T_{t=1} (v^\\pi_t - \\hat{v}(s_t, w))^2 } \\\\ &amp;amp;= \\mathsf{ \\mathbb{E}_\\mathcal{D} [(v^\\pi - \\hat{v}(s,w))^2] } \\end{aligned}\\]Stochastic Gradient Descent with Experience ReplayGiven experience consisting of &amp;lt;state, value&amp;gt; pairs\\[\\mathsf{ \\mathcal{D} = \\lbrace \\langle s_1, v^\\pi_1 \\rangle , \\langle s_2, v^\\pi_2 \\rangle , \\dots , \\langle s_T, v^\\pi_T \\rangle \\rbrace }\\]Repeat: Sample state, value from experience$$ \\mathsf{ \\langle s, v^\\pi \\rangle \\sim \\mathcal{D} } $$ Apply stochastic gradient descent update$$ \\mathsf{ \\Delta w = \\alpha (v^\\pi - \\hat{v} (s,w)) \\nabla _w \\hat{v} (s,w) } $$Converges to least squares solution\\[\\mathsf{ w^\\pi = \\underset{w}{argmin} \\; LS(w) }\\]Experience Replay in Deep Q-Networks (DQN)DQN uses experience replay and fixed Q-targets Take action $\\mathsf{a_t}$ according to $\\epsilon$-greedy policy Store transition $\\mathsf{(s_t, a_t, r_{t+1}, s_{t+1})}$ in replay memory $\\mathcal{D}$ Sample random mini-batch of transitions $\\mathsf{(s, a, r, s’)}$ from $\\mathcal{D}$ Compute !-learning targets w.r.t. old, fixed parameters $w^-$ Optimise MSE between Q-network and Q-learning targets\\[\\mathsf{ \\mathcal{L}_i(w_i) = \\mathbb{E}_{s,a,r,s&#39; \\sim \\mathcal{D}_i} \\left[ \\left( r+\\gamma \\,\\underset{a&#39;}{max} \\; Q(s&#39;, a&#39;; w^-_i ) - Q(s,a,;w_i) \\right)^2 \\right] }\\] Using variant of stochastic gradient descentDQN in Atari End-toend learning of values Q(s,a) from pixels s Input state s is stack of raw pixels from last 4 frames Output is $\\mathsf{Q(s,a)}$ for 18 joystick/button positions Reward is change in score for that stepLinear Least Squares Prediction Experience replay finds least squares solution But it may take many iterations Using linear value funciton approximation $\\mathsf{\\hat{v}(s,w) = x(s)^T w}$ We can solve the least squares solution directly At minimum of $\\mathsf{LS(w)}$, the expected update must be zero\\[\\begin{aligned}\\mathbb{E}_\\mathcal{D} [\\Delta w] &amp;amp;= 0 \\\\\\mathsf{ \\alpha \\displaystyle\\sum^T_{t=1} x(s_t) (v^\\pi_t - x(s_t)^T w ) } &amp;amp;= 0 \\\\\\mathsf{\\displaystyle\\sum^T_{t=1} x(s_t) v^\\pi_t} &amp;amp;= \\mathsf{ \\displaystyle\\sum^T_{t=1} x(s_t)x(s_t)^T w } \\\\\\mathsf{w} &amp;amp;= \\mathsf{ \\left( \\displaystyle\\sum^T_{t=1} x(s_t)x(s_t)^T \\right)^{-1} \\displaystyle\\sum^T_{t=1} x(s_t)v^\\pi_t }\\end{aligned}\\] For N features, direct solution time is $\\mathsf{O(N^3)}$ Incremental solution time is $\\mathsf{O(N^2)}$ using Shermann-MorrisonLinear Least Squares Prediction Algorithms We do not know true value $\\mathsf{v^\\pi_t}$ In practice, our “training data” must use noisy or biased samples of $\\mathsf{v^\\pi_t}$\\[\\begin{alignat*}{2}&amp;amp;\\color{ProcessBlue}{\\text{LSMC}}\\; \\; &amp;amp;&amp;amp; \\text{Least Squares Monte-Carlo uses return} \\\\&amp;amp; \\; &amp;amp;&amp;amp; \\mathsf{v^pi_t \\approx \\color{Red}{G_t}} \\\\&amp;amp;\\color{ProcessBlue}{\\text{LSTD}}\\; \\; &amp;amp;&amp;amp; \\text{Least Squares Temporal-Difference uses TD target} \\\\&amp;amp; \\; &amp;amp;&amp;amp; \\mathsf{v^pi_t \\approx \\color{Red}{R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w)}} \\\\&amp;amp;\\color{ProcessBlue}{\\text{LSTD} (\\lambda)}\\; \\; &amp;amp;&amp;amp; \\text{Least Squares TD($\\lambda$) uses $\\lambda$ -return} \\\\&amp;amp; \\; &amp;amp;&amp;amp; \\mathsf{v^pi_t \\approx \\color{Red}{G^\\lambda_t}} \\\\\\end{alignat*}\\] In each case solve directly for fixed point of MC / TD / TD($\\lambda$)\\[\\begin{align*}\\color{ProcessBlue}{\\text{LSMC}} &amp;amp;&amp;amp; \\; 0\\; &amp;amp; \\mathsf{= \\displaystyle\\sum^T_{t=1} \\alpha (G_t - \\hat{v}(S_t,w))x(S_t)} \\\\&amp;amp;&amp;amp; \\; w\\; &amp;amp; \\mathsf{= \\left( \\displaystyle\\sum^T_{t=1} x(S_t) x(S_t)^T \\right)^{-1} \\displaystyle\\sum^T_{t=1}x(S_t)G_t } \\\\\\color{ProcessBlue}{\\text{LSTD}} &amp;amp;&amp;amp; \\; 0\\; &amp;amp; \\mathsf{= \\displaystyle\\sum^T_{t=1} \\alpha (R_{t+1} + \\gamma \\hat{v} (S_{t+1}, w) - \\hat{v}(S_t,w))x(S_t)} \\\\&amp;amp;&amp;amp; \\; w\\; &amp;amp; \\mathsf{= \\left( \\displaystyle\\sum^T_{t=1} x(S_t)( x(S_t) - \\gamma x(S_{t+1}))^T \\right)^{-1} \\displaystyle\\sum^T_{t=1}x(S_t)R_{t+1} } \\\\\\color{ProcessBlue}{\\text{LSTD} (\\lambda)} &amp;amp;&amp;amp; \\; 0\\; &amp;amp; \\mathsf{= \\displaystyle\\sum^T_{t=1} \\alpha \\delta _t E_t} \\\\&amp;amp;&amp;amp; \\; w\\; &amp;amp; \\mathsf{= \\left( \\displaystyle\\sum^T_{t=1} E_t( x(S_t) - \\gamma x(S_{t+1}) )^T \\right)^{-1} \\displaystyle\\sum^T_{t=1} E_t R_{t+1} } \\\\\\end{align*}\\]Convergence of Linear Least Squares Prediction Algorithms On/Off-Policy Algorithm Table Lookup Linear Non-Linear On-Policy MC LSMC TD LSTD $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ - X - Off-Policy MC LSMC TD LSTD $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ X $\\checkmark$ $\\checkmark$ - X - Least Squares ControlLeast Squares Policy IterationPolicy evaluation - Policy evaluation by least squares Q-learning Policy improvement Greedy policy improvementLeast Squares Action-Value Function Approximation Approximate action-value function $\\mathsf{q_\\pi(s,a)}$ using linear combination of features $\\mathsf{x(s,a)}$\\[\\mathsf{ \\hat{q}(s,a,w) = x(s,a)^T w \\approx q_\\pi(s,a) }\\] Minimise least squares error between $\\mathsf{\\hat{q}(s,a,w)}$ and $\\mathsf{q(s,a)}$ from experience generated using policy $\\pi$ consisting of &amp;lt;(state, action), value&amp;gt; pairs\\[\\mathsf{ \\mathcal{D} = \\lbrace \\langle (s_1,a_1), v^\\pi_1\\rangle, \\langle (s_2,a_2), v^\\pi_2\\rangle, \\dots, \\langle (s_T,a_T), v^\\pi_T\\rangle \\rbrace }\\]Least Square Control For policy evaluation, we want to efficiently use all experience For control, we also want to improve the policy This experience is generated from many policies So to evaluate $\\mathsf{q_\\pi(S,A)}$ we must learn off-policy We use the same idea as Q-learning: Use experience generated by old policy\\(\\mathsf{ S_t, A_t, R_{t+1}, S_{t+1} \\sim \\pi_{old} }\\) Consider alternative successor action $\\mathsf{A’ = \\pi_{new}(S_{t+1})}$ Update $\\mathsf{\\hat{q}(S_t,A_t,w)}$ towards value of alternative action\\(\\mathsf{ R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A&#39;, w) }\\) Least Squares Q-Learning Consider the following linear Q-learning update\\[\\begin{aligned}\\delta &amp;amp;= \\mathsf{ R_{t+1} + \\gamma \\hat{q} (S_{t+1}, \\pi(S_{t+1}),w) - \\hat{q}(S_t,A_t,w) } \\\\\\Delta \\mathsf{w} &amp;amp;= \\mathsf{ \\alpha \\delta x(S_t, A_t) }\\end{aligned}\\] LSTDQ algorithm: solve for total update = zero\\[\\begin{aligned}0 &amp;amp;= \\mathsf{ \\displaystyle\\sum^T_{t=1} \\alpha(R_{t+1} + \\gamma \\hat{q} (S_{t+1}, \\pi(S_{t+1}),w) - \\hat{q}(S_t,A_t,w)) x(S_t,A_t) } \\\\\\mathsf{w} &amp;amp;= \\mathsf{ \\left( \\displaystyle\\sum^T_{t=1} x(S_t,A_t)(x(S_t,A_t) - \\gamma x(S_{t+1}, \\pi(S_{t+1})))^T \\right)^{-1} \\displaystyle\\sum^T_{t=1} x(S_t, A_t)R_{t+1} }\\end{aligned}\\]Least Squares Policy Iteration Algorithm The following pseudocode uses LSTDQ for policy evaluation It repeatedly re-evaluates experience $\\mathcal{D}$ with different policies function LSPI-TD($\\mathcal{D}, \\pi_0$) $\\quad$ $\\pi’ \\leftarrow \\pi_0$ $\\quad$ Repeat $\\quad\\quad$ $\\pi \\leftarrow \\pi’$ $\\quad\\quad$ \\(\\mathsf{ Q \\leftarrow LSTDQ(\\pi, \\mathcal{D})}\\) $\\quad\\quad$ for all $\\mathsf{s} \\in \\mathcal{S}$ do $\\quad\\quad\\quad$ $ \\mathsf{ \\pi’(s) \\leftarrow \\underset{a\\in\\mathcal{A}}{argmax}\\; Q(s,a) } $ $\\quad\\quad$ end for $\\quad$ until $(\\pi \\approx \\pi’)$ $\\quad$ return $\\pi$ end function Convergence of Control Algorithms Algorithm Table Lookup Linear Non-Linear Monte-Carlo Control Sarsa Q-learning LSPI $\\checkmark$ $\\checkmark$ $\\checkmark$ $\\checkmark$ ($\\checkmark$) ($\\checkmark$) X ($\\checkmark$) X X X - ($\\checkmark$) = chatters around near-optimal value function" }, { "title": "RLcourse note - Lecture 5 Model-Control", "url": "/posts/RL-course-Note-5/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-12-16 06:00:00 +0900", "snippet": "Video Link :Introduction of Model-Free ControlOptimise the Value function of an unknown MDPModel-free control can solve Some MDP problems which modelled: MDP model is unknown, but experience can be sampled MDP model is known, but is too big to use, except by samples5강 소감Q-Learning으로 간단하지만 깔끔한 update 방법으로 정리되는 과정을 하나하나 짚어간다. 이해하고 보니 별거 아니긴 하네. 결국 여기까지 발전하기까지 누군가가 Idea를 냈다는 것 뿐. Q-learning이 유명하고 많이 보이는데 Return이 action에서 오는 경우가 많은가 보다. 그래서 action value를 사용하고 update를 조금 더 효율적으로 하면서 off-policy를 사용하도록 Algorithm이 정리된 것이다. 근데 Off policy는 이전에 학습된 episode에서도 배울수 있다는 게 내가 보기엔 중요한 장점인 것 같은데, 결국 여기까지에선 같은 state와 action을 공유하는 상황에서 확률이 다른 것들을 서로 다른 policy로 본다는 뜻이고 한두번만 개선되어도 다른 policy로 정의한다는 뜻이라서 내 기대와는 조금 다르다. 물론 설계만 잘 된다면 model 자체가 조금 달라지더라도 어느정도 공유하는 부분에 대해서는 off policy를 활용할 방법을 찾을 수도 있을 것이다. 또 다르게 학습된 policy를 사용한다면 여기서 말하는 Q-Learning과는 다른 부분이 되겠지만 이렇게 되면 target policy는 개선되지 않고 고정이 되는 만큼 다른 문제가 발생할 수 있다. 이 부분이 오히려 개선을 방해할 수 있는 만큼 policy 또한 exploration이 가능하도록 target policy를 $\\epsilon$ 방법을 활용하는 방법도 체계화할 수도 있을 것이다. Q-Learning에서 Off Policy 중 다른 학습 결과를 활용하는 방법이 나오면 좋겠다.On-Policy Monte-Carlo Control On-policy Learning Learn on the job Learn about policy $\\pi$ from experience sampled from $\\pi$ Off-policy Learning Look over someone’s shoulder Learn about poilcy $\\pi$ from experience sampled from $\\mu$ Generalised Policy IterationGeneralised Policy Iteration With Monte-Carlo EvaluationPolicy evaluation - Monte-Carlo policy evaluationPolicy improvement - Greedy policy improvement?Model-Free Policy Iteration Using Action-Value Function Greedy policy improvement over $\\mathsf{V(s)}$ requires model of MDP\\[\\mathsf{\\pi&#39;(s) = \\underset{a \\in \\mathcal{A}}{argmax}\\;\\mathcal{R}^a_s + \\mathcal{P}^a_{ss&#39;}V(s&#39;) }\\] Greedy policy improvement over $\\mathsf{Q(s,a)}$ is model-free\\[\\mathsf{\\pi&#39;(s) = \\underset{a \\in \\mathcal{A}}{argmax}\\;Q(s,a) }\\]이 부분이 아직 이해가 안된거같기도 하다. 왜 state value로 업데이트 하는건 MDP 모델이 필요하고 Action value로 업데이트하는건 model-free 인가… 왜 Q는 알수 있는데 P V는 알 수 없는가…Exporation$\\epsilon$-Greedy Exploration Simplest idea for ensuring continual exploration All m actions are tried with non-zero probability With probability $1-\\epsilon$ choose the greedy action With probability $\\epsilon$ choose an action at random\\[\\mathsf{\\pi(a\\vert s)=} \\begin{cases}\\epsilon/\\mathsf{m} +1 - \\epsilon &amp;amp; \\mathsf{if\\;a^*=\\underset{a\\in \\mathcal{A}}{argmax}\\;Q(s,a)} \\\\\\epsilon/\\mathsf{m} &amp;amp; \\mathsf{otherwise}\\end{cases}\\]또 또 식 이상하게 쓴다… policy 결과가 왜 숫자가 나와….. 이번엔 $\\pi$ output이 action이 아니라 action을 선택할 확률이다 이거지…. 좀 문자 다른거 써라..$\\epsilon$-Greedy Policy Improvement Theorem For any $\\epsilon$-greedy policy $\\pi$, the $\\epsilon$-greedy policy $\\pi’$ with respect to $\\mathsf{q_\\pi}$ is an improvement, $\\mathsf{v_{\\pi’}(s)\\geq v_\\pi(s)}$ \\[\\begin{aligned}\\mathsf{q_\\pi (s,\\pi&#39;(s))} &amp;amp;= \\mathsf{\\displaystyle\\sum_{a\\in\\mathcal{A}}\\pi&#39;(a\\vert s)q_\\pi(s,a)}\\\\&amp;amp;=\\mathsf{\\epsilon/m \\displaystyle\\sum_{a\\in\\mathcal{A}}q_\\pi (s,a)+(1-\\epsilon)\\underset{a\\in\\mathcal{A}}{max}\\;q_\\pi(s,a)}\\\\&amp;amp;\\leq \\mathsf{ \\epsilon/m \\displaystyle\\sum_{a\\in\\mathcal{A}}q_\\pi (s,a)+(1-\\epsilon) \\displaystyle\\sum_{a\\in\\mathcal{A}} \\frac{\\pi(a\\vert s)-\\epsilon/m}{1-\\epsilon}q_\\pi(s,a) } \\\\&amp;amp;=\\mathsf{\\displaystyle\\sum_{a\\in\\mathcal{A}}\\pi(a\\vert s)q_\\pi(s,a)=v_\\pi(s)}\\end{aligned}\\]Therefor from policy improvement theorem, $\\mathsf{v_{\\pi’}(s)\\geq v_\\pi(s)}$사실 greedy만 해도 improvement가 보장되었었는데 일부만 greedy 나머지는 random 할때도 보장되는 건 당연한거긴 하다. 굳이 이런건 또 증명해주네Monte-Carlo Policy IterationPolicy evaluation - Monte-Carlo policy evaluation $\\mathsf{Q=q_\\pi}$Policy improvement $\\epsilon$-greedy policy improvementMonte-Carlo Policy ControlEvery episode:Policy evaluation - Monte-Carlo policy evaluation $\\mathsf{Q\\approx q_\\pi}$Policy improvement $\\epsilon$-greedy policy improvementMonte-Carlo evaluation 할때 iteration 하지 않고 every episode 마다 improvement 진행GLIE Definition Greedy in the Limit with Infinite Exploration(GLIE)$\\quad \\bullet\\;$ All state-action pairs are explored infinitely many times,$$ \\mathsf{ \\underset{k\\rightarrow\\infty}{lim}\\;\\, N_k(s,a) = \\infty } $$$\\quad \\bullet\\;$ The policy converges on a greedy policy,$$ \\mathsf{ \\underset{k\\rightarrow\\infty}{lim}\\;\\, \\pi_k(a\\vert a) = 1(a=\\underset{a&#39;\\in\\mathcal{A}}{argmax}\\;Q_k(s,a&#39;)) } $$ For example, $\\epsilon$-greedy is GLIE if $\\epsilon$ reduces to zero at $\\mathsf{\\epsilon_k = \\frac{1}{k}}$GLIE Monte-Carlo Control Sample kth episode using $\\pi:\\mathsf{ \\lbrace S_1,A_1,R_2,\\dots,S_T \\rbrace \\sim \\pi}$ For each state $\\mathsf{S_t}$ and action $\\mathsf{A_t}$ in the episode,\\[\\begin{aligned}&amp;amp;\\mathsf{ N(S_t,A_t)\\leftarrow N(S_t,A_t)+1 }\\\\&amp;amp;\\mathsf{ Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\frac{1}{N(S_t,A_t)}(G_t - Q(S_t,A_t)) }\\end{aligned}\\] Improve policy based on new action-value function\\[\\begin{aligned}\\epsilon &amp;amp;\\leftarrow \\mathsf{ 1/k }\\\\\\pi &amp;amp;\\leftarrow \\mathsf{ \\epsilon-greedy(Q) }\\end{aligned}\\]$\\epsilon$ 값이 episode 마다 점점 작아짐 - exploration 감소 Theorem GLIE Monte-Carlo control converges to the optimal action-value functino, $\\mathsf{ Q(s,a)\\rightarrow q_*(s,a) }$ On-Policy Temporal-Difference LearningMC vs. TD Control Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC) Lower variance Online Incomplete sequence Natural idea: use TD instead of MC in our control loop Apply TD to $\\mathsf{Q(S,A)}$ Use $\\epsilon$-greedy policy improvement Update every time-step Sarsa($\\lambda$)Updating Action-Value Functions with Sarsa\\[\\mathsf{ Q(S,A) \\leftarrow Q(S,A) + \\alpha (R+ \\gamma Q(S&#39;,A&#39;) - Q(S,A)) }\\]On-Policy Control With SarsaEvery time-stepPolicy evaluation Sarsa, $\\mathsf{Q \\approx q_\\pi}$Policy improvement $\\epsilon$-greedy policy improvementSarsa Algorithm for On-Policy Control Initialize $\\mathsf{Q(s,a), \\forall s \\in S, a \\in A(s)}$, arbitrarily,and $\\mathsf{Q}(terminal state, \\cdot)=0$Repeat (for each episode):$\\quad$Initialize S$\\quad$Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)$\\quad$Repeat (for each step of episode)$\\quad\\quad$Take action A, observe R, S’$\\quad\\quad$Choose A’ from S’ using policy derived from Q (e.g., $\\epsilon$-greedy)\\(\\quad\\quad\\mathsf{Q(S,A)\\leftarrow Q(S,A) + \\alpha[R+\\gamma Q(S&#39;,Q&#39;) - Q(S,A)] }\\)\\(\\quad\\quad\\mathsf{ S\\leftarrow S&#39;; A \\leftarrow A&#39;;}\\)$\\quad$until S is terminal 다음 state의 action 까지 포함하는 TD의 action 확장 버전Convergence of Sarsa Theorem Sarsa converges to the optimal action-value function,$\\mathsf{Q(s,a)\\rightarrow q_\\pi(s,a)}$, under the following conditions:$\\quad \\bullet\\;$ GLOE sequence of policies $\\mathsf{ \\pi_t(a\\vert s) }$$\\quad \\bullet\\;$ Robbins-Monro sequence of step-sizes $\\alpha_t$$$\\begin{aligned} &amp;amp;\\mathsf{ \\displaystyle\\sum^\\infty_{t=1} \\alpha_t = \\infty } \\\\ &amp;amp;\\mathsf{ \\displaystyle\\sum^\\infty_{t=1} \\alpha^2_t &amp;lt; \\infty } \\end{aligned} $$ n-Step Sarsa Consider the following n-step returns for $n = 1, 2, \\infty$:\\[\\begin{aligned}\\mathsf{n=1\\;\\; (Sarsa)\\;\\;} &amp;amp; \\mathsf{q^{(1)}_t = R_{t+1} + \\gamma Q(S_{t+1})} \\\\\\mathsf{n=2\\quad \\quad\\quad\\quad\\,} &amp;amp; \\mathsf{q^{(2)}_t = R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 Q(S_{t+2})} \\\\\\vdots \\quad\\quad\\quad\\quad\\quad\\; &amp;amp; \\;\\, \\vdots \\\\\\mathsf{n=\\infty \\;\\, (MC)\\quad} &amp;amp; \\mathsf{q^{(\\infty)}_t = R_{t+1} + \\gamma R_{t+2} +\\dots + \\gamma ^{T-1} R_T}\\end{aligned}\\] Define the n-step return\\[\\mathsf{q^{(n)}_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma ^{n-1} R_{t+n} + \\gamma ^n Q(S_{t+n})}\\] n-step Sarsa updates $\\mathsf{Q(s,a)}$ towards the n-step Q-return\\[\\mathsf{ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) +\\alpha \\left(q^{(n)}_t - Q(S_t,A_t)\\right) }\\]Forward View Sarsa($\\lambda$) The $q^\\lambda$ return combines all n-step Q-reutrns $\\mathsf{q^{(n)}_t}$ Using weight $\\mathsf{(1-\\lambda)\\lambda ^{n-1}}$\\[\\mathsf{q^\\lambda_t = (1-\\lambda) \\displaystyle\\sum^\\infty_{n=1} \\lambda^{n-1} q^{(n)}_t}\\] Forward-view Sarsa($\\lambda$)\\[\\mathsf{ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) +\\alpha \\left(q^\\lambda_t - Q(S_t,A_t)\\right)}\\]Backward View Sarsa($\\lambda$) Just like TD($\\lambda$), we use eligibility traces in an online algorithm But Sarsa($\\lambda$) has one eligibility trace for each state-action pair\\[\\begin{aligned}&amp;amp;\\mathsf{E_0(s,a) =0} \\\\&amp;amp;\\mathsf{E_t(s,a) =\\gamma \\lambda E_{t-1}(s,a) + 1(S_t=s, A_t=a)}\\end{aligned}\\]이거 $\\gamma$는 알겠는데 $\\lambda$는 왜 곱해진거지. 그리고 식 제발…. 나야 이해하겠지만 진짜 쉽게 갈걸 어렵게가게 만드네, 수식을 개발자마인드로 쓴건가 $\\mathsf{Q(s,a)}$ is updated for every state s and action a In proportion to TD-error $\\delta_t$ and eligibility trace $\\mathsf{E_t(s,a)}$\\[\\begin{aligned}&amp;amp;\\delta_t = \\mathsf{R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t))}\\\\&amp;amp;\\mathsf{Q(s,a) \\leftarrow Q(s,a) + \\alpha\\delta _t E_t(s,a)}\\end{aligned}\\]Sarsa($\\lambda$) Algorithm Initialize $\\mathsf{Q(s,a)}$ aribitrarily, for all $s \\in S, a \\in A(s)$Repeat (for each episode):$\\quad$E(s,a) = 0, for all $s \\in S, a \\in A(s)$$\\quad$Initialize S, A$\\quad$Repeat (for each step of episode)$\\quad\\quad$Take action A, observe R, S’$\\quad\\quad$Choose A’ from S’ using policy derived from Q (e.g., $\\epsilon$-greedy)\\(\\quad\\quad \\delta \\leftarrow R+ \\gamma Q(S&#39;, A&#39;) - Q(S,A)\\) \\(\\quad\\quad E(S,A) \\leftarrow E(S,A) + 1\\) $ \\quad\\quad$ For all $s \\in S, a \\in A(s)$:\\(\\quad\\quad\\quad Q(s,a)\\leftarrow Q(s,a) + \\alpha\\delta E(s,a)\\)\\(\\quad\\quad\\quad E(s,a) \\leftarrow \\gamma\\lambda E(s,a)\\)\\(\\quad\\quad S\\leftarrow S&#39;; A \\leftarrow A&#39;;\\)$\\quad$until S is terminal $\\delta$는 S,A 에 대한 error 값인데 이 값을 episode 내에서 지나온 모든 value를 업데이트하는데 사용한다. Eligibility factor에 의해 감소되어 멀수록 점점 영향은 적어지고 최근에 영향을 미친 곳일수록 크게 작용하기는 한다. 이 업데이트는 때로는 방해가 되기도 하고 잘못된 방향을 강화하는 것도 가능하다. 그러나 충분한 양을 한다고 했을 때 결국 최적 값으로 수렴하긴 할것이다. 그러나 Exploration 비율과 개성되는 값에 따라서는 어딘가에 물려서 개선되지 못하는 것도 가능할 것 같다.Off-Policy Learning Evaluate target policy $\\mathsf{ \\pi(a \\vert s)}$ to compute $\\mathsf{v_\\pi(s)}$ or $\\mathsf{q_\\pi(s,a)}$ While following behaviour policy $\\mathsf{\\mu(a\\vert s)}$\\[\\mathsf{ \\lbrace S_1, A_1, R_2, \\dots, S_T \\rbrace \\sim \\mu }\\]Why is this important Learn from observing humans or other agents Re-use experience generated from old policies $\\pi_1, \\pi_2, \\dots, \\pi_{t-1}$ Learn about optimal policy while following exploratory policy Learn about multiple policies while following one policy Policy가 다르면 Value가 어느정도 다르게 계산될 텐데 그래도 그 결과를 사용할수 있다라.. 매우 중요하고 도움이 될 내용이긴 하다Importance Sampling Estimate the expectation of a different distribution\\[\\begin{aligned}\\mathsf{ \\mathbb{E}_{X \\sim P}[f(X)] } &amp;amp;= \\sum \\mathsf{P(X)f(X)} \\\\&amp;amp;= \\sum \\mathsf{Q(X) \\frac{P(X)}{Q(X)} f(X) } \\\\&amp;amp;= \\mathsf{ \\mathbb{E}_{X \\sim Q} \\left[ \\frac{P(X)}{Q(X)}f(X)\\right]}\\end{aligned}\\]plicy가 정하는 확률에 대해서 Environment에서 Action reward 는 고정이고 policy에 의한 확률만 고정이므로 이 확률 비율만 보정해주면 return을 다른 policy에 대해서도 구할 수 있다는 뜻인데 이는 기본적으로 같은 Action을 따라 갈 때만 가능하다. $\\epsilon$-greedy 등으로 exploration을 포함하는 policy를 통해 충분히 많은 action 가짓 수를 확보하고 모든 episode 데이터가 있을 때에만 return을 계산할 수 있으므로 deterministic policy의 data는 학습이 어렵다고 볼 수 있다.Importance Sampling for Off-Policy Monte-Carlo Use returns generated from $\\mu$ to evaluate $\\pi$ Weight return $\\mathsf{G_t}$ according to similarity between policies Multiply importance sampling corrections along whole episode\\[\\mathsf{ G^{\\pi/\\mu}_t = \\frac{\\pi(A_t \\vert S_t)}{\\mu(A_t \\vert S_t)} \\frac{\\pi(A_{t+1} \\vert S_{t+1})}{\\mu(A_{t+1} \\vert S_{t+1})} \\dots \\frac{\\pi(A_T \\vert S_T)}{\\mu(A_T \\vert S_T)} G_t }\\] Update value towards corrected return\\[\\mathsf{ V(S_t) \\leftarrow V(S_T) + \\alpha\\left(G^{\\pi/\\mu}_t - V(S_t)\\right) }\\] Cannot use if $\\mu$ is zero when $\\pi$ is non-zero Importance sampling can dramatically increase varianceImportance Sampling for Off-Pollicy TD Use TD targets generated from $\\mu$ to evaluate $\\pi$ Weight TD target $\\mathsf{R+\\gamma V(S’)}$ by importance sampling Only need a single importance sampling correction\\[\\mathsf{ V(S_t) \\leftarrow V(S_t) + \\alpha \\left( \\frac{\\pi(A_t\\vert S_t)}{\\mu(A_t\\vert S_t)} (R_{t+1} + \\gamma V(S_{t+1})) - V(S_t)\\right) }\\] Much lower variance than Monte-Carlo importance sampling Policies only need to be similar over a single step즉 $\\mu$ 가 0이거나 너무 작으면 사용하기 힘들다고 하는데 이건 내용을 잘못 파악한것 같다. 물론 episode의 다양성이 커질수록 연산이 힘들어지는 문제가 있다고 볼 수도 있지만 조금만 더 접근하면 충분히 가능할것 같은데. TD를 통해서 solution을 구하는 방법과도 엄청나게 다르지 않을것같은데. 제약조건만 만족하면 상상속에서나 가능한 건 아닐듯Q-Learning We now consider off-policy learning of action-values $\\mathsf{Q(s,a)}$ No importance sampling is required Next action ischosen using begaviour policy $\\mathsf{ A_{t+1} \\sim \\mu(\\cdot \\vert S_t) }$ But we consider alternative successor action $\\mathsf{ A’ \\sim \\pi(\\cdot \\vert S_t) }$ And update $\\mathsf{Q(S_t,A_t)}$ towards value of alternative action\\[\\mathsf{ Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha \\left(R_{t+1} + \\gamma Q(S_{t+1},A&#39;) - Q(S_t,A_t)\\right) }\\]Error를 다른 action 결과값에서 가져와서 update 해도 된다는건데 그럼 결국 그 다른 policy에만 가까워지는거 아닌가 학습하는 policy가 아니라는 말은 그 결과를 가져오는 target policy는 개선이 안된다는거 같은데 그게 optimal하다고 가정하는건가 그럼 의미가 없는데Off-Policy Control with Q-Learning We now allow both behaviour and target policies to improve The target policy $\\pi$ is greedy w.r.t. $\\mathsf{Q(s,a)}$\\[\\mathsf{ \\pi (S_{t+1}) = \\underset{a&#39;}{argmax}\\;Q(S_{t+1}, a&#39;) }\\] The behaviour policy $\\mu$ is e.g. $\\epsilon$-greedy w.r.t. $\\mathsf{Q(s,a)}$ The Q-learning target then simplifies:\\[\\begin{aligned}&amp;amp; \\mathsf{ R_{t+1} + \\gamma Q(S_{t+1}, A&#39;) } \\\\=&amp;amp; \\mathsf{ R_{t+1} + \\gamma Q(S_{t+1}, \\underset{a&#39;}{argmax}\\; Q(S_{t+1}, a&#39;)) } \\\\=&amp;amp; \\mathsf{ R_{t+1} + \\underset{a&#39;}{max}\\;\\gamma Q(S_{t+1}, a&#39;) } \\\\\\end{aligned}\\]target은 greedy, behaviour은 $\\epsilon$-greedy라서 target policy도 결국 개선되긴 한다는 말이네 그니까 결국 exploration을 유지하되 update에 사용되는 return은 greedy로 유지해서 error term이 잘못되지 않도록 한다는 말이군. 그럼 이미 축적된 학습 Data는 Behaviour로 사용되는건가Q-Learning Control Algorithm\\[\\mathsf{ Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left( R + \\gamma\\; \\underset{a&#39;}{max}\\; Q(S&#39;,a&#39;) - Q(S,A)\\right) }\\] Theorem Q-learning control converges to the optimal action-value function, $\\mathsf{Q(s,a)\\rightarrow q_*(s,a)}$ Q-learning을 Sarsa max라고도 부른다고 함 결국 action value를 improve하는 방법에서 behaviour는 exploration을 포함하고 update는 greedy로 한다는 뜻Q-Learning Algorithm for Off-Policy Control Initialize $\\mathsf{Q(s,a) \\forall s \\in S, a \\in A(s)}$, arbitrarily, and $\\mathsf{Q(terminal\\, state,\\cdot)=0}$Repeat (for each episode):$\\quad$Initialize S$\\quad$Repeat (for each step of episode)$\\quad\\quad$Choose A from S using policy derived from Q (e.g., $\\epsilon$-greedy)$\\quad\\quad$Take action A, observe R, S’\\(\\quad\\quad Q(S,A)\\leftarrow Q(S,A) + \\alpha [ R + \\gamma\\; max_a Q(S&#39;,a)-Q(S,A)]\\)\\(\\quad\\quad S\\leftarrow S&#39;;\\)$\\quad$until S is terminal Summary   Full Backup (DP) Sample Backup (TD) Bellman Expectation Equation for $\\mathsf{v_\\pi(s)}$ Iterative Policy Evaluation\\(\\mathsf{ V(s) \\leftarrow \\mathbb{E} [R+\\gamma V(S&#39;) \\vert s] }\\) TD Learning \\(\\mathsf{ V(S)\\;\\overset{\\alpha}{\\leftarrow}\\; R+\\gamma V(S&#39;)}\\) Bellman Expectation Equation for $\\mathsf{q_\\pi(s,a)}$ Q-Policy Iteration\\(\\mathsf{ Q(s,a) \\leftarrow \\mathbb{E} [R+\\gamma Q(S&#39;,A&#39;) \\vert s,a] }\\) Sarsa \\(\\mathsf{ Q(S,A)\\;\\overset{\\alpha}{\\leftarrow}\\; R+\\gamma Q(S&#39;,A&#39;)}\\) Bellman Optimality Equation for $\\mathsf{q_*(s,a)}$ Q-Value Iteration\\(\\mathsf{ Q(s,a) \\leftarrow \\mathbb{E} \\left[ R+\\gamma \\;\\underset{a&#39;\\in\\mathcal{A}}{max}\\;Q(S&#39;,a&#39;) \\vert s,a \\right] }\\) Q-Learning \\(\\mathsf{ Q(S,A)\\;\\overset{\\alpha}{\\leftarrow}\\; R+\\gamma \\;\\underset{a&#39;\\in\\mathcal{A}}{max}\\; Q(S&#39;,a&#39;)}\\) where \\(\\mathsf{ x \\overset{\\alpha}{\\leftarrow} y \\equiv x \\leftarrow x + \\alpha(y-x) }\\)" }, { "title": "RLcourse note - Lecture 4 Model-Free Prediction", "url": "/posts/RL-course-Note-4/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-12-09 21:20:00 +0900", "snippet": "Video Link :Introduction of Model-Free PredictionEstimate the Value function of an unknown MDP4강 소감Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으로 정리가 가능한 방법이 TD인데 수식이 많이 어렵지는 않지만 강의에서는 생략된 곳도 많은 것 같다. 이 부분은 차후 책으로 증명같은 부분을 검토할 필요가 있을 것도 같다. 사실 직관적으로는 당연하고 이해하기 너무 쉬운 부분이지만 그냥 넘어가면 찝찝하니까… 이해가 잘 되면 필요에 따라 활용할 방법도 늘어나는 법이니까. 슬슬 용어들이 혼용되거나 혼동을 주는 부분들이 있는데 이것도 헷갈리지 않도록 신경써야겠다Monte-Carlo Reinforcement Learning MC methods learn directly from episodes of experience MC is model-free: no knowledge of MDP transitions / rewards MC learns from complete episodes: no booststrapping MC uses the simplest possible idea: value = mean return Caveat: can only apply MC to episodic MDPs All episodes must terminate 각 에피소드가 완료된 후에 그 Reward로부터 학습을 진행한다. bootstrapping은 불가능하고 각 에피소드는 종료되어야 한다. Expected return을 계산하는 것이 아닌 실제 반환되는 return으로 계산한다.First-Vist Monte-Carlo Policy Evaluation Goal: learn $\\mathsf{v_\\pi}$ from episodes of experience under policy $\\pi$\\[\\mathsf{ S_1, A_1, R_2, \\dots, S_k \\sim \\pi }\\] Monte-Carlo policy evaluation uses empirical mean return instead of expected return- state s 를 최초로 visit한 time t만 사용한다. counter $\\mathsf{N(s)}$ state 별로 따로 counter를 가진다는 뜻, 각 state를 update 할 때 쓰기 위함 Total return $\\mathsf{S(s) \\leftarrow S(s) + G_t}$ Estimated Value by mean return $\\mathsf{V(s) = S(s) /N}$ By law of large number, $\\mathsf{V(s) \\rightarrow v_\\pi(s) \\;as\\; N \\rightarrow \\infty}$ V는 mean return이고 v는 state return인데 이것도 어이없네…Every-Visit Monte-Carlo Policy Evaluation Goal: learn $\\mathsf{v_\\pi}$ from episodes of experience under policy $\\pi$\\[\\mathsf{ S_1, A_1, R_2, \\dots, S_k \\sim \\pi }\\] Monte-Carlo policy evaluation uses empirical mean return instead of expected return state s를 방문하는 모든 step t를 이용한다. counter $\\mathsf{N(s)}$ Total return $\\mathsf{S(s) \\leftarrow S(s) + G_t}$ Estimated Value by mean return $\\mathsf{V(s) = S(s) /N}$ By law of large number, $\\mathsf{V(s) \\rightarrow v_\\pi(s) \\;as\\; N \\rightarrow \\infty}$ 예시로 Blackjack 이 주어졌는데 사실 이 예시는 MDP로 정의되지 않음 ㅋㅋIncremental MeanThe mean $\\mu_1, \\mu_2, \\dots$ of a sequence $\\mathsf{x_1,x_2,\\dots}$ can be computed incrementally, $\\mu_k$ 는 sequence $\\mathsf{x_1, \\dots, x_k}$ 의 평균\\[\\begin{aligned}\\mu _k &amp;amp;=\\mathsf{\\frac{1}{k} \\displaystyle\\sum^{k}_{j=1} x_j =\\frac{1}{k} \\left( x_k + \\displaystyle\\sum^{k-1}_{j=1} x_j \\right)}\\\\&amp;amp;=\\mathsf{\\frac{1}{k} (x_k + (k-1)\\mu_{k-1})}\\\\&amp;amp;=\\mathsf{\\mu_{k-1}+\\frac{1}{k}(x_k-\\mu_{k-1})}\\\\\\end{aligned}\\]별거아닌 식인데 귀찮게 써버렸네Incremental Monte-Carlo Updates For each state $\\mathsf{S_t}$ with return $\\mathsf{G_t}$\\[\\mathsf{V(S_t) \\leftarrow V(S_t) + \\frac{1}{N} (G_t - V(S_t))}\\]S_t 흠…. 그리고 S total return이랑 겹치네 In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes\\[\\mathsf{V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))}\\] 1/N 이 아니라 $\\alpha$로 고정하는 경우 1/N보다 과거 정보 비중이 exponential로 줄어든다. 이것저것 해보는 느낌이군Temporal-Difference Learning TD methods learn directly from episodes of experience TD is model-free: no knowledge of MDP transitions / rewards TD learns from incomplete episodes, by bootstrapping TD updates a guess towards a guess 간단하게는 실시간으로 업데이트가 가능한 것도 장점이긴 하다 업데이트 방법을 다양하게 활용할 수 있는게 더 좋은듯MC and TD Goal: learn $\\mathsf{v_\\pi}$ online from experience under policy $\\pi$ Incremental every-visit Monte-Carlo Update value $\\mathsf{V(S_t)}$ toward actual return $\\mathsf{G_t}$ \\[\\mathsf{ V(S_t) \\leftarrow V(S_t) +\\alpha (G_t - V(S_t))}\\] Simplest temporal-difference learning algorithm: TD(0) Update value $\\mathsf{V(S_t)}$ toward estimated return $\\mathsf{R_{t+1} +\\gamma V(S_t)}$ \\[\\mathsf{ V(S_t) \\leftarrow V(S_t) +\\alpha (R_{t+1} +\\gamma V(S_t) - V(S_t))}\\] $\\mathsf{R_{t+1} +\\gamma V(S_{t+1})} $ is called the TD target $\\mathsf{ \\delta _t = R _{t+1} + \\gamma V ( S _{t+1})-V(S _t)}$ is called the TD errorAdvantages and Disadvantages of MC vs. TD TD can learn before knowing the final outcome TD can learn online after every step MC must wait until end of episode before reutrn is known TD can learn without the final outcome TD can learn from incomplete sequences MC can only learn from complete sequences TD works in continuing (non-terminating) environmnets MC only works for episodic (terminating) environments 장단점이라더니 다 TD 장점이네Bias/Variance Trade-Off Return $\\mathsf{G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma ^{T-1} R_T}$ is unbiased estimate of $\\mathsf{v_\\pi(S_t)}$ $\\mathsf{v_\\pi}$가 DP일 때랑 다른가? $\\mathsf{G_t}$는 단일 episode에서의 return이고 최적 value와는 다른 값이 나올수 있을것같은데… episode마다 다를거같은데 True TD target $\\mathsf{R_{t+1} + \\gamma v-\\pi(S_{t+1})}$ is unbiased estimate of $\\mathsf{v_\\pi(S_t)}$ 이것도 마찬가지… $v_\\pi$는 TD target의 expectation이지 모든 episode에서 같아지는 건 아닐거 같은데… estimate라고 해도 말이 거꾸로 뒤집한거같고.. $v_\\pi$가 target의 estimate 아닌가 TD target $\\mathsf{R_{t+1} + \\gamma V(S_{t+1})}$ is biased estimate of $\\mathsf{v_\\pi(S_t)}$ 수렴할 때 bias가 생기는 건 흔한 일이긴 한데 그 뜻 맞나 TD target is much lower variance than the return: Return depends on many random actions, transitions, rewards TD target depends on one random action, transition, reward return은 최종결과까지 다 고려하니까 variance가 큰 건 당연Advantages and Disadvantages of MC vs. TD 2 MC has gigh variance, zero bias Good convergence properties (even with function approximation) Not very sensitive to initial value Very simple to understand and use TD has low variance, some bias Usually more efficient than MC TD(0) converges to $\\mathsf{v_\\pi(s)}$ (but not always with function approximation) More sensitive to initial value MC와 결국 통하는거 같은데 왜 bias가 생기는 걸까Batch MC and TD MC and TD converge: $\\mathsf{V(s) \\rightarrow v_\\pi(s)}$ as experience $\\rightarrow \\infty$ But what about batch solution for finite experience?\\[\\begin{aligned}&amp;amp;\\mathsf{s^1_s, a^1_1, r^1_2, \\dots , s^1_{T_1}} \\\\&amp;amp;\\quad\\quad\\vdots \\\\&amp;amp;\\mathsf{s^K_s, a^K_1, r^K_2, \\dots , s^K_{T_K}}\\end{aligned}\\] e.g Repeatedly sample episode $\\mathsf{k\\in[1,K]}$ Apply MC or TD(0) to episode $\\mathsf{k}$Certainty Equivalence EC converges to solution with minimum mean-squared error Best fit to the observed returns\\(\\displaystyle\\sum^K_{k=1} \\sum^{T_k}_{t=1}(G^k_t - V (s^k_t))^2\\) TD(0) converges to solution of max likelihood Markov model Solution to the MDP $\\mathcal{\\langle S, A, \\hat{P}, \\hat{R}, \\gamma \\rangle}$ that best fits the data\\(\\begin{aligned}\\mathsf{\\hat{\\mathcal{P}}^a_{s,s&#39;}} &amp;amp;= \\mathsf{\\frac{1}{N(s,a)} \\displaystyle\\sum^K_{k=1}\\sum^{T_k}_{t=1}1(s^k_t, a^k_t, s^k_{t+1} = s, a, s&#39;)} \\\\ \\mathsf{\\hat{\\mathcal{R}}^a_s} &amp;amp;= \\mathsf{\\frac{1}{N(s,a)} \\displaystyle\\sum^K_{k=1}\\sum^{T_k}_{t=1}1(s^k_t, a^k_t = s, a)r^k_t}\\end{aligned}\\) Advantages and Disadvantages of MC vs. TD 3 TD exploits Markov property Usually more efficient in Markov environments MC does not exploit Markov property Usually more effective in non-Markov environmnets Bootstrapping and Sampling Bootstrapping: update involves an estimate MC does not bootstrap DP bootstraps TD bootstraps Sampling: update samples an expectation MC samples DP does not sample TD samples TD ($\\lambda$)n-Step Prediction TD target look n steps into the future $\\infty$-step prediction goes to MCn-Step Return Consider the following n-step returns for n=1,2,$\\infty$:\\[\\begin{aligned}\\mathsf{n=1\\quad (TD)\\quad} &amp;amp; \\mathsf{G^{(1)}_t = R_{t+1} + \\gamma V(S_{t+1})} \\\\\\mathsf{n=2\\quad \\quad\\quad\\quad\\,} &amp;amp; \\mathsf{G^{(2)}_t = R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 V(S_{t+2})} \\\\\\vdots \\quad\\quad\\quad\\quad\\quad\\; &amp;amp; \\;\\, \\vdots \\\\\\mathsf{n=\\infty \\;\\, (MC)\\quad} &amp;amp; \\mathsf{G^{(\\infty)}_t = R_{t+1} + \\gamma R_{t+2} +\\dots + \\gamma ^{T-1} R_T}\\end{aligned}\\] Define the n-step return\\[\\mathsf{G^{(n)}_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma ^{n-1} R_{t+n} + \\gamma ^n V(S_{t+n})}\\] n-step temporal-difference learning\\[\\mathsf{ V(S_t) \\leftarrow V(S_t) +\\alpha (G^{(n)}_t - V(S_t))}\\]Averaging n-Step Returns We can average n-step returns over different n e.g. average the 2-step and 4-step returns\\[\\mathsf{\\frac{1}{2} G^{(2)} + \\frac{1}{2} G^{(4)} }\\] Combines information from two different time-steps Can we efficiently combine information from all time-steps?$\\lambda$-return The $\\lambda$-return $\\mathsf{G^\\lambda _t }$ combines all n-step reutrns $\\mathsf{G^{(n)}_t}$ Using weight $\\mathsf{(1-\\lambda)\\lambda ^{n-1}}$\\[\\mathsf{G^\\lambda_t = (1-\\lambda) \\displaystyle\\sum^\\infty_{n=1} \\lambda^{n-1} G^{(n)}_t}\\] discount $\\lambda$를 주면서 평균을 구하는 방법임 Forward-view TD($\\lambda$) \\[\\mathsf{ V(S_t) \\leftarrow V(S_t) +\\alpha (G^\\lambda_t - V(S_t))}\\] 정체 평균 G 에 대하여 update 함TD($\\lambda$) Weighting Function\\[\\mathsf{G^\\lambda_t = (1-\\lambda) \\displaystyle\\sum^\\infty_{n=1} \\lambda^{n-1} G^{(n)}_t}\\]Forward-view TD($\\lambda$) update value function towards the $\\lambda$-return Forward-view looks into the future to compute $mathsf{G^\\lambda_t}$ Like MC, can only be computed from episodesBackward View TD($\\lambda$) Forward view provides theory Backward view provides mechanism Update online, every step, from incomplete sequences Keep an eligibility trace for every state s Update value $\\mathsf{V(s)}$ for every state $\\mathsf{s}$\\[\\begin{aligned}\\delta_t &amp;amp;= \\mathsf{R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)}\\\\\\mathsf{V(s)} &amp;amp;\\leftarrow V(s) + \\alpha\\delta _t E_t(s)\\end{aligned}\\]TD($\\lambda$) and TD(0) When $\\lambda = 0$, only current state is updated\\[\\begin{aligned}\\mathsf{E_t(s)} &amp;amp;= \\mathsf{1(S_t = s)}\\\\\\mathsf{V(s)} &amp;amp;\\leftarrow V(s) + \\alpha\\delta _t E_t(s)\\end{aligned}\\] This is exactly equivalent to TD(0) update\\[\\mathsf{V(s) \\leftarrow V(s) + \\alpha\\delta _t}\\]TD($\\lambda$) and MC When $\\lambda =1$, credit is deferred until end of episode Consider episodic environmnets with offline updates Over the course of an episode, total update for TD(1) is the same as total update for MC 이유가 생략되었는데 나중에 책봐야할듯 Theorem The sum of offline updates is identical for forward-view and backward-view TD($\\lambda$)$$ \\mathsf{\\displaystyle\\sum ^T_{t=1} \\alpha\\delta_t E_t(s) = \\sum^T_{t=1} \\alpha \\left( G^\\lambda_t - V(S_t)\\right)1(S_t=s)} $$ 강의는 여기까지MC and TD(1) Consider an episode where $\\mathsf{s}$ is visited once at time-step k, TD(1) eligibility trace discounts time since visit,\\[\\begin{aligned}\\mathsf{E_t(s)} &amp;amp;= \\mathsf{ \\gamma E_{t-1}(s) + 1(S_t =s)} \\\\&amp;amp;= \\begin{cases}\\; 0 &amp;amp; \\mathsf{if\\; t&amp;lt; k} \\\\ \\;\\mathsf{\\gamma ^{t-k}} &amp;amp; \\mathsf{if\\; t\\geq k} \\end{cases}\\end{aligned}\\] TD(1) updates accumulate error online\\[\\mathsf{\\displaystyle \\sum^{T-1}_{t=1} \\alpha\\delta_t E_t(s) = \\alpha \\sum^{T-1}_{t=k} \\gamma^{t-k}\\delta_t = \\alpha(G_k - V(S_k))}\\] By end of episode it accumulates total error\\[\\mathsf{\\delta_k + \\gamma\\delta_{k+1} + \\gamma^2\\delta_{k+2} + \\dots + \\gamma^{T-1-k} \\delta_{T-1}}\\]Telescoping in TD(1)When $\\lambda = 1$, sum of TD errors telescopes into MC error,\\[\\begin{aligned}&amp;amp; \\mathsf{\\delta_k + \\gamma\\delta_{k+1} + \\gamma^2\\delta_{k+2} + \\dots + \\gamma^{T-1-k} \\delta_{T-1}} \\\\=&amp;amp; \\mathsf{R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)} \\\\+&amp;amp; \\gamma \\mathsf{R_{t+2} + \\gamma^2 V(S_{t+2}) - \\gamma V(S_{t+1})} \\\\+&amp;amp; \\gamma^2 \\mathsf{R_{t+3} + \\gamma^3 V(S_{t+3}) - \\gamma^2 V(S_{t+2})} \\\\&amp;amp; \\quad \\vdots \\\\+&amp;amp; \\gamma^{T-1-t} \\mathsf{R_T + \\gamma^{T-t} V(S_T) - \\gamma^{T-1-t} V(S_{T-1})} \\\\=&amp;amp; \\mathsf{R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-1-t} R_T - V(S_t)} \\\\=&amp;amp; \\mathsf{G_t - V(S_t)}\\end{aligned}\\]TD($\\lambda$) and TD(1) TD(1) is roughly equivalent to every-visit Monte-Carlo Error is accumulated online, step-by-step If value function is only updated offline at end of episode Then total update is exactly the same as MCTelescoping in TD($\\lambda$)For general $\\lambda$, TD errors also telescope to $\\lambda$-error, $\\mathsf{G^\\lambda_t - V(S_t)} $\\[\\begin{aligned}\\mathsf{G^\\lambda _t - V(S_t) }=&amp;amp; \\mathsf{-V(S_t) + (1-\\lambda)\\lambda^0(R_{t+1} + \\gamma V(S_{t+1})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (1-\\lambda)\\lambda^1 (R_{t+1}+\\gamma R_{t+2} + \\gamma^2 V(S_{t+2})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (1-\\lambda)\\lambda^2 (R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3} + \\gamma^3 V(S_{t+3})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,+\\,\\dots\\\\=&amp;amp; \\mathsf{-V(S_t) + (\\gamma\\lambda)^0(R_{t+1} + \\gamma V(S_{t+1}) -\\gamma\\lambda V(S_{t+1})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (\\gamma\\lambda)^1 (R_{t+2}+ \\gamma V(S_{t+2}) -\\gamma\\lambda V(S_{t+2})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (\\gamma\\lambda)^2 (R_{t+3}+ \\gamma V(S_{t+3}) -\\gamma\\lambda V(S_{t+3})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,+\\,\\dots\\\\=&amp;amp; \\quad\\quad\\quad\\quad\\;\\, \\mathsf{ (\\gamma\\lambda)^0(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (\\gamma\\lambda)^1 (R_{t+2}+ \\gamma V(S_{t+2}) -V(S_{t+1})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,\\mathsf{+\\, (\\gamma\\lambda)^2 (R_{t+3}+ \\gamma V(S_{t+3}) -V(S_{t+2})) } \\\\&amp;amp; \\quad\\quad\\quad\\;\\,+\\,\\dots\\\\=&amp;amp;\\;\\mathsf{\\delta _t + \\gamma\\lambda\\delta _{t+1} + (\\gamma\\lambda)^2 \\delta_{t+2} + \\dots}\\end{aligned}\\]Forward and Backwards TD($\\lambda$) Consider an episode where s is visited once at time-step k TD($\\lambda$) eligibility trace discounts time since visit,\\[\\begin{aligned}\\mathsf{E_t(s)} &amp;amp;= \\mathsf{ \\gamma\\lambda E_{t-1}(s) + 1(S_t =s)} \\\\&amp;amp;= \\begin{cases}\\; 0 &amp;amp; \\mathsf{if\\; t&amp;lt; k} \\\\ \\;\\mathsf{(\\gamma\\lambda)^{t-k}} &amp;amp; \\mathsf{if\\; t\\geq k} \\end{cases}\\end{aligned}\\] Backward TD($\\lambda$) updates accumulate error online\\[\\mathsf{\\displaystyle \\sum^T_{t=1} \\alpha\\delta_t E_t(s) = \\alpha \\sum^T_{t=k} (\\gamma\\lambda)^{t-k}\\delta_t = \\alpha(G^\\lambda_k - V(S_k))}\\] By end of episode it accumulates total error for $\\lambda$-return For multiple visits to $\\mathsf{s, E_t(s)}$ accumulates many errorsOffline Equivalence of Forward and Backward TDOffline updates Updates are accumulated within episode but applied in batch at the end of episodeOnline updates TD($\\lambda$) updates are applied online at each step within episode Forward and backward-view TD($\\lambda$) are slightly different NEW: Exact online TD($\\lambda$) achieves perfect equivalene By using a slightly different form of eligibility traceSummary of Forward and Backward TD($\\lambda$) Offline updates $\\lambda=0$ $\\lambda \\in (0,1)$ $\\lambda =1$ Backward view Forward view TD(0) $\\parallel$ TD(0) TD($\\lambda$) $\\parallel$ Forward TD($\\lambda$) TD(1) $\\parallel$ MC Online updates $\\lambda=0$ $\\lambda \\in (0,1)$ $\\lambda =1$ Backward view Forward view Exact Online TD(0) $\\parallel$ TD(0) $\\parallel$ TD(0) TD($\\lambda$) $\\nparallel$ Forward TD($\\lambda$) $\\parallel$ Exact Online TD($\\lambda$) TD(1) $\\nparallel$ MC $\\nparallel$ Exact Online TD(1) here indicates equivalence in total update at end of episode." }, { "title": "RLcourse note - Lecture 3 Planning by Dynamic Programming", "url": "/posts/RL-course-Note-3/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-12-02 10:00:00 +0900", "snippet": "Video Link :Introduction of Dynamic ProgrammingDynamic sequential or temporal component to the problemProgramming optimising a “program”, i.e. a policy A method for solving complex problems By breaking them down into subproblems Solve the subproblems Combine solutions to subproblems Dynamic programming이라 이름이 흠 어울리는것 같지 않은데3강 소감3강은 Bellman equation에 기초하여 policy와 value를 iteration으로 계산하는 기본적인 논리를 보여준다. 기초적인 방법인 만큼 조금 더 깔끔하게 정리가 되어 있으면 좋겠지만 적당히 개념위주로 보여주고 수학적 논리적 기반은 가볍게 넘어가는것 같거나 조금 허점?이 있는 것도 같다. 어쨌거나 학습 문제를 풀기 위해 정의한 state, policy, action, value의 첫번째 활용이니 이번엔 그냥저냥 넘어가볼까.Requirements for Dynamic ProgrammingDynamic Programming is a very general solution method for problems which have two properties: Optimal substructure Principle of optimality applies Optimal solution can be decomposed into subproblems Overlapping subproblems Subproblems recure many times Solutions can be cached and reused Markov decision processes satisfy both properties Bellman equation gives recursive decomposition Value function stores and reuses solutions Planning by Dynamic Programming Dynamic programming assumes full knowledge of the MDP It is used for planning in an MDP For prediction: Input: MDP $\\mathcal{\\langle S, A, P, R, \\gamma \\rangle} $ and policy $\\pi$        or: MRP $\\mathcal{\\langle S, P^{\\pi}, R^{\\pi}, \\gamma \\rangle}$ Output: value function $\\mathsf{v_{\\pi}}$ Or for control: Input: MDP $\\mathcal{\\langle S, A, P, R, \\gamma \\rangle} $ Output: optimal value function $\\mathsf{v_*}$        and: optimal policy $\\pi_*$ Iterative Policy EvaluationTo evaluate given policy $\\pi$, iterative application of Bellman expectation backup is appliedReward를 모르는 것(0)으로 간주하고 backup을 반복하여 Reward 계산 - Policy 변경 없음synchronous backup At each iteration $k+1$, For all states $\\mathsf{s} \\in \\mathcal{S}$ Update $\\mathsf{v_{k+1}(s)}$ from $\\mathsf{v_{k}(s’)}$ where $\\mathsf{s’}$ is a successor state of s\\[\\begin{aligned}\\mathsf{v_{k+1}(s)} &amp;amp;= \\mathsf{\\displaystyle\\sum_{a \\in \\mathcal{A}}\\pi(a \\vert s)\\left( \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39; \\in \\mathcal{S}} \\mathcal{P}^a_{ss&#39;}v_k(s&#39;)\\right)} \\\\\\mathsf{\\boldsymbol{v}^{k+1}} &amp;amp;= \\mathsf{\\mathcal{\\boldsymbol{R}}^{\\pi} + \\gamma \\mathcal{\\boldsymbol{P} }^{\\pi} v^k}\\end{aligned}\\] $\\mathcal{R}$ 은 $\\sum$ ( policy * action-reward ) $\\mathcal{P}$ 는 $\\sum$ ( policy * $\\sum$ ( Probability of transition from s to s’ by action * value of s’ state ) ) 단순화 하겠다고 쫌 너무 막나가는거 아니냐Policy Iteration Given a policy $\\pi$ Evaluate the policy $\\pi$ \\[\\mathsf{v_{\\pi} (s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\dots \\vert S_t = s \\right]}\\] Improve the policy by acting greedily with respect to $\\mathsf{v_{\\pi}}$ - 그냥 value 높은 쪽으로 가게 하는 policy라는 뜻\\[\\mathsf{\\pi &#39; = greedy(v_{\\pi})}\\] This process of policy iteration always converges to $\\pi_*$Policy Improvement Consider a determinisitc policy, $\\mathsf{a = \\pi (s)}$ Improve the policy by acting greedily\\[\\mathsf{\\pi &#39; (s) = \\underset{a \\in \\mathcal{A}}{argmax}\\; q_{\\pi} (s,a)}\\] This improves the value from any state $\\mathsf{s}$ over one step\\[\\mathsf{q_{\\pi} (s, \\pi &#39; (s)) = \\underset{a \\in \\mathcal {A}}{mas} \\; q_{\\pi}(s,a) \\geq q_{\\pi}(s,\\pi (s)) = v_{\\pi} (s)}\\] This imporves the value from any state s over one step\\[\\mathsf{q_{\\pi} (s, \\pi &#39;(s)) = \\underset{a \\in \\mathcal{A}}{max} \\; q_{\\pi} (s,a) \\geq q_\\pi (s,\\pi(s)) = v_\\pi (s)}\\] 이 action value와 state value를 동일시하는 이 부분은 deterministic일 때만 성립한다. It therefor imporves the value function, $\\mathsf{v_{\\pi’}(s) \\geq v_\\pi(s)}$ \\[\\begin{aligned}\\mathsf{v_\\pi(s)} &amp;amp;\\leq \\mathsf{q_\\pi(s,\\pi&#39;(s)) = \\mathbb{E}_{\\pi&#39;} \\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1})\\;\\vert\\;S_t = s \\right]} \\\\&amp;amp;\\leq \\mathsf{\\mathbb{E}_{\\pi&#39;} \\left[ R_{t+1} + \\gamma q_\\pi (S_{t+1}, \\pi &#39;(S_{t+1}))\\;\\vert\\;S_t = s \\right]}\\\\&amp;amp;\\leq \\mathsf{\\mathbb{E}_{\\pi&#39;} \\left[ R_{t+1} + \\gamma R_{t+2} +\\gamma ^2q_\\pi (S_{t+2}, \\pi &#39;(S_{t+2}))\\;\\vert\\;S_t = s \\right]}\\\\&amp;amp;\\leq \\mathsf{\\mathbb{E}_{\\pi&#39;} \\left[ R_{t+1} + \\gamma R_{t+2} + \\dots \\;\\vert\\;S_t = s \\right] = v_{\\pi &#39;} (s)}\\end{aligned}\\] Deterministic인 경우에 한정하는 식 전개로써 $\\mathsf{R}$은 $\\mathbb{E}$내부에 있을 필요가 없다. 이 강의의 수식부분들이 때때로 찜찜한 이유는 이런 부분을 명확히 하지 않기 때문이다. 수학적인 논리 전개를 좀 제대로 해주면 좋겠다. small grid 같은 예제는 deterministic이 아니기 때문에 이 경우에 적절하지않고 $\\mathsf{v}$가 점점 작아질 수 있는 이유가 deterministic이 아니기 때문에 위 식이 성립하지 않기 때문이다. 이 방법으로 반복하면 deterministic이 아니어도 optimal에 가까워질 수 있을까? 반례가 있을지 궁금하긴 한데 강의 다 듣고나서로 미뤄두겠다. If improvements stop, \\[\\mathsf{q_{\\pi} (s, \\pi &#39;(s)) = \\underset{a \\in \\mathcal{A}}{max} \\; q_{\\pi} (s,a) = q_\\pi (s,\\pi(s)) = v_\\pi (s)}\\] Then the Bellman optimality equation has been satisfied\\[\\mathsf{ v_\\pi(s) = \\underset{a \\in \\mathcal{A}}{max} \\; q_\\pi(s,a)}\\] 이 정의는 적절하지 않다. $\\mathsf{v}$가 최대 action value일 때라고 하는건 바보같은 정의이며 그보다는 Value Iteration을 반복해도 Value가 바뀌지 않으며 모든 state에서 Bellman expectation과 현재 value가 같을 때 optimality가 성립한다고 하는게 적절할 것이다. Therefore $\\mathsf{v_\\pi(s)=v_*(s)}$ for all $\\mathsf{s} \\in \\mathcal{S}$ so $\\pi$ is an optimal policy 주어진 Policy에 대해서 Value를 계산한다. 그 방법 중 대표로써 Bellman Expectation Backup 을 이용해 Iterative한 Value를 계산하는데 이 방법이 무조건 optimal 해질 수 있는지에 대해서는 Deterministic한 경우에 대해서만 증명이 된 것이다.Modified Policy Iteration Does policy evaluation need to converge to $\\mathsf{v_\\pi}$? Or should we introduce a stopping condition e.g. $\\epsilon$-convergence of value function Or simply stop after $\\mathsf{k}$ iterations of iterative policy evaluation? Why not update policy every iteration? This is equivalent to value iteration Principle of OptimalityAn optimal policy can be subdivided into two components: An optimal first action $\\mathsf{A_*}$ Followed by an optimal policy from successor state $\\mathsf{S’}$ Theorem (Principle of Optimality) A policy $\\pi\\mathsf{(a \\vert s)} $ achieves the optimal value from state s, $\\mathsf{v_\\pi (s) = v_* (s)}$, if and only if$\\quad\\bullet\\;\\;$ For any state $\\mathsf{s’}$ teachable from $\\mathsf{s}$$\\quad\\bullet\\;\\;$ $\\pi$ achieves the optimal value from state $\\mathsf{s’, v_\\pi(s’)=v_*(s’)}$ 이 Theorem은 당연한 소리를 써놓은거 같은데 왜 Theorem인지는 나중에 생각해볼 필요가 있겠다.Value IterationDeterministic Value Iteration If we know the solution to subproblems $\\mathsf{v_*(s’)}$ solution $\\mathsf{v_*(s)}$ can be found by one-step lookahead\\[\\mathsf{v_*(s) \\leftarrow \\underset{a \\in \\mathcal{A}}{max} \\; \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39; \\in \\mathcal{S}}\\mathcal{P}^a_{ss&#39;}v_*(s&#39;)}\\] The idea of value iteration is to apply these updates iteratively Intuition: start with final rewards and work backwards Still works with loopy, stochastic MDPs 증명이 복잡해서 보여주지 않는건가 싶지만…. 앞에 stochastic으로 계산하던 거에서 deterministic으로 backup만 바뀌었다. policy iteration Value iteration 큰 차이도 없는것 같다. 그저 policy를 명시하지 않았을 뿐. greedy만 했던 poicy는 있으나 없으나 차이를 모르겠다.Value Iteration - Define Probleam: find optimal policy $\\pi$ Solution: ierative application of Bellman optimality backup $\\mathsf{v_1 \\rightarrow v_2 \\rightarrow \\dots \\rightarrow v_*}$ Using synchronous backups At each iteration $\\mathsf{k+1}$ For all states $\\mathsf{s \\in \\mathcal{S}}$ Update $\\mathsf{v_{k+1}(s)}$ from $\\mathsf{v_k(s’)}$ Convergence to $\\mathsf{v_*}$ will be proven later Unlike policy iteration, there is no explicit policy Intermediate value functions may not correspond to any policy\\[\\begin{aligned}\\mathsf{v_{k+1}(s)} &amp;amp;= \\mathsf{\\underset{a \\in \\mathcal{A}}{max}\\left( \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39; \\in \\mathcal{S}} \\mathcal{P}^a_{ss&#39;}v_k(s&#39;)\\right)} \\\\\\mathsf{\\boldsymbol{v}_{k+1}} &amp;amp;= \\mathsf{\\underset{a \\in \\mathcal{A}}{max}\\;\\mathcal{\\boldsymbol{R}}^a+ \\gamma \\mathcal{\\boldsymbol{P} }^a v^k}\\end{aligned}\\]Extensions to Dynamic ProgrammingSynchronous Dynamic Programming Algorithms Problem Bellman Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration Algorithms are based on state-value function $\\mathsf{v_\\pi(s)}$ or $\\mathsf{v_*(s)}$ Complexiy $\\mathcal{O}\\mathsf{mc^2}$ per iteration, for $\\mathsf{m}$ actions and $\\mathsf{n}$ states Could also apply to action-valuefunction $\\mathsf{q_\\pi(s,a)}$ or $\\mathsf{q_*(s,a)}$ Complexity $\\mathcal{O}\\mathsf{(m^2n^2)}$ per iteration Expectation에서는 Stochastic 할수 있는 경우를 포함하여 Value를 계산하고 Control 하고 싶은경우 policy를 greedy 로 deterministic하게 바꿔주는 것이고 Optimality에서는 policy를 계속 deterministic으로 간주하고 value를 계산하고 policy를 바꿔주는 거라서 식 자체는 거의다 그대로인데 policy를 바꾸지 않는게 prediction이고 Control은 deterministic하게 해주는 것 뿐이다. Policy는 기본적으로 deterministic을 가정하고 하는건가? Probability는 내부적으로 유지한 채로 value를 계산하면서 policy를 greedy하게 설정해주는것과 얼마나 다를지 생각해볼 필요도 있겠다.ASynchronous Dynamic Programming DP methods described so far used synchronous backups i.e. all states are backed up in parallel Asynchronous DP backs up states individually, in any order For each selected state, apply the appropriate backup Can significantly reduce computation Guaranteed to converge if all states continue to be selected Update의 방법과 순서를 모델링함으로써 성능 개선Three simple ideas for asynchronous dynamic programming: In-placedynamic programming Prioritised sweeping Real-time dynamic programmingIn-Place Dynamic Programming Synchronous value iteration stores two copies of value functionfor all $\\mathsf{s}$ in $\\mathcal{S}$\\[\\mathsf{v_{new}(s)\\leftarrow\\underset{a\\in\\mathcal{A}}{max}\\;\\left(\\mathcal{R}^a_s+\\gamma\\displaystyle\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}^a_{ss&#39;}v_{old}(s&#39;)\\right)}\\]\\[\\mathsf{ v_{old} \\leftarrow v_{new} }\\] In-place value iteration only stores one copy of value functionfor all $\\mathsf{s}$ in $\\mathcal{S}$\\[\\mathsf{v(s)\\leftarrow\\underset{a\\in\\mathcal{A}}{max}\\;\\left(\\mathcal{R}^a_s+\\gamma\\displaystyle\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}^a_{ss&#39;}v(s&#39;)\\right)}\\]Value function을 즉각 교체Prioritised Sweeping Use magnitude of Bellman error to guide state selection, e.g.\\[\\mathsf{\\left\\vert \\underset{a\\in\\mathcal{A}}{max}\\; \\left( \\mathcal{R}^a_s+\\gamma \\displaystyle\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}^a_{ss&#39;}v(s&#39;) \\right) - v(s) \\right\\vert }\\] Backup the state with the largest remaining Bellman error Update Bellman error of affected states after each backup Requires knowledge of reverse dynamics (predecessor states) Can be implemented efficiently by maintaining a priority queueBellman error가 큰 state부터 backupReal-Time dynamic Programming Idea: only states that are relevant to agent Use agent’s experience to guide the selection of states After each time-step $\\mathsf{S_t, A_t, R_{t+1}}$ Backup the stae $\\mathsf{S_t}$\\[\\mathsf{v(S_t)\\leftarrow\\underset{a\\in\\mathcal{A}}{max}\\;\\left(\\mathcal{R}^a_{S_t}+\\gamma\\displaystyle\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}^a_{S_t s&#39;}v(s&#39;)\\right)}\\]Agent가 사용한 state들을 backupFull-Width Backups DP uses full-width backups For each backup (sync or async) Every succesor state and action is considered Using knowledge of the MDP transitions and reward function DP is effective for medium-sized probelms (millons of states) For large problems DP suffers Bellman’s curse of dimensionality Number of states $\\mathsf{n= \\vert \\mathcal{S} \\vert}$ grows exponentially with number of state variables Even one backup can be too expensive모든 state의 전이를 고려해서 backup을 하는 것은 비용이 너무 크다.Sample Backups Using sample rewards and sample transitions $\\mathsf{ \\langle S, A, R, S’ \\rangle}$ Instead of reward function $\\mathcal{R}$ and transition dynamics $\\mathcal{P}$ Advantages: Model-free: no advance knowledge of MDP required Break the curse of dimensionality through sampling Cost of backup is constant, independent of $\\mathsf{n} = \\vert \\mathcal{S} \\vert$ 모델을 몰라도 sample만 이용해서 backup. 단점은?여기부터는 강의에서는 다루지 않음Approximate Dynamic Programming Approximate the value function Using a function approximator $\\mathsf{\\hat{v}(s,\\boldsymbol{w})}$ Apply dynamic programming to $\\mathsf{\\hat{v}(\\cdot ,\\boldsymbol{w})}$ e.g. Fitted VAlue Iteration repeats at each iteration $\\mathsf{k}$, Sample states $\\mathcal{\\tilde{S} \\subseteq S}$ For each state $\\mathcal{s \\in \\tilde{S}}$, estimate target value using Bellman optimality equation,\\(\\mathsf{\\tilde{v}_k(s)=\\underset{a\\in\\mathcal{A}}{max}\\;\\left(\\mathcal{R}^a_s+\\gamma\\displaystyle\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}^a_{ss&#39;}\\hat{v}(s&#39;, \\boldsymbol{w_k})\\right)}\\) Train next value function $\\mathsf{\\hat{v}(s’, \\boldsymbol{w_{k+1}})}$ using targets $\\mathsf{ \\lbrace \\langle s,\\tilde{v}_k(s)\\rangle\\rbrace}$ function approximator가 뭔지는 써줘야 하는거 아니냐Contraction Mappingcontraction mapping theorem resolve convergence of value iteration $v_*$, policy evaluation $v_\\pi$, policy iteration, uniqueness of solution, convergence speed.수렴성과 uniqueness를 보여주겠다 함Value Function Space Consider the vector space $\\mathcal{V} $ over value functions, $\\mathcal{\\vert S \\vert}$ dimensions Each point in this space fully specifies a value function $\\mathsf{v(s)}$ Bellman backup brings value function closer therefore backup must converge on a unique solutionUnique optimal solution을 포함하는 value function spaceValue Function $\\infty$-Norm distance between state-value functions $\\mathsf{u}$ and $\\mathsf{v}$ by $\\infty$-norm is largest difference\\[\\mathsf{\\Vert u-v\\Vert_\\infty = \\underset{s\\in\\mathcal{S}}{max}\\;\\vert u(s) -v(s)\\vert}\\]Error가 제일 큰게 $\\infty$-normBellman Expectation Backup is a Contraction Define the Bellman expectation backup operator $\\mathsf{T^\\pi}$,\\[\\mathsf{T^\\pi(v) = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v}\\] This operator is a $\\gamma$-contraction, i/e/ it makes value functions closer by at least $\\gamma$,\\[\\begin{aligned}\\mathsf{\\Vert T^\\pi(u) - T^\\pi (v) \\Vert_\\infty} &amp;amp;= \\mathsf{\\Vert (\\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi u) - (\\mathcal{R}^\\pi + \\gamma\\mathcal{P}^\\pi v)\\Vert_\\infty} \\\\&amp;amp;= \\mathsf{\\Vert\\gamma\\mathcal{P}^\\pi (u-v) \\Vert_\\infty} \\\\&amp;amp;\\leq \\mathsf{\\Vert\\gamma\\mathcal{P}^\\pi \\Vert u-v \\Vert_\\infty \\Vert_\\infty} \\\\&amp;amp;\\leq \\mathsf{\\gamma \\Vert u-v \\Vert_\\infty} \\\\\\end{aligned}\\]backup을 통해 항상 최소한 최대 error의 $\\gamma$배 만큼 가까워진다. 근데 식은 u의 value와 v의 value를 backup했을 때 value 차이가 가장 큰 것이 backup 하기 전 u와 v의 value 차이가 가장 큰 곳의 $\\gamma$배 보다 작거나 같다는 뜻. $\\mathcal{R}$이 소거된 걸 보면 u와 v는 action reward가 같으니 동일 state 및 동일 action이고 actino이 바뀌지 않는다는 건 policy evaluation에 한하는 증명이다. u가 optimal value라고 가정해보면 v가 점점 optimal에 가까워진다는 뜻이기는 하다. 하지만 표기를 보면 그것만 의도한 것은 아닌 것 같다. 그러나 만약 서로 다른 state라면 R이 소거되지 않으니 성립하지 않는다. 거기다가 optimal할 때와 action이 같다고 가정해야하니 그것마저 버리면 evaluation 외에는 정말 무의미한 일이 되어버린다. 아이고…. 여기도 강의 다 보고나서 다시 한번 볼까… 아니면 이게 나왔다는 리처드 서튼 Reinforcement learning an introduction 책이라도 찾아봐야 하나Contraction Mapping Theorem Theorem (Contraction Mapping Theorem) For any metric space $\\mathcal{V}$ that is complete (i.e. closed) under an operator $\\mathsf{T(v)}$, where T is a $\\gamma$-contraction,$\\quad\\bullet\\;\\;\\mathsf{T}$ converges to a unique fixed point$\\quad\\bullet\\;\\;$ At a linear convergence rate of $\\gamma$ Convergence of Iter. Policy Evaluation and Policy Iteration The Bellman expectation operator $\\mathsf{T^\\pi}$ has a unique fixed point $\\mathsf{v_\\pi}$ is a fixed point of $\\mathsf{T^\\pi}$ (by Bellman expectation equation) By contraction mapping theorem Iterative policy evaluation converges on $\\mathsf{v_\\pi}$ Policy iteration converges on $\\mathsf{v_*}$Bellman Optimality Backup is a Contraction Define the Bellman optimality backup operator $\\mathsf{T^*}$,\\[\\mathsf{T^* (v) = \\underset{a\\in\\mathcal{A}}{max}\\;\\mathcal{R}^a + \\gamma \\mathcal{P}^a v}\\] This operator is a $\\gamma$-contraction, i.e. it makes value functions closer by at least $\\gamma$ (similar to previous proof)\\[\\mathsf{\\Vert T^*(u)-T^* (v) \\Vert_\\infty \\leq \\gamma \\Vert u-v \\Vert_\\infty }\\]Convergence of Value Iteration The Bellman optimality operator $\\mathsf{T^*}$ has a unique fixed point $\\mathsf{v_}$ is a fixed point of $\\mathsf{T^}$ (by Bellman expectation equation) By contraction mapping theorem Value iteration converges on $\\mathsf{v_*}$" }, { "title": "RLcourse note - Lecture 2 Markov Decision Processes", "url": "/posts/RL-course-Note-2/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-11-27 10:00:00 +0900", "snippet": "Video Link :2강 소감 및 생각결국 이렇게 정의된 MDP, Bellman equation을 푸는 것. 일반적인 모든 문제에서 Reward라는 것과 Action이라는 것으로 최대한 수학적으로 해를 구하기 쉽도록 학습 문제를 정의하기 위한 첫번째 단계인 듯하다. 최대한 Linear하게 만들고 싶은 것. 그러나 나는 AI 학습이 모든 것을 Linear하게 정의하려고 노력하는 것이 모든 상황을 커버할 수 있을지는 모르겠다. 물론 대부분의 수학적 접근은 Convolution과 Linear한 식의 combination으로 표현될 수 있다고 대충 느낌상 기억하고 있긴 하다만 그러면 Convolution과 Dense layer를 잘 구성해야 하며 수많은 Unrelated 변수가 생겨난다. 물론 그를 통해서 미처 파악하지 못한 변수를 이용해 추상적 접근을 모델링해내는 것이 가능할 수도 있다. 그러나 이 부분을 구현하기 위해서 Laplace transform과 거기서 표현되는 여러가지 수학적 접근을 구현할 수 있도록 Layer를 구현해야 할 것이다.Introduction ot MDPs Markov decision processes formally describe and envrionment for reinforcement learning Where the envrionment is fully observable i.e. The current state completely characterise the process - state는 현재 상태 변수들로 markov 상태가 될 수 있도록 충분한 변수들을 갖추어야 함 Almost all RL problems can be formalised as MDPs 뭐 잘 설계해서 MDP 맞추란 소린데 누가 이런말 못함Markov Property“The future is independent of the past given the present” 문제를 자꾸 단정하고 싶어한다. 그게 속도나 효율면에서 맞긴 하지만 이게 다른 문제를 만들진 않을까? Markov로 만들수 없는 경우도 있을 것 같지만 복잡하다고 불가능하진 않을것 같은데, 혹은 간혹 더 간단한 모델을 만들 수도 있지 않을까 Definition   A state \\(S_t\\) is Markov if and only if \\(\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, ...,S_t]\\) The state captures all relevant information from the history Once the state is known, the history may be thrown away 흐음… 그보다는 environment의 information에 적절히 history data를 축적한 변수를 만들어야 하지 않을까. 그 변수를 전부 현재의 변수라고 설정하는 것 뿐 현재의 envrionment에서 관찰할 수 있는 것은 아니지 않으려나.State Transition MatrixFor a Markov state s and successor state s’, the state transition probability is defined by,\\[\\mathcal{P}_{ss&#39;}=\\mathbb{P}[S_{t+1}=s&#39;|S_t =s]\\]State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s’$\\[\\mathcal{P} = from \\begin{bmatrix} \\mathcal{P}_{11}&amp;amp;\\dots&amp;amp;\\mathcal{P}_{1n}\\\\ \\vdots &amp;amp; &amp;amp; \\\\ \\mathcal{P}_{n1}&amp;amp;\\dots&amp;amp;\\mathcal{P}_{nn} \\end{bmatrix}\\] state 의 갯수가 n개로 제한된 경우에만 사용하는 건가? 확률로 state 변화가 표현된 만큼 action이 개입할 여지가 없다. 결국 이 확률 $\\mathcal{P}$ 는 이후 가변 성질의 policy로 대체되는 것 같다.Markov Process (1단계)A Markov process is a memoryless random process마르코브 형태의 프로세스에 대한 설명, 확률만 가지고 state의 진행만을 보여주므로 agent, reward 이런건 없음. memoryless 이전 state과 필요없음. random, sampling 할 수 있다. Definition a Markov Process (or Markov Chain) is a tuple $\\langle\\mathcal{S,P}\\rangle$$\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{S}}\\;$ is a (finite) set of states$\\tiny{\\blacksquare}\\quad\\normalsize{\\mathcal{P}}\\;$ is a state transition probability matrix, \\(\\quad\\mathcal{P}_{ss&#39;}=\\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) Markov Reward Process (MRP)A Markov reward process is a Markov chain with valuesReward와 Discount factor를 정의함. Definition a Markov Reward Process is a tuple $\\langle\\mathcal{S,P,}$$\\mathcal{R,\\gamma}$$\\rangle$$\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{S}}\\;$ is a (finite) set of states$\\tiny{\\blacksquare}\\quad\\normalsize{\\mathcal{P}}\\;$ is a state transition probability matrix, \\(\\quad\\mathcal{P}_{ss&#39;}=\\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) $\\tiny{\\blacksquare}\\quad$ $\\normalsize{\\mathcal{R}}\\;$ is a reward function, \\(\\mathcal{R}_s=\\mathbb{E}[R_{t+1} | S_t=s]\\)$\\tiny{\\blacksquare}\\quad$ $\\normalsize{\\mathcal{\\gamma}}\\;$ is a discount factor, \\(\\gamma\\in[0,1]\\) Return $G_t$ Definition The return $G_t$ is the total discounted reward from tiem-step $t$.$$G_t = R_{t+1}+\\gamma R_{t+2}+... = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$ The discount $\\gamma \\in [0,1]$ is the present value of future rewards The value of receiving reward $R$ after $k+1$ time-steps is $\\gamma^k R$ This values immediate reward above delayed reward $\\gamma$ close to 0 leads to “myopic” evaluation $\\gamma$ close to 1 leads to “far-sighted” evaluation 단순 급수적인 형태로 감가를 시행하고 있지만 경우에 따라서 감가 함수를 별도로 함수로 만들어 사용할 수 있을 듯. 크게 필요한 경우가 있을 진 모르겠지만.. 여기부터 이상해 $R_t$는 어디간거야 계산할 땐 다 쓰면서… 별로 중요한건 아닌데 거슬리네Reason of discount 수학적으로 편리함 무한히 증가하는 return을 제거 state가 항상 terminate 될 것이 보장된다면 discount를 1로 해도 괜찮을 수 있음Value FunctionThe value function $v(s)$ gives the long-term value of state $s$기댓값(평균) Definition The state vlaue function $v(s)$ of an MRP is the expected return starting from state $s$ $$\\mathsf{v}(s)=\\mathbb{E}[G_t\\;|\\;S_t=s] $$ Bellman Equation for MRPsThe value function can be decomposed into two parts:Value function을 학습시키기 위한 기본 논리근데 이상한게 $R_{t+1}$ 써놓고 계산한거 보면 다 $R_t$ 쓴 것 같음. 그리고 예시를 보면 그게 맞는거 같기도 함. 식 중에서 중간에 뜬금없이 $\\mathbb{E}[R_{t+1}]$을 $R_s$로 바꿔놓음 아주 지멋대로야. immediate reward $R_{t+1}$ discounted value of successor state $\\gamma \\mathsf{v}(S_{t+1})$\\(\\quad\\quad\\quad\\quad\\mathsf{v}(s) =\\mathbb{E}\\,[\\,G_t\\;\\|\\;S_t=s\\,]\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\,[\\,R_{t+1}+\\gamma R_{t+2} + \\gamma ^2 R_{t+3} + \\dots\\;\\|\\;S_t=s\\,]\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\,[\\,R_{t+1}+\\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots)\\;\\|\\;S_t=s\\,]\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\,[\\,R_{t+1}+\\gamma G_{t+1}\\;\\|\\;S_t=s\\,]\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\,[\\,R_{t+1}+\\gamma \\mathsf{v} (S_{t+1})\\;\\|\\;S_t=s\\,]\\)\\(\\quad\\quad\\quad\\quad\\mathsf{v}(s) =\\mathcal{R}_s + \\gamma \\displaystyle\\sum_{s&#39; \\in S} ^{}\\mathcal{P}_{ss&#39;} \\mathsf{v} (s&#39;)\\)Bellman Equation in Matrix FormThe Bellman equation can be expressed concisely using matrices.\\[\\mathsf{v} = \\mathcal{R} + \\gamma \\mathcal{P} \\mathsf{v}\\]where $\\mathsf{v}$ is a column vector with one entry per state\\[\\begin{bmatrix} \\mathsf{v} (1) \\\\ \\vdots \\\\ \\mathsf{v} (n) \\end{bmatrix} = \\begin{bmatrix} \\mathcal{R}_1 \\\\ \\vdots \\\\ \\mathcal{R}_n \\end{bmatrix} + \\gamma \\begin{bmatrix} \\mathcal{P}_{11} &amp;amp; \\dots &amp;amp; \\mathcal{P}_{1n} \\\\ \\vdots &amp;amp; &amp;amp; \\\\ \\mathcal{P}_{n1} &amp;amp; \\dots &amp;amp; \\mathcal{P}_{nn} \\end{bmatrix} \\begin{bmatrix} \\mathsf{v} (1) \\\\ \\vdots \\\\ \\mathsf{v} (n) \\end{bmatrix}\\]각 state 1~n에서의 value function은 각 state의 reward와 dot (각 state로의 확률), (각 state의 value) * discount factor 의 합이다.너무 흔한 형태의 식… 그러나 수치해석해야함…Solving the Bellman Equation Linear equation\\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\mathsf{v} = \\mathcal{R+ \\gamma P}\\mathsf{v}\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\; (1 - \\gamma \\mathcal{P}) \\mathsf{v} = \\mathcal{R}\\)\\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\mathsf{v} = (1 - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\\) Computational complexiy is $O(n^3)$ for n state Direct solution only possible for small MRPs 근데 되겠냐 There are many iterative methods for large MRPs, e.g. Dynamic programming Monte-Carlo evaluation Temporal-Difference learning Markov Decision Process (main)드디어 Action으로 개입이 추가되었다. 근데 policy는 왜 빼놓고 하냐 논리적으로 문제가 생기잖아 이미 probability를 정의해 놓고서는 Action을 어떻게 해, 결국 예제에서는 Action을 정의하지 않는 자동 전이 state에서만 Probability를 사용한다. 물론 실제로 둘이 공존할수 있기는 하다. 그리고 Action을 만드는 애는 policy라서 이미 있는 것이다. Definition a Markov Reward Process is a tuple $\\langle\\mathcal{S, }$$\\mathcal{A}$$\\mathcal{, P, R, \\gamma \\rangle}$$\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{S}}\\;$ is a finite set of states$\\tiny{\\blacksquare}\\quad$ $\\normalsize{\\mathcal{A}}\\;$ is a finite set of actions$\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{P}}\\;$ is a state transition probability matrix, \\(\\quad\\mathcal{P}^a_{ss&#39;}=\\mathbb{P}[S_{t+1}=s&#39;|S_t=s,\\) \\(A_t =a\\)\\(]\\) $\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{R}}\\;$ is a reward function, \\(\\mathcal{R}^a_s=\\mathbb{E}[R_{t+1} | S_t=s,\\) \\(A_t =a\\)\\(]\\)$\\tiny{\\blacksquare}\\quad \\normalsize{\\mathcal{\\gamma}}\\;$ is a discount factor, \\(\\gamma\\in[0,1]\\) PoliciesPolicy는 stochastic으로 시작해 확률을 정의하는 것처럼 해놓았지만 결국엔 deterministic으로 은근슬쩍 바뀐다. 물론 내부 policy 정의는 양쪽을 다 가다가 우선순위 높은 놈으로 바뀌는 식이니 둘다 맞는 말이긴 하다. Definition A policy $\\pi$ is a distribution over actions given states.$$\\pi (a| s) = \\mathbb{P} [A_t = a | S_t = s]$$ A policy fully defines the behaviour of an agent MDP policies depend on the current state (not the history) Policies are stationary (time-independent)$\\quad A_t ~ \\pi(. | S_t), \\forall t &amp;gt; 0$ Given an MDP $\\mathcal{M = \\langle S, A, P ,R ,\\gamma \\rangle}$ and a policy $ \\pi$ The state sequence $\\mathcal{S_1, S_2, \\dots }$ is a Markov process $\\mathcal{\\langle S, P^{\\pi} \\rangle}$ The state and reward sewuence $\\mathcal{S_1, R_2, S_2, \\dots}$ is a Markov reward process $\\mathcal{\\langle S, P^{\\pi}, R^{\\pi}, \\gamma \\rangle }$ 슬쩍 A를 뺐지만 policy에 포함된 것이므로 없어진 게 아니다 바뀐것 없다 where$$ \\mathcal{P^{\\pi}_{s, s&#39;}= \\displaystyle\\sum_{a \\in A} \\pi (a \\| s) P^a_{ss&#39;}}$$$$ \\mathcal{R^{\\pi}_{s}= \\displaystyle\\sum_{a \\in A} \\pi (a \\| s) R^a_{s}}$$Value Function 2각 state와 action의 value를 정의한다.The state-value function $\\mathsf{v_{\\pi} (s)} $ of an MDP is the expected return starting from state s, and then following policy $\\pi$\\[\\mathsf{v_{\\pi} (s) = \\mathbb{E}_{\\pi} [G_t | S_t = s]}\\]The action-value function $\\mathsf{q_{\\pi} (s,a)}$ is the expected return starting from state s, taking action a, and then following policy $\\pi$\\[\\mathsf{q_{\\pi} (s,a) = \\mathbb{e}_{\\pi} [G_t | S_t = s, A_t = a] }\\]Bellman Expectation Equation필요에 따라 연쇄적으로 대충 정의할 수 있다.Action에 따라서 따라오는 State도 stochastic할 수 있다.The state-value function can again be decomposed into immediate reward plus discounted value of successor state\\[\\mathsf{v_{\\pi} (s) = \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1}) | S_t = s]}\\]The action-value function can similarly be decomposed\\[\\mathsf{q_{\\pi} (s,a) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma q_{\\pi} (S_{t+1}, A_{t+1}) | S_t = s, A_t=a]}\\]Bellman Expectation Equation Matrix FormThe Bellman expectation equation can be expressed concisely using the induced MRP\\[\\mathsf{v_{\\pi} = \\mathcal{R}^{\\pi} + \\gamma \\mathcal{P}^{\\pi} v_{\\pi}}\\]with direct solution\\[\\mathsf{v_{\\pi} = (1- \\gamma \\mathcal{P}^{\\pi})^{-1} \\mathcal{R}^{\\pi}}\\]Optimal Value FunctionThe optimal state-value function $\\mathsf{v_* (s)}$ is the maximum value function over all policies\\[\\mathsf{v_* (s) = \\displaystyle\\max_{\\pi} v_{\\pi} (s)}\\]The optimal action-value function $\\mathsf{a_* (s,a)}$ is the maximum action-value function over all policies\\[\\mathsf{q_* (s,a) = \\displaystyle\\max_{\\pi} q_{\\pi} (s,a)}\\] The optimal value function specifies the best possible performance in the MDP An MDP is “solved” when we know the optimal value fnOptimal Policy증명생략 - 하여튼 존재하고 반드시 그렇다. 흠 잘못 수렴되는 경우가 발생할 수 있을 것 같은데..Define a partial ordering over policies\\[\\mathsf{\\pi \\ge \\pi &#39; \\quad if \\quad v_{\\pi} (s) \\ge v_{\\pi &#39;} (s), \\forall s}\\] Theorem For any Markov Decision Process$\\bullet\\;$ There exists an optimal policy $\\pi$, that is better than or equal to all other policies, $\\pi _* \\geq \\pi, \\forall \\pi $$\\bullet\\;$ All optimal policies achieve the optimal value function, \\(\\mathsf{v_{\\pi_*} (s) = v_* (s)}\\)$\\bullet\\;$ All optimal policies achieve the optimal action-value function, \\(\\mathsf{q_{\\pi_*} (s,a) = q_* (s,a)}\\) Finding an Optimal PolicyAn optimal policy can be found by maximising over \\(\\mathsf{q_* (s,a)}\\)\\[\\mathsf{\\pi_*(a\\vert s)} = \\begin{cases} \\mathsf{1\\quad if \\; a = \\underset{a\\in\\mathcal{A}}{argmax} \\; q_* (s,a)} \\\\ \\mathsf{0 \\quad otherwise} \\end{cases}\\] There is always a deterministic optimal policy for any MDP If we know $\\mathsf{q_* (s,a)}$, we immediately have the optimal policy 당연하지만 optimal이 되면 deterministic이 된다.Bellman Optimality EquationThe optimal value functions are recursively related by the Bellman optimality equations:\\[\\mathsf{v_* (s) = \\underset{a}{max} \\;q_* (s,a)}\\]\\[\\mathsf{q_* (s,a) = \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39;\\in S} \\mathcal{P}^a_{ss&#39;} v_* (s&#39;)}\\]\\[\\mathsf{v_* (s) = \\underset{a}{max} \\; \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39; \\in S} \\mathcal{P}^a_{ss&#39;}v_* (s&#39;)}\\]\\[\\mathsf{q_* (s,a) = \\mathcal{R}^a_s + \\gamma \\displaystyle\\sum_{s&#39; \\in S} \\mathcal{P}^a_{ss&#39;} \\underset{a&#39;}{max} \\; q_* (s&#39;, a&#39;)}\\]Solving the Bellman Optimality Equation 수치해석 하라는 소리. 오랜만에 복습해야겠는 걸 잘 기억이안나네 쉽긴한데 Bellman Optimality Equation is non-linear No closed form solution (in general) Many iterative solution methods Value Iteration Policy Iteration Q-learning Sarsa Extensions to MDPs여기부턴 강의는 없다. PPT만 있다. Infinite and continuous MDPs Partially observable MDPs Undiscounted, average reward MDPsInfinite MDPsContinuous한 MDP, 물리적상태 같은건가 HJB equation 다시한번 봐야겠다The following extensions are all possible: Countably infinite state and/or action spaces Straightforward Continuous state and/or action spaces Closed form for linear quadratic model(LQR) Continuous Time Requires partial differential equations Hamilton-Jaccobian-Bellman (HJB) equation Limiting case of Bellman equation as time-step $\\rightarrow 0$ POMDPs실제 모든것이 관측 가능하지 않은 문제에 대한 정의로 observation이 들어간다. 실제로 이런게 많을것 같은데 정의를 어떻게 할 것인가A Partially Observab;e Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions. Definition A POMDP is a tuple $\\mathcal{\\langle S, A, \\textcolor{red}{O}, P, R,\\textcolor{red}{Z}, \\gamma \\rangle}$$\\bullet\\;\\; \\mathcal{S}\\;$ is a finite set of states$\\bullet\\;\\; \\mathcal{A}\\;$ is a finite set of actions$\\bullet\\;\\; \\textcolor{red}{\\mathcal{A}}\\;$ is a finite set of observation$\\bullet\\;\\; \\mathcal{P}\\;$ is a state transition probability matrix\\(\\quad\\,\\mathsf{\\mathcal{P}^a_{ss&#39;} = \\mathbb{P} [S_{t+1} = s&#39; \\vert S_t = s, A_t = a] }\\)$\\bullet\\;\\; \\mathcal{R}\\;$ is a reward function\\(\\quad\\,\\mathsf{\\mathcal{R}^a_s = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a] }\\)$\\bullet\\;\\; \\mathcal{\\textcolor{red}{Z}}\\;$ is an observation function\\(\\quad\\,\\mathsf{\\mathcal{Z}^a_{s&#39;o} = \\mathbb{P} [O_{t+1} = o \\vert S_{t+1} = s&#39;, A_t = a] }\\)$\\bullet\\;\\; \\mathcal{\\gamma}\\;$ is a discount factor $\\gamma \\in [0, 1]$ Belief States현재 state가 어딘지 모르고 history에 따라 확률로 정의한다는 뜻인가? 흐음 어떤 경우인지 아직 모르겠다.A history $H_t$ is a sequence of actions, observations and rewards\\[\\mathsf{H_t = A_0, O_1, R_1, \\dots , A_{t-1}, O_t, R_t}\\]A belief state $b(h) is a probablity distribution over states, conditioned on the history h\\[\\mathsf{b(h) = (\\mathbb{P} [ S_t = s^1 \\vert H_t = h ], \\dots , \\mathbb{P}[S_t = s^n \\vert H_t = h])}\\]Reductinos of POMDPsState tree로 표현하는게 뭘까 흠 bellman equation으로 표현하기 어려운 경우가 있는건가 The history $\\mathsf{H_t}$ satisfies the Markov property The belief state $\\mathsf{b(H_t)}$ satisfies the Markov property A POMDP can be reduced to an (infinite) history tree A POMDP can be reduced to an (infinite) belief state treeErgodic Markov Process각 policy에 대해서 각 state에 나타나는 확률적 분포에 대한 이야기같다. 나중에 다시 생각해볼까..An ergodic Markov process is Recurrent: each state is visted an infinite number of times Aperiodic: each state is visited without any systematic period Theorem An ergodic Markov process has a limiting stationary distribution $\\mathsf{d^{\\pi}(s)}$ with the property$$ \\mathsf{d^{\\pi} (s) = \\displaystyle\\sum_{s&#39; \\in S} d^{\\pi} (s&#39;) \\mathcal{P}_{s&#39;s}}$$ Ergodic MDP그렇게 state의 확률 분포가 수렴하는 형태의 MDP에 대해서 policy에 의한 각 타임 step의 평균적인 reward가 존재한다 흠… 이게 마이너스면 time이 흘러갈수록 reward는 계속 낮아지고 그런거일 뿐 아닌가…? 이 평균은 어따쓴담 Definition An MDP is ergodic if the Markov chain induced by any policy is ergodic For any policy $\\pi$, and ergodic MDP has an average reward per time-step $\\rho ^{\\pi}$ that is independent of start state.\\[\\mathsf{\\rho^{\\pi} = \\underset{T\\rightarrow \\infty}{lim} \\; {1\\over T}\\mathbb{E} \\left[\\displaystyle\\sum^T_{t=1} R_t\\right]}\\]Average Reward Value FunctionErgodic 내용을 적용한 Value Function The value function of an undiscounted, ergodic MDP can be expressed in terms of average reward. $\\mathsf{\\tilde{v}_{\\pi} (s)$ is the extra reward due to starting from state s,\\[\\mathsf{\\tilde{v}_{\\pi} (s) = \\mathbb{E}_{\\pi} \\left[ \\displaystyle\\sum^{\\infty}_{k=1} (R_{t+k} - \\rho^{\\pi})\\;\\vert\\;S_t =s \\right ]}\\]There is a corresponding average reward Bellman equation,\\[\\mathsf{\\tilde{v}_{\\pi} (s) = \\mathbb{E}_{\\pi} \\left[(R_{t+1} - \\rho^{\\pi}) + \\displaystyle\\sum^{\\infty}_{k=1} (R_{t+k} - \\rho^{\\pi})\\;\\vert\\;S_t =s \\right ]}\\]\\[\\quad\\quad = \\mathsf{\\mathbb{E}_{\\pi} \\left[(R_{t+1} - \\rho^{\\pi}) + \\tilde{v}_{\\pi} (s+1)\\;\\vert\\;S_t =s \\right ]}\\]" }, { "title": "RLcourse note - Lecture 1 Introduction to Reinforcement Learning", "url": "/posts/RL-course-Note-1/", "categories": "RLcourse, Note", "tags": "reinforcementlearning, lecturenote", "date": "2021-11-26 16:00:00 +0900", "snippet": "Video Link :Characteristics Label이 필요하지 않다. Reward 설계를 통해 Model이 학습한다. Feedback is delayed. 지도학습과 비교해 RL은 Reward 설정으로 부터 가시적인 변화를 얻기까지 시간이 걸린다. Time really matters 학습된 모델이 즉각적으로 환경에 반응하여 동작하는 만큼 시간과의 관계 또한 중요하다. Agent’s actions affect the subsequent data it recieves 위와 비슷한 맥락Rewards reward $R_t$ 는 scalar feedback 신호로 정의한다. 왜 벡터로는 안되는 걸까. 그게 정의하기나 학습이 용이한 건 알겠으나 Vector는 안된다고 선을 그어야 할 이유는 아직 모르겠다. Agent는 Reward를 극대화 시켜야 한다.Reinforcement Learning은 Reward Hypothesis로 시작한다. Definition (Reward Hypothesis) All goals can be described by the maximisation of expected cumulative reward Sequential Decision Making Goal : select actions to maximise total future reward Actions may have long term consequences Reward may be delayed It may be better to sacrifice immediate reward to gain more long-term reward 당연한 내용들이랄까Agent and Environment At each step $t$ the agent : Executes action $A_t$ Receives observation $O_t$ from environmnet Receives scalar reward $R_t$ The environmen : Receives actions $A_t$ from agent Emits observation $O_{t+1}$ Emits scalar reward $R_{t+1}$ calculated History and State The history is sequence of observations, actions, rewards\\[H_t = A_1, O_1, R_1, ... , A_t, O_t, R_t\\] State is the information used to determine what happens next Formally, state is a function of the history:\\[S_t = f(H_t)\\]Environment State The environment state $S^e_t$ is the environment’s private representation not usually visible to the agent including irrelevant informationAgent State The agent state $S^\\mathsf{a}_t$ is the agent’s internal representation information used by reinforcement learning algorithms It can be any function of history:\\[S^\\mathsf{a}_t = f(H_t)\\]Information StateAn information state (a.k.a Markov state) contains all useful information from the history. Definition   A state \\(S_t\\) is Markov if and only if \\(\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, ...,S_t]\\) Fully Observable Environments Full observability: agent directly observes envrionment state\\[O_t = S^\\mathsf{a}_t - S^e_t\\] Agent state = Environment state = information State Formally, this is a Markov decision process (MDP) 이부분은 약간의 논리적 오류가..?Partially Observable Environments Partial observability : agent indirectly observes environment 실질적인 경우 - 환경의 일부 정보만 습득 가능 agent state $\\neq$ environment state Formally this is called partially observable Markov decision process (POMDP) Agent must construct its own state representation $S^\\mathsf{a}_t$ Complete history $S^\\mathsf{a}_t = H_t$ Beliefs of envrionment state : $S^\\mathsf{a}_t = (\\mathbb{P}[S^e_t = s^1], … , \\mathbb{P}[S^e_t = s^n])$ 분포 형태의 state Recurrent neural network : $S^\\mathsf{a}t = \\sigma (S^\\mathsf{a}{t-1}W_s + O_tW_o)$ 회귀 신경망 Major Components of an RL Agent An RL agent may include one or more of these components Policy: agent’s behaviour function Value function: how good is each state and/or action Model: agents’s representation of the environment Policy A policy isthe agent’s behaviour map from state to action Deterministic policy: $\\mathsf{a}=\\pi(s)$ - 확정적으로 action을 결정 Stochastic policy: $ \\pi(\\mathsf{a}|s) = \\mathbb{P}[A=\\mathsf{a}|S=s] $ - 확률적으로 action을 선택Value Function Value function is a prediction of future reward Used to evaluate the goodness/badness of states And therefore to select between actions\\[v_\\pi(s) = \\mathbb{E}_\\pi[R_t + \\gamma R_{t+1}+ \\gamma ^2R_{t+2}| S_t = s]\\]Model A model predicts what the environment will do next $\\mathcal{P}$ predicts the next state $\\mathcal{R}$ predicts the next (immediate) reward\\[\\mathcal{P}^\\mathsf{a}_{ss&#39;} = \\mathbb{P}[S_{t+1} =s&#39; | S_t =s, A_t=\\mathsf{a}]\\]\\[\\mathcal{R}^\\mathsf{a}_{s} = \\mathbb{E}[R_{t+1} | S_t =s, A_t=\\mathsf{a}]\\]Categorizing RL agents 1 Value Based Policy Value Function Policy Based Policy Value Function Actor Critic Policy Value Function Categorizing RL agents 2 Model Free Policy and/or Value Function Model Model Based Policy and/or Value Function Model RL Agent TaxonomyLearning and PlanningTwo fundamental probelms in sequential decision making Reinforcement Learning The environmnet is initially unknown The agent interacts with the environment The agent imporves its policy Planning A model of the environment is known The agent performs computations with its model (without any external interaction) The agent imporves its policy Exploration and Exploitation Exploration finds more information about the envrionment Exploitation expolits known information to maximise reward 둘다 적절히 시행하는 것이 중요Prediction and Control Prediction: evaluate the future with given policy Control: optimise the future finding the best policy" }, { "title": "Writing a New Post - Chirpy", "url": "/posts/write-a-new-post/", "categories": "Tutorial", "tags": "", "date": "2019-08-08 15:10:00 +0900", "snippet": "Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts/ of the root directory. Please note that the EXTENSION must be one of md and markdown.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- Note: The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of dateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:categories: [Animal, Insect]tags: [bee]Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsSimilar to TOC, the Disqus comments are loaded by default in each post, and the global switch is defined by variable comments in file _config.yml . If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---MermaidMermaid is a great diagrams generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesPreview imageIf you want to add an image to the top of the post contents, specify the attribute src, width, height, and alt for the image:---image: src: /path/to/image/file width: 1000 # in pixels height: 400 # in pixels alt: image alternative text---Except for alt, all other options are necessary, especially the width and height, which are related to user experience and web page loading performance. Later section “Image size” will also mention this.Image captionAdd italics to the next line of an image，then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_Image sizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image:![Desktop View](/assets/img/sample/mockup.png){: width=&quot;700&quot; height=&quot;400&quot; }Image positionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. For example: Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } Limitation: Once the position of the image is specified, the image caption should not be added.Image shadowThe screenshots of the program window can be considered to show the shadow effect, and the shadow will be visible in the light mode:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&amp;lt;img src=&quot;https://cdn.com/path/to/flower.png&quot; alt=&quot;The flower&quot;&amp;gt;Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---Code BlockMarkdown symbols ``` can easily create a code block as follows:This is a plaintext code snippet.Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` Limitation: The Jekyll style highlight tag is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of the code block, you can append {: .nolineno} at the next line:```shellecho &#39;No more line numbers!&#39;```{: .nolineno}Specifying the FilenameYou may have noticed that the code language will be displayed on the left side of the header of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=&quot;path/to/file&quot; }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains &#39;Pack&#39; %} This product&#39;s title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography - Chirpy", "url": "/posts/text-and-typography/", "categories": "Tutorial", "tags": "", "date": "2019-08-08 12:33:00 +0900", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.TitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Task list TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line to shows the Block Quote.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Images Default (with caption)Full screen width and center alignment Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " } ]
