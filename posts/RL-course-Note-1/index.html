<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 1 Introduction to Reinforcement Learning" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-1/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-1/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-11-26T16:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 1 Introduction to Reinforcement Learning" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-1/","@type":"BlogPosting","headline":"RLcourse note - Lecture 1 Introduction to Reinforcement Learning","dateModified":"2021-12-16T06:06:37+09:00","datePublished":"2021-11-26T16:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-1/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 1 Introduction to Reinforcement Learning | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 1 Introduction to Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 1 Introduction to Reinforcement Learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Nov 26, 2021, 4:00 PM +0900" >Nov 26, 2021<i class="unloaded">2021-11-26T16:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 16, 2021, 6:06 AM +0900" >Dec 16, 2021<i class="unloaded">2021-12-16T06:06:37+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="853 words">4 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 200px 100px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec1_thumb.JPG" alt="thumb1" width="200px" height="100px" /></a></p><h2 id="characteristics">Characteristics</h2><ul><li>Label이 필요하지 않다. Reward 설계를 통해 Model이 학습한다.<li>Feedback is delayed. 지도학습과 비교해 RL은 Reward 설정으로 부터 가시적인 변화를 얻기까지 시간이 걸린다.<li>Time really matters 학습된 모델이 즉각적으로 환경에 반응하여 동작하는 만큼 시간과의 관계 또한 중요하다.<li>Agent’s actions affect the subsequent data it recieves 위와 비슷한 맥락</ul><h3 id="rewards">Rewards</h3><ul><li>reward $R_t$ 는 scalar feedback 신호로 정의한다. 왜 벡터로는 안되는 걸까. 그게 정의하기나 학습이 용이한 건 알겠으나 Vector는 안된다고 선을 그어야 할 이유는 아직 모르겠다.<li>Agent는 Reward를 극대화 시켜야 한다.</ul><p>Reinforcement Learning은 Reward Hypothesis로 시작한다.</p><div class="table-wrapper"><table><thead><tr><th>Definition (Reward Hypothesis)<tbody><tr><td>All goals can be described by the maximisation of expected cumulative reward</table></div><h3 id="sequential-decision-making">Sequential Decision Making</h3><ul><li>Goal : select actions to maximise total future reward<li>Actions may have long term consequences<li>Reward may be delayed<li>It may be better to sacrifice immediate reward to gain more long-term reward<li>당연한 내용들이랄까</ul><h2 id="agent-and-environment">Agent and Environment</h2><ul><li>At each step $t$ the agent :<ul><li>Executes action $A_t$<li>Receives observation $O_t$ from environmnet<li>Receives scalar reward $R_t$</ul><li>The environmen :<ul><li>Receives actions $A_t$ from agent<li>Emits observation $O_{t+1}$<li>Emits scalar reward $R_{t+1}$ calculated</ul></ul><h3 id="history-and-state">History and State</h3><ul><li>The history is sequence of observations, actions, rewards</ul>\[H_t = A_1, O_1, R_1, ... , A_t, O_t, R_t\]<ul><li>State is the information used to determine what happens next<li>Formally, state is a function of the history:</ul>\[S_t = f(H_t)\]<h4 id="environment-state">Environment State</h4><ul><li>The environment state $S^e_t$ is the environment’s private representation<li>not usually visible to the agent<li>including irrelevant information</ul><h4 id="agent-state">Agent State</h4><ul><li>The agent state $S^\mathsf{a}_t$ is the agent’s internal representation<li>information used by reinforcement learning algorithms<li>It can be any function of history:</ul>\[S^\mathsf{a}_t = f(H_t)\]<h4 id="information-state">Information State</h4><p>An information state (a.k.a Markov state) contains all useful information from the history.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: left">Definition<th style="text-align: right"> <tbody><tr><td style="text-align: left">A state \(S_t\) is Markov if and only if<td style="text-align: right">\(\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, ...,S_t]\)</table></div><h3 id="fully-observable-environments">Fully Observable Environments</h3><ul><li>Full observability: agent directly observes envrionment state</ul>\[O_t = S^\mathsf{a}_t - S^e_t\]<ul><li>Agent state = Environment state = information State<li>Formally, this is a Markov decision process (MDP) 이부분은 약간의 논리적 오류가..?</ul><h3 id="partially-observable-environments">Partially Observable Environments</h3><ul><li>Partial observability : agent indirectly observes environment 실질적인 경우 - 환경의 일부 정보만 습득 가능<li>agent state $\neq$ environment state<li>Formally this is called partially observable Markov decision process (POMDP)<li>Agent must construct its own state representation $S^\mathsf{a}_t$<ul><li>Complete history $S^\mathsf{a}_t = H_t$<li>Beliefs of envrionment state : $S^\mathsf{a}_t = (\mathbb{P}[S^e_t = s^1], … , \mathbb{P}[S^e_t = s^n])$ 분포 형태의 state<li>Recurrent neural network : $S^\mathsf{a}<em>t = \sigma (S^\mathsf{a}</em>{t-1}W_s + O_tW_o)$ 회귀 신경망</ul></ul><h2 id="major-components-of-an-rl-agent">Major Components of an RL Agent</h2><ul><li>An RL agent may include one or more of these components<ul><li>Policy: agent’s behaviour function<li>Value function: how good is each state and/or action<li>Model: agents’s representation of the environment</ul></ul><h3 id="policy">Policy</h3><ul><li>A policy isthe agent’s behaviour<li>map from state to action<li>Deterministic policy: $\mathsf{a}=\pi(s)$ - 확정적으로 action을 결정<li>Stochastic policy: $ \pi(\mathsf{a}|s) = \mathbb{P}[A=\mathsf{a}|S=s] $ - 확률적으로 action을 선택</ul><h3 id="value-function">Value Function</h3><ul><li>Value function is a prediction of future reward<li>Used to evaluate the goodness/badness of states<li>And therefore to select between actions</ul>\[v_\pi(s) = \mathbb{E}_\pi[R_t + \gamma R_{t+1}+ \gamma ^2R_{t+2}| S_t = s]\]<h3 id="model">Model</h3><ul><li>A model predicts what the environment will do next<li>$\mathcal{P}$ predicts the next state<li>$\mathcal{R}$ predicts the next (immediate) reward</ul>\[\mathcal{P}^\mathsf{a}_{ss'} = \mathbb{P}[S_{t+1} =s' | S_t =s, A_t=\mathsf{a}]\] \[\mathcal{R}^\mathsf{a}_{s} = \mathbb{E}[R_{t+1} | S_t =s, A_t=\mathsf{a}]\]<h3 id="categorizing-rl-agents-1">Categorizing RL agents 1</h3><ul><li>Value Based<ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><del>Policy</del><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Value Function</ul><li>Policy Based<ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Policy<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><del>Value Function</del></ul><li>Actor Critic<ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Policy<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Value Function</ul></ul><h3 id="categorizing-rl-agents-2">Categorizing RL agents 2</h3><ul><li>Model Free<ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Policy and/or Value Function<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><del>Model</del></ul><li>Model Based<ul class="task-list"><li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Policy and/or Value Function<li class="task-list-item" hide-bullet><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Model</ul></ul><h3 id="rl-agent-taxonomy">RL Agent Taxonomy</h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 400'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_agent_taxonomy.jpg" alt="RL Agent Taxonomy" width="400px" height="400 px" /></p><h2 id="learning-and-planning">Learning and Planning</h2><p>Two fundamental probelms in sequential decision making</p><ul><li>Reinforcement Learning<ul><li>The environmnet is initially unknown<li>The agent interacts with the environment<li>The agent imporves its policy</ul><li>Planning<ul><li>A model of the environment is known<li>The agent performs computations with its model (without any external interaction)<li>The agent imporves its policy</ul></ul><h3 id="exploration-and-exploitation">Exploration and Exploitation</h3><ul><li><strong>Exploration</strong> finds more information about the envrionment<li><strong>Exploitation</strong> expolits known information to maximise reward<li>둘다 적절히 시행하는 것이 중요</ul><h3 id="prediction-and-control">Prediction and Control</h3><ul><li>Prediction: evaluate the future with given policy<li>Control: optimise the future finding the best policy</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 1 Introduction to Reinforcement Learning - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 1 Introduction to Reinforcement Learning - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 1 Introduction to Reinforcement Learning - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/write-a-new-post/" class="btn btn-outline-primary" prompt="Older"><p>Writing a New Post - Chirpy</p></a> <a href="/posts/RL-course-Note-2/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 2 Markov Decision Processes</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
