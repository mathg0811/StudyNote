<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 9 Exploration and Exploitation" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-9/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-9/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-14T19:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 9 Exploration and Exploitation" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-9/","@type":"BlogPosting","headline":"RLcourse note - Lecture 9 Exploration and Exploitation","dateModified":"2022-01-21T19:18:02+09:00","datePublished":"2022-01-14T19:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-9/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 9 Exploration and Exploitation | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 9 Exploration and Exploitation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 9 Exploration and Exploitation</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jan 14, 2022, 7:00 PM +0900" >Jan 14<i class="unloaded">2022-01-14T19:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Fri, Jan 21, 2022, 7:18 PM +0900" >Jan 21<i class="unloaded">2022-01-21T19:18:02+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2954 words">16 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/sGuiWX07sKw"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec9_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h3 id="9강-소감">9강 소감</h3><p>Exploration과 Exploitation을 밸런스를 맞추면서 학습하도록하는 몇가지 기법에 대한 내용인데 별로 중요하진 않은것같다..? 솔직히 몇가지 배웠어도 큰 성능차이가 나는 부분도 아니고 몇 가지 환경적 상황 변수를 파악하고 거기에 맞게 설계한다면 이런 문제는 훨씬 효율적이고 적합하게 해결될 수 있다. 몇개 내용설명이 미흡하여 따로 공부해볼까했지만 중요하지 않아보여서 패스</p><h2 id="introduction">Introduction</h2><p>Exploration vs. Exploitation Dilemma</p><ul><li>Inline decision-making invloves a fundamental choice:<ul><li>Exploitation : Make the best decision given current information<li>Exploration : Gather more information</ul><li>The best long-term strategy may involve short-term sacrifices<li>Gather enough information to make the best overall decisions</ul><h3 id="principles">Principles</h3><ul><li>Naive Exploration<ul><li>Add noise to greedy policy (e.g. $\epsilon$-greedy)</ul><li>Optimistic Initialisation<ul><li>Assume the best until proven otherwise</ul><li>Optimism in the Face of Uncertainty<ul><li>Prefer actions with uncertain values</ul><li>Probability Matching<ul><li>Select actions according to probability they are best</ul><li>Infromation State Search<ul><li>Lookahead search incorporating value of information</ul></ul><h2 id="multi-armed-bandits">Multi-Armed Bandits</h2><ul><li>A multi-armed bandit is tuple $\langle\mathcal{A,R}\rangle$<li>$\mathcal{A}$ is a known set of m actions (or “arms”)<li>$\mathsf{\mathcal{R}^a(r) = \mathbb{P}[r\vert a]}$ is an unknown probability distribution over rewards<li>At each step t the agent selects an action $a_t \in \mathcal{A}$<li>The environment generates a reward $r_t \sim \mathcal{R}^{a_t}$<li>The goal is to maximise cumulative reward $\sum^t_{\tau = 1} r_\tau$</ul><p>tuple에서 P를 빼놓고 내용에서는 정작 사용하고 있는 아이러니, state transition probability와 reward probability가 다를 수는 있지만 state에서 정의될 수 있는 부분이기도 하다. reward varation을 state에서 정의하지 못할 만한 경우가 있을까?</p><h3 id="regret">Regret</h3><ul><li>The action-value is the mean reward for action a,</ul>\[\mathsf{ Q(a) = \mathbb{E} [r \vert a] }\]<ul><li>The optimal value $\mathsf{V}^*$ is</ul>\[\mathsf{ V^* = Q(a^*) = \underset{a\in\mathcal{A}}{max}\; Q(a) }\]<ul><li>The regret is the opportunity loss for one step</ul>\[\mathsf{ l_t = \mathbb{E} [V^* - Q(a_t)] }\]<ul><li>The total regret is the total opportunity loss</ul>\[\mathsf{ L_t = \mathbb{E} \left[ \displaystyle\sum^t_{\tau = 1 } V^* - Q(a_\tau) \right] }\]<ul><li>Maximise cumulative reward $\equiv$ minimise total regret</ul><p>Optimal도 그냥 Q 쓰면되지 난데없이 왜 V야. mean action value는 따로 표시하면 되지 왜 똑같이 q를 쓰려고 하는가. 여기서 regret은 경우에 따라 minus인 경우도 생길 것이다. 근데 regret은 왜 Expected value인가. one step 이라면서 왜 expected야 짜증나게. Summation 해놓고 E 붙이는것도 마찬가지고</p><h4 id="counting-regret">Counting Regret</h4><ul><li>The count $\mathsf{N_t(a)}$ is expected number of elections for action a<li>The gap $\Delta _a$ is the difference in value between action a and optimal action $\mathsf{ a^<em>, \Delta_a = V^</em> - Q(a) }$<li>Regret is a function of gaps and the counts</ul>\[\begin{aligned} \mathsf{L_t}&amp;= \mathsf{ \mathbb{E} \left[ \displaystyle\sum^t_{\tau=1} V^* - Q(a_\tau) \right] } \\ &amp;= \mathsf{ \displaystyle\sum_{a\in\mathcal{A}} \mathbb{E} [N_t(a)] (V^* - Q(a)) } \\ &amp;= \mathsf{ \displaystyle\sum_{a\in\mathcal{A}} \mathbb{E} [N_t(a)] \Delta_a } \end{aligned}\]<ul><li>A good algorithm ensures small counts for large gaps<li>Problem: gaps are not known!</ul><p>이미 Summation을 했는데 $\mathbb{E}$는 왜 붙어있는건가…</p><h4 id="linear-or-sublinear-regret">Linear or Sublinear Regret</h4><ul><li>If an algorithm forever explores it will have linear total regret<li>If an algorithm never explores it will have linear total regret<li>Is it possible to achieve sublinear total regret?</ul><h3 id="greedy-and-epsilon-greedy-algorithms">Greedy and $\epsilon$-greedy algorithms</h3><h4 id="greedy-algorithm">Greedy Algorithm</h4><ul><li>We consider algorithms that estimate $\mathsf{\hat{Q}_t(a)\approx Q(a)}$<li>Estimate the value of each action by Monte-Carlo evaluation</ul>\[\mathsf{ \hat{Q}_t(a) = \frac{1}{N_t(a)} \displaystyle\sum^T_{t=1} r_t 1(a_t=a) }\]<ul><li>The greedy algorithm selects action with highest value</ul>\[\mathsf{ a^*_t = \underset{a\in\mathcal{A}}{argmax}\; \hat{Q}_t(a) }\]<ul><li>Greedy can lock onto a suboptimal action forever<li>$\Rightarrow$ Greedy has linear total regret</ul><h4 id="epsilon-greedy-algorithm">$\epsilon$-greedy Algorithm</h4><ul><li>The $\epsilon$-greedy algorithm continues to explore forever<ul><li>With probability $1-\epsilon$ select $\mathsf{a=\underset{a\in\mathcal{A}}{argmax}\;\hat{Q}(a)}$<li>With probability $\epsilon$ select a random action</ul><li>Constant $\epsilon$ ensures minimum regret</ul>\[\mathsf{ l_t \geq \frac{\epsilon}{\mathcal{A}} \displaystyle\sum_{a\in\mathcal{A}}\Delta_a }\]<ul><li>$\Rightarrow$ $\epsilon$-greedy has linear total regret</ul><h4 id="optimistic-initialisation">Optimistic Initialisation</h4><ul><li>Simple and practical idea: initialise $\mathsf{Q(a)}$ to high value<li>Update action value by incremental Monte-Carlo evaluation<li>Starting with $\mathsf{N(a) &gt; 0}$</ul>\[\mathsf{ \hat{Q}_t(a_t) = \hat{Q}_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat{Q}_{t-1}) }\]<ul><li>Encourages systematic exploration early on<li>But can still lock onto suboptimal action<li>$\Rightarrow$ greedy + optimistic initialisation has linear total regret<li>$\Rightarrow$ $\epsilon$-greedy + optimistic initialisation has linear total regret</ul><p>초기 Value를 높게 설정해두고 시작함으로써 Exploration이 저절로 일어나지만 여전히 suboptimal 수렴은 가능하다. 간단하지만 꽤 괜찮은 접근법</p><h4 id="decating-epsilon_t-greedy-algorithm">Decating $\epsilon_t$-Greedy Algorithm</h4><ul><li>Pick a decay schedule for $\epsilon_1, \epsilon_2, \dots$<li>Consider the following schedule</ul>\[\begin{aligned} \mathsf{ c } &amp;&gt; \mathsf{ 0 } \\ \mathsf{ d } &amp;= \mathsf{ \underset{a\vert\Delta_a&gt;0}{min}\; \Delta_i } \\ \mathsf{ \epsilon_t } &amp;= \mathsf{ min \lbrace 1, \frac{c\vert \mathcal{A}\vert}{d^2t} \rbrace } \end{aligned}\]<ul><li>Decaying $\epsilon_t$-greedy has logarithmic asymptotic total regret!<li>Unfortunately, schedule requires advance knowledge of gaps<li>Goal: find an algorithm with sublinear regret for any multi-armed bandit (without knowledge of $\mathcal{R}$)</ul><p>gap notation 또 엉망이다 action 끼리의 regret 차이라고 한다. 그리고 regret은 실제로 알수 없는 값이므로 이론적인 내용일뿐 아직 의미는 없다. 그냥 exploration이 decay해서 regret이 logarithmic 해지고 기회비용이 줄었을 뿐</p><h3 id="lower-bound">Lower Bound</h3><ul><li>The performance of any algorithm is determined by similarity between optimal arm and other arms<li>Hard problems have similar-looking arms with different means<li>This is described formally by the gap $\Delta_a$ and the similarity in distributions $\mathsf{KL(\mathcal{R}^a\Vert\mathcal{R}^a *)}$</ul><div class="table-wrapper"><table><thead><tr><th>Theorem (Lai and Robbins<tbody><tr><td>Asymptotic total regret is at least logarithmic in number of steps<br /><center>$$ \mathsf{ \underset{t\rightarrow\infty}{\lim}\; L_t \leq \log\, t \displaystyle\sum_{a\vert\Delta_a&gt;0} \frac{\Delta_a}{KL(\mathcal{R}^a\Vert \mathcal{R}^{a^*})} } $$</center></table></div><p>흠 상황과 모델을 만들기에 따라 극복 가능할것 같은 느낌도 있지만 굳이. 근데 이걸 왜 하는지 나오려나?</p><h3 id="upper-confidence-bound">Upper Confidence Bound</h3><h4 id="optimism-in-the-face-of-uncertainty">Optimism in the Face of Uncertainty</h4><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 700px 400px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/Note9_figure1.JPG" alt="graph" width="700px" height="400px" class="text-center" /></p><ul><li>Which action should we pick?<li>The more uncertain we are about an action-value<li>The more important it is to eplore that action<li><p>It could turn out to be the best action</p><li>After picking blue action<li>We are less uncertain about the value<li>And more likely to pick another action<li>Until we home in on best action</ul><p>흠 충분히 data를 수집한 뒤에도 1번 그래프가 유지된다면 그래도 1번을 고를 것인가</p><h4 id="upper-confidence-bounds">Upper Confidence Bounds</h4><ul><li>Estimate an upper confidence $\mathsf{\hat{U}_t(a)}$ for each action value<li>Such that $\mathsf{Q(a)\leq\hat{Q}_t(a)+\hat{U}_t(a)}$ with high probability<li>This depends on the number of times $\mathsf{N(a)}$ has been selected<ul><li>Small $\mathsf{N_t(a) \Rightarrow}$ large $\mathsf{\hat{U}_t(a)}$ (estimated value is uncertain)<li>Large $\mathsf{N_t(a) \Rightarrow}$ small $\mathsf{\hat{U}_t(a)}$ (estimated value is accurate)</ul><li>Select action maximising Upper Confidence Bound (UCB)</ul>\[\mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; \hat{Q}_t(a) + \hat{U}_t(a) }\]<p>평균적인 reward 기대값이 큰 action이 아니라불확실성이 있는 action을 선택할 가능성을 높이기 위한 방법으로써 U를 사용하는 것.</p><h4 id="hoeffdings-inequality">Hoeffding’s Inequality</h4><div class="table-wrapper"><table><thead><tr><th>Theorem (Hoeffding’s Inequality)<tbody><tr><td>Let \(\mathsf{X_1,\dots ,X_t}\) be i.i.d. random variables in [0,1], and let <br />\(\mathsf{ \bar{X}_t = \frac{1}{t}} \sum^t_{\tau=1} X_\tau }\) be the sample mean. Then<br /><center>$$\mathsf{ \mathbb{P}[\mathbb{E}[X]&gt;\bar{X}_t+u]\leq e^{-2tu^2} }$$</center></table></div><ul><li>We will apply Hoeffding’s Inequality to rewards of the bandit<li>conditioned on selecting action a</ul>\[\mathsf{ \mathbb{P} \left[ Q(a) &gt; \hat{Q}_t(a) + U_t(a) \right] \leq e^{-2N_t(a)U_t(a)^2} }\]<h4 id="calculating-upper-confidence-bounds">Calculating Upper Confidence Bounds</h4><ul><li>Pick a probability p that true value exceeds UCB<li>Now solve for $\mathsf{U_t(a)}$</ul>\[\begin{aligned} \mathsf{ e^{-2N_t(a)U_t(a)^2} } &amp;= \mathsf{ p } \\ \mathsf{ U_t(a) } &amp;= \mathsf{ \sqrt{\frac{-\log p}{2N_t(a)}} } \end{aligned}\]<ul><li>Reduce p as we observe more rewards, e.g. $\mathsf{p=t^{-4}}$<li>Ensures we select optimal action as $\mathsf{t\rightarrow \infty}$</ul>\[\mathsf{ U_t(a) = \sqrt{\frac{-\log p}{2N_t(a)}} }\]<h4 id="ucb1">UCB1</h4><ul><li>This leads to the UCB1 algorithm</ul>\[\mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(a) + \sqrt{\frac{2\log t}{N_t(a)}} }\]<div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>The UCB algorithm achieves logarithmic asymtotic total regret<br /><center>$$\mathsf{ \displaystyle\lim_{t\rightarrow\infty} L_t \leq 8\log t \displaystyle\sum_{a\vert\Delta_a&gt;0} \Delta_a }$$</center></table></div><p>UCB중 확률이 decay하는 \(t^{-4}\)를 사용하는 알고리즘</p><h3 id="bayesisan-bandits">Bayesisan Bandits</h3><ul><li>So far we have made no assumptions about the reward distribution $\mathcal{R}$<ul><li>Except bounds on rewards</ul><li>Bayesian bandits exploit prior knowledge of rewards, $\mathsf{p}[\mathcal{R}]$<li>They compute posterior distribution of reward $\mathsf{p[\mathcal{R}\vert h_t]}$<ul><li>where $\mathsf{h_t=a_1,r_1,\dots a_{t-1},r_{t-1}}$ is the history</ul><li>Use posterior to guide exploration<ul><li>Upper confidence bounds (Bayesian UCB)<li>Probability matching (Thompson sampling)</ul><li>Better performance if prior knowledge is accurate</ul><h4 id="bayesian-ucb-example-independent-gaussians">Bayesian UCB Example: Independent Gaussians</h4><ul><li>Assume reward distribution is Gaussian, $\mathsf{\mathcal{R}_a(r) = \mathcal{N}(r;\mu_a,\sigma^2_a)}$<li>Compute Gaussian posterior over $\mu_a$ and $\sigma_a^2$ (by Bayes law)</ul>\[\mathsf{ p[\mu_a, \sigma^2_a \vert h_t] \propto p[\mu_a,\sigma^2_a] \displaystyle\prod_{t\vert a_t=a} \mathcal{N}(r_t; \mu_a,\sigma^2_a) }\]<ul><li>Pick action that maximises standard deviation of $\mathsf{Q(a)}$</ul>\[\mathsf{ a_t = argmax \;\mu_a + c\sigma_a/\sqrt{N(a)} }\]<h4 id="probability-matching">Probability Matching</h4><ul><li>Probability matching selects action a according to probability that a is the optimal action</ul>\[\mathsf{ \pi(a\vert h_t) = \mathbb{P} [Q(a) &gt; Q(a'), \forall a' \neq a \vert h_t] }\]<ul><li>Probability matching is optimistic in the face of uncertainty<ul><li>Uncertain actions have higher probability of being max</ul><li>Can be difficult to compute anlytically from posterior</ul><h4 id="thompson-sampling">Thompson Sampling</h4><ul><li>Thompson sampling implements probability matching</ul>\[\begin{aligned} \mathsf{ \pi(a\vert h_t) } &amp;= \mathsf{ \mathbb{P} [ Q(a) &gt; Q(a'), \forall a' \neq a \vert h_t ] } \\ &amp;= \mathsf{ \mathbb{E}_{\mathcal{R}\vert h_t} \left[ 1(a=\underset{a\in\mathcal{A}}{argmax}\; Q(a))\right] } \end{aligned}\]<ul><li>Use Bayes law to compute posterior distribution $\mathsf{p[\mathcal{R}\vert h_t]}$<li>Sample a reward distribution $\mathcal{R}$ from posterior<li>Compute action-value function $\mathsf{Q(a) = \mathbb{E}[\mathcal{R}_a]}$<li>Select action maximising value on ssample, $\mathsf{a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(a)}$<li>Thompson sampling achieves Lai and Robbins lower bound!</ul><h4 id="value-of-information">Value of Information</h4><ul><li>Exploration is useful because it gains information<li>Can we quantify the value of information?<ul><li>How much reward a decision-maker would be prepared to pay in order to have that information, prior to making a decision<li>Long-term reward after getting information - immediate reward</ul><li>Information gain is higher in uncertain situations<li>Therefor it makes sense to explore uncertain situations more<li>If we know value of information, we can trade-off exploration and exploitation optimally</ul><p>exploration은 거의 random하게 시행되지만 좀더 smart, logically efficient한 방법이 있지않을까, 여기서는 uncertatinty만 쫓아서 모든 tree를 검증하는 방향으로만 하고 있다.</p><h3 id="information-state-search">Information State Search</h3><ul><li>We have viewed bandits as one-step decision-making problems<li>Can also view as sequential decision-making problems<li>At each step there is an information state $\mathsf{\tilde{s}}$<ul><li>$\mathsf{\tilde{s}}$ is a statistic of the history, $\mathsf{\tilde{s_t} = f(h_t)}$<li>summarising all information accumulated so far</ul><li>Each action a causes a transition to a new information state $\mathsf{\tilde{s}’}$ (by adding information), with probability $\mathsf{\mathcal{\tilde{P}}^a_{\tilde{s},\tilde{s}’}}$<li>This defines MDP $\mathcal{\tilde{M}}$ in augmented information state space</ul>\[\mathcal{ \tilde{M} = \langle \tilde{S}, A, \tilde{P}, R, \gamma\rangle }\]<h4 id="example-bernoulli-bandits">Example: Bernoulli Bandits</h4><ul><li>Consider a Bernoulli bandit, such that $\mathsf{\mathcal{R}^a = \mathcal{B}(\mu_a)}$<li>e.g. Win or lose a game with probability $\mu_a$<li>Want to find which arm has the highest $\mu_a$<li>The information state is $\mathsf{\tilde{s}} = \langle\alpha,\beta\rangle$<ul><li>$\alpha_a$ counts the pulls of arm a where reward was 0<li>$\beta_a$ counts the pulls of arm a where reward was 1</ul></ul><h4 id="solving-information-state-space-bandits">Solving Information State Space Bandits</h4><ul><li>We now have an infinite MDP over information states<li>This MDP can be solved by reinforcement learning<li>Model-free reinforcement learning<ul><li>e.g. Q-learning (Duff, 1994)</ul><li>Bayesian model-based reinforcement learning<ul><li>e.g. Gittins indices (Gittins, 1979)<li>This approach is known as Bayes-adaptive RL<li>Finds Bayes-optimal exploration/exploitation trade-off with respect to prior distribution</ul></ul><h4 id="bayes-adaptive-bernoulli-bandits">Bayes-Adaptive Bernoulli Bandits</h4><ul><li>Start with Beta$(\alpha_a, \beta_a)$ prior over reward function $\mathcal{R}^a$<li>Each time a is selected, update posterior for \(\mathcal{R}^a\)<ul><li>Beta($\alpha_a + 1 ,\beta_a$) if $r= 0$<li>Beta(\(\alpha_a,\beta_a+1\)) if \(r=1\)</ul><li>This defines transition function \(\mathcal{\tilde{P}}\) for the Bayes-adaptive MDP<li>Information state \(\langle\alpha,\beta\rangle\) corresponds to reward model Beta(\(\alpha,\beta\))<li>Each state transition corresponds to a Bayesian model update</ul><h4 id="gittins-indices-for-bercoulli-bandits">Gittins Indices for Bercoulli Bandits</h4><ul><li>Bayes-adaptive MDP can be solved by dynamic programming<li>The solution is known as the Gittins index<li>Exact solution to Bayes-adaptive MDP is typically intractable<ul><li>Information state space is too large</ul><li>Recent idea: apply simulation-based search (Guez et al. 2012)<ul><li>Forward search in information state space<li>Using simulations from current information state</ul></ul><h2 id="contextual-bandits">Contextual Bandits</h2><ul><li>A contextual bandit is a tuple \(\langle\mathcal{A,S,R}\rangle\)<li>\(\mathcal{A}\) is a known set of actions (or “arms”)<li>\(\mathcal{S= \mathbb{P}}\mathsf{[s]}\) is an unknown distribution over states (or “contexts”)<li>\(\mathsf{\mathcal{R}^a_s(r) = \mathbb{P}[r\vert s,a]}\) is an unknown probability distribution over rewards<li>At each step t<ul><li>Environmnet generates state \(s_t\sim \mathcal{S}\)<li>Agent selects action \(\mathsf{a_t}\in\mathcal{A}\)<li>Environment generates reward \(\mathsf{r_t}\sim\mathcal{R}^{a_t}_{s_t}\)</ul><li>Goal is to maximise cumulative reward \(\mathsf{\sum^t_{\tau=1} r_\tau}\)</ul><h3 id="linear-ucb">Linear UCB</h3><h4 id="linear-regression">Linear Regression</h4><ul><li>Action-value function is expected reward for state s and action a</ul>\[\mathsf{Q(s,a) = \mathbb{E}[r\vert s,a]}\]<ul><li>Estimate value function with a linear function approximator</ul>\[\mathsf{ Q_\theta(s,a) = \phi (s,a)^\top \theta \approx Q(s,a) }\]<ul><li>Estimate parameters by least squares regression</ul>\[\begin{aligned} \mathsf{ A_t } &amp;= \mathsf{ \displaystyle\sum^t_{\tau=1} \phi(s_\tau , a_\tau) \phi(s_\tau, a_\tau)^\top } \\ \mathsf{ b_t } &amp;= \mathsf{ \displaystyle\sum^t_{\tau=1} \phi(s_\tau,a_\tau)r_\tau } \\ \mathsf{ \theta_t } &amp;= \mathsf{ A_t^{-1} b_t } \end{aligned}\]<h4 id="linear-upper-confidence-bounds">Linear Upper Confidence Bounds</h4><ul><li>Least squares regression estimates the mean action-value \(\mathsf{Q_\theta(s,a)}\)<li>But it can also estimate the variance of the action-value \(\mathsf{\sigma^2_\theta(s,a)}\)<li>i.e. the uncertainty due to parameter estimation error<li>Add on a bonus for uncertainty, \(\mathsf{U_\theta(s,a) = c\sigma}\)<li>i.e. define UCB to be c standard deviations above the mean</ul><h4 id="geometric-interpretation">Geometric Interpretation</h4><ul><li>Define confidence ellipsoid \(\mathcal{E}_t\) around parameters \(\theta_t\)<li>Such that \(\mathcal{E}_t\) includes true parameters \(\theta^*\) with high probability<li>Use this ellipsoid to estimate the uncertainty of action values<li>Pick parameters within ellipsoid that maximise action value</ul>\[\mathsf{\underset{\theta\in\mathcal{E}}{argmax}\; Q_\theta(s,a)}\]<h4 id="calculating-linear-upper-confdence-bounds">Calculating Linear Upper Confdence Bounds</h4><ul><li>For least quares regression, parameter covariance is \(\mathsf{A^{-1}}\)<li>Action-value is linear in features, \(\mathsf{Q_\theta(s,a) = \phi (s,a)^\top \theta}\)<li>So action-value variance is quadratic, \(\mathsf{\sigma^2_\theta(s,a) = \phi(s,a)^\top A^{-1} \phi(s,a)}\)<li>Upper confidence bound is \(\mathsf{Q_\theta(s_t,a) + c\sqrt{\phi(s,a)^\top A^{-1} \phi(s,a)}}\)<li>Select action maximising upper confidence bound</ul>\[\mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q_\theta(s_t,a)+ c\sqrt{\phi(s_t,a)^\top A^{-1}_t \phi(s_t,a)}}\]<h2 id="mdps">MDPs</h2><p>Exploration/Exploitation Principles to MDPS</p><p>The same principles for exploration/exploitation apply to MDPs</p><ul><li>Naive Exploration<li>Optimistic Initialisation<li>Optimism in the Face of Uncertainty<li>Probability Matching<li>Information State Search</ul><h3 id="optimistic-initialisations">Optimistic Initialisations</h3><h4 id="optimistic-initialisation-model-free-rl">Optimistic Initialisation: Model-Free RL</h4><ul><li>Initialise action-value function Q(s,a) to \(\frac{r_{max}}{1-\gamma}\)<li>Run favourite model-free RL algorithm<ul><li>Monte-Carlo control<li>Sarsa<li>Q-learning<li>…</ul><li>Encourages systematic exploration of states and actions</ul><h4 id="optimistic-initialisation-model-based-rl">Optimistic Initialisation: Model-Based RL</h4><ul><li>Construct an optimistic model of the MDP<li>Initialise transitions to go to heaven<ul><li>(i.e. transition to terminal state with \(r_{max}\) reward)</ul><li>Solve optimistic MDP by favourite planning algorithm<ul><li>policy iteration<li>value iteration<li>tree search<li>…</ul><li>Encourages systematic exploration of states and actions<li>e.g. RMax algorithm (Brafman and Tennenholtz)</ul><h3 id="optimism-in-the-face-of-uncertainty-1">Optimism in the Face of Uncertainty</h3><h4 id="upper-confidence-bounds-model-free-rl">Upper Confidence Bounds: Model-Free RL</h4><ul><li>Maximise UCB on action-value function \(\mathsf{Q^\pi (s,a)}\) <br /><center>$$\mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(s_t,a) + U(s_t,a) }$$</center><ul><li>Estimate uncertainty in policy evaluation (easy)<li>Ignores uncertainty from policy improvement</ul><li>Maximise UCB on optimal action-value function \(\mathsf{Q^*(s,a)}\) <br /><center>$$\mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(s_t,a) + U_1(s_t,a) + U_2(s_t,a) }$$</center><ul><li>Estimate uncertainty in policy evaluation (easy)<li>plus uncertainty from policy improvement (hard)</ul></ul><h4 id="bayesian-model-based-rl">Bayesian Model-Based RL</h4><ul><li>Maintain posterior distribution over MDP models<li>Estimate both transitions and rewards, \(\mathsf{p[\mathcal{P,R}\vert h_t]}\)<ul><li>where \(\mathsf{h_t = s_1,a_1,r_2,\dots ,s_t}\) is the history</ul><li>Use posterior to guide exploration<ul><li>Upper confidence bounds (Bayesian UCB)<li>Probability matching (Thompson sampling)</ul></ul><h3 id="probability-matching-1">Probability Matching</h3><h4 id="thompson-sampling-model-based-rl">Thompson Sampling: Model-Based RL</h4><ul><li>Thompson sampling implements probability matching</ul>\[\begin{aligned} \mathsf{ \pi(s,a,\vert h_t)} &amp;= \mathsf{ \mathbb{P} [Q^*(s,a) &gt; Q^*(s,a'), \forall a' \neq a \vert h_t] } \\ &amp;= \mathsf{ \mathbb{E}_{\mathcal{P,R}\vert h_t} \left[ 1(a=\underset{a\in\mathcal{A}}{argmax}\; Q^*(s,a))\right] } \end{aligned}\]<ul><li>Use Bayes law to compute posterior distribution \(\mathsf{p[\mathcal{P,R}\vert h_t]}\)<li>Sample an MDP \(\mathcal{P,R}\) from posterior<li>Solve MDP using favourite planning algorithm to get \(\mathsf{Q^*(s,a)}\)<li>Select optimal action for sample MDP, \(\mathsf{a_t = \underset{a\in\mathcal{A}}{argmax}\; Q^*(s_t,a)}\)</ul><h3 id="information-state-search-1">Information State Search</h3><h4 id="information-state-search-in-mdps">Information State Search in MDPs</h4><ul><li>MDPs can be augmented to include infromation state<li>Now the augmented state is \(\langle s,\tilde{s}\rangle\)<ul><li>where s is original state within MDP<li>and \(\tilde{s}\) is a statistic of the history (accumulated information)</ul><li>Each action a causes a transition<ul><li>to a new state s’ with probability \(\mathcal{P}^a_{s,s'}\)<li>to a new information state \(\tilde{s}'\)</ul><li>Defines MDP \(\mathcal{\tilde{M}}\) in augmented information state space</ul>\[\mathcal{\tilde{M} = \langle \tilde{S},A,\tilde{P},R,\gamma\rangle}\]<h4 id="bayes-adaptive-mdps">Bayes Adaptive MDPs</h4><ul><li>Posterior distribution over MDP model is an information state</ul>\[\mathsf{\tilde{s}_t = \mathbb{P}[\mathcal{P,R}\vert h_t]}\]<ul><li>Augmented MDP over \(\mathsf{\langle s, \tilde{s}\rangle}\) is called Bayes-adaptive MDP<li>Solve this MDP to find optimal exploration/exploitation trade-off (with respect to prior)<li>However, Bayes-adaptive MDP is typically enormous<li>Simulation-based search has proven effective (Guez et al.)</ul><h4 id="conclusion">Conclusion</h4><ul><li>Have covered several principles for exploration/exploitation<ul><li>Naive methods such as $\epsilon$-greedy<li>Optimistic initialisation<li>Upper confidence bounds<li>Probability matching<li>Information state search</ul><li>Each principle was developed in bandit setting<li>But same principles also apply to MDP setting</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 9 Exploration and Exploitation - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-9/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 9 Exploration and Exploitation - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-9/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 9 Exploration and Exploitation - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-9/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-8/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 8 Integrating Learning and Planning</p></a> <a href="/posts/Review-Alphago-Zero/" class="btn btn-outline-primary" prompt="Newer"><p>Review - Article Alphago Zero</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
