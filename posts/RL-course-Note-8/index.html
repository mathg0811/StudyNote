<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 8 Integrating Learning and Planning" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-8/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-8/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-04T18:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 8 Integrating Learning and Planning" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-8/","@type":"BlogPosting","headline":"RLcourse note - Lecture 8 Integrating Learning and Planning","dateModified":"2022-01-17T21:27:24+09:00","datePublished":"2022-01-04T18:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-8/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 8 Integrating Learning and Planning | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 8 Integrating Learning and Planning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 8 Integrating Learning and Planning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 4, 2022, 6:00 PM +0900" >Jan 4<i class="unloaded">2022-01-04T18:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 9:27 PM +0900" >Jan 17<i class="unloaded">2022-01-17T21:27:24+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2258 words">12 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/ItMutbeOHtc"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec8_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h3 id="8강-소감">8강 소감</h3><p>Tree Search 의 장점은 알 수 없는 Model을 가정하여 거기에서 현재까지 학습된 policy에 따라 여러 시나리오들을 진행해보고 그를 바탕으로 Value를 빠르게 계산해볼 수 있다는 것이다. 강의에서는 Model로부터 Sample Experience를 확보하여 빠른 학습이 가능하다는 것을 중점으로 두고 설명하고 있으나, 사실 이 부분은 논리적으로 빠른 학습은 어려워 보인다. 특히 모델의 오차를 더 강화할 수도 있다는 부분에서 그렇다. 그러나 Tree Search 과정은 상당히 유의미한데, 이 방법을 통해 실제 Policy를 기반으로 유의미한 미래 예측과 그 Return 예측을 통해 Action을 선택할 수 있게 함으로써 무가치한 미래 예측에 할당될 자원을 절약할 수 있고 보다 빠르게 실시간으로 Model을 완벽히 모르는 상태에서도 최선의 선택을 할 수 있다는 점이다. 이 구조는 실제 인간 등이 선택을 하는 사고 과정과도 유사하지 않을까?</p><h2 id="introduction">Introduction</h2><p>Learn model directly from experience, and use planning to construct a value function or policy. Integrate learning and planning into a single architecture</p><p>model을 경험으로부터 학습한다고 하는데, 이게 제일 중요한 부분인거 같은데 잘 다뤄져있지 않다. 이 실제 environment에 근접하는 model을 결정하는 부분이야말로 RL의 학습을 결정하는 가장 중요한 부분이 아닐까?</p><h3 id="model-based-and-model-free-rl">Model-Based and Model-Free RL</h3><ul><li>Model-Free RL<ul><li>No model<li>Learn value function (and/or policy) from experience</ul><li>Model-Based RL<ul><li>Learn a model from experience<li>Plan value function (and/or policy) from model</ul></ul><h2 id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</h2><p>Advantages:</p><ul><li>Can efficiently learn model by supervised learning methods<li>Can reason about model uncertainty</ul><p>Disadvantages:</p><ul><li>First learn a model, then construct a value function <br />$\Rightarrow$ two sources of approximation error</ul><h3 id="model">Model</h3><ul><li>A model $\mathcal{M}$ is a representation of an MDP $\langle\mathcal{S,A,P,R}\rangle$ parametrized by $\eta$<li>We will assume state space $\mathcal{S}$ and action space $\mathcal{A}$ are known<li>So a model $\mathcal{M=\langle P_\eta ,R_\eta \rangle}$ represents state transitions $\mathcal{P_\eta \approx P}$ and rewards $\mathcal{R_\eta \approx R}$</ul>\[\begin{aligned} \mathsf{ S_{t+1} } &amp; \sim \mathsf{ \mathcal{P}_\eta(S_{t+1}\;\vert\;S_t, A_t) } \\ \mathsf{ R_{t+1} } &amp;= \mathsf{ \mathcal{R}_\eta(R_{t+1}\;\vert\;S_t, A_t) } \end{aligned}\]<ul><li>Typically assume conditional independence between state transitions and rewards</ul>\[\mathsf{ \mathbb{P}[S_{t+1}, R_{t+1}\;\vert\;S_t, A_t] = \mathbb{P}[S_{t+1}\;\vert\;S_t, A_t]\mathbb{P}[R_{t+1}\;\vert\;S_t, A_t] }\]<p>경험으로부터 Model의 P, R을 기록하고 이를 통해 비슷한 결과를 내는 모델을 만든다. 즉 모델을 확률적인 state 변화와 Reward로 준다는 것이다. 이보다 중요한 구성 요소가 충분히 있을 것 같은데 전부 필드 개발자의 직관에 맡기겠다고 말하는 것 같다. State와 Action, Probability와 Reward를 잘 구성하기 어려울 것 같은데. 단순한 문제 또는 Model을 완벽하게 알고 있는 문제가 아니고서야.</p><p>또한 Action의 결과로 어느 State로 전이되는지를 은근슬쩍 무시하기 시작했다. 어쩐지 지난 강의에서 Reward중 next state 부분을 없애버리더라. 이렇게 은근슬쩍 하나씩 맘대로 하는게 쌓이고 쌓인다. 물론 그 모든 조건을 만족하도록 Model을 만들어낼 수 있을지도 모른다. AI가 점점 어려워지겠지</p><h4 id="model-learning">Model Learning</h4><ul><li>Goal: estimate model $\mathcal{M}_\eta$ from experience $\lbrace \mathsf{ S_1, A_1, R_2, \dots, S_T } \rbrace $<li>This is a supervised learning problem</ul>\[\begin{aligned} \mathsf{ S_1, A_1 } &amp;\rightarrow \mathsf{ R_2,S_2 } \\ \mathsf{ S_2, A_2 } &amp;\rightarrow \mathsf{ R_3,S_3 } \\ &amp;\vdots\\ \mathsf{ S_{T-1}, A_{T-1} } &amp;\rightarrow \mathsf{ R_T,S_T } \end{aligned}\]<ul><li>Learning $\mathsf{s,a,\rightarrow r}$ is a regression problem<li>Learning $\mathsf{s,a,\rightarrow s’}$ is a density estimation problem<li>Pick loss function, e.g. mean-squared error, KL divergence, $\dots$<li>Find parameters $\eta$ that minimise empirical loss</ul><p>Reward가 항상 Regression으로 결정될 수 있는 건 아닐 것 같은데, 전이 되는 state 에 따라 Reward가 바뀐다면 s,a 만 가지고는 결정할 수 없다.</p><h4 id="examples-of-models">Examples of Models</h4><ul><li>Table Lookup Model<li>Linear Expectation Model<li>Linear Gaussian Model<li>Gaussian Process Model<li>Deep Belief Network Model<li>…</ul><h4 id="table-lookup-model">Table Lookup Model</h4><ul><li>Model is an explicit MDP, $\mathcal{\hat{P}, \hat{R}}$<li>Count visits $\mathsf{N(s,a)}$ to each state action pair</ul>\[\begin{aligned} \mathsf{ \mathcal{\hat{P}}^a_{s,s'} } &amp;= \mathsf{ \frac{1}{N(s,a)} \displaystyle\sum^T_{t=1} 1(S_t,A_t,S_{t+1} = s,a,s') } \\ \mathsf{ \mathcal{\hat{R}}^a_s } &amp;= \mathsf{ \frac{1}{N(s,a)} \displaystyle\sum^T_{t=1} 1(S_t,A_t = s,a)R_t } \end{aligned}\]<ul><li>Alternatively<ul><li>At each time-step t, record experience tuple <br />\(\mathsf{ \langle S_t, A_t, R_{t+1}, S_{t+1} \rangle }\)<li>To sample, model, randomly pick tuple matching $ \mathsf{ \langle s,a, \cdot, \cdot \rangle } $</ul></ul><h3 id="planning-with-a-model">Planning with a Model</h3><ul><li>Given a model $\mathcal{M_\eta = \langle P_\eta, R_\eta \rangle}$<li>Solve the MDP $\langle \mathcal{S,A,P_\eta, R_\eta} \rangle$<li>Using favourite planning algorithm<ul><li>Value iteration<li>Policy iteration<li>Tree search<li>…</ul></ul><h4 id="sample-based-planning">Sample-Based Planning</h4><ul><li>A simple but powerful approach to planning<li>Use the model only to generate samples<li>Sample experience from model</ul>\[\begin{aligned} \mathsf{ S_{t+1} } &amp; \sim \mathsf{ \mathcal{P}_\eta(S_{t+1}\;\vert\;S_t, A_t) } \\ \mathsf{ R_{t+1} } &amp;= \mathsf{ \mathcal{R}_\eta(R_{t+1}\;\vert\;S_t, A_t) } \end{aligned}\]<ul><li>Apply model-free RL to samples, e.g.:<ul><li>Monte-Carlo control<li>Sarsa<li>Q-learning</ul><li>Sample-based planning methods are often more efficient</ul><h4 id="planning-with-an-inaccurate-model">Planning with an Inaccurate Model</h4><ul><li>Given an imperfect model $\langle \mathcal{P_\eta, R_\eta} \rangle \neq \langle \mathcal{P, R} \rangle$<li>Performance of model-based RL is limited to optimal policy for approximate MDP $\langle \mathcal{S,A,P_\eta, R_\eta} \rangle$<li>i.e. Model-based RL is only as good as the estimated model<li>When the model is inaccurate, planning process will compute a suboptimal policy<li>Solution 1: when model is wrong, use model-free RL<li>Solution 2: reason explicitly about model uncertainty</ul><p>이런 사례가 많이 생길것 같은데 그 이유는 여태까지 쌓아온 가정이 너무 많으면서 굳이 필요없어보이는 부분까지도 가정하기 때문이라는 느낌</p><h2 id="integrated-architectures">Integrated Architectures</h2><h3 id="dyna">Dyna</h3><h4 id="real-and-simulated-experience">Real and Simulated Experience</h4><p>We consider two sources of experience</p><p>Real experience - Sample from envrionment (True MDP)</p>\[\begin{aligned} \mathsf{ S' } &amp; \sim \mathsf{ \mathcal{P}^a_{s,s'} } \\ \mathsf{ R } &amp;= \mathsf{ \mathcal{R}^a_s } \end{aligned}\]<p>Simulated experience - Sampled from model (approximate MDP)</p>\[\begin{aligned} \mathsf{ S' } &amp; \sim \mathsf{ \mathcal{P}_\eta(S'\;\vert\;S, A) } \\ \mathsf{ R } &amp;= \mathsf{ \mathcal{R}_\eta(R\;\vert\;S, A) } \end{aligned}\]<h4 id="integrating-learning-and-planning">Integrating Learning and Planning</h4><ul><li>Model-Free RL<ul><li>No model<li>Learn value funciton (and/or policy) from real experience</ul><li>Model-Based RL (using Sample-Based Planning)<ul><li>Learn a model from real experience<li>Plan value function (and/or policy) from simulated experience</ul><li>Dyna<ul><li>Learn a model from real experience<li>Learn and plan value function (and/or policy) from real and simlated experience</ul></ul><h4 id="dyna-q-algorithm">Dyna-Q Algorithm</h4><div class="table-wrapper"><table><tbody><tr><td>Initialize Q(s,a) and Model(s,a) for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$<br />Do forever:<br />$\quad$ (a) $S\leftarrow$ current (nonterminal) state<br />$\quad$ (b) $A\leftarrow\epsilon$-greedy (S,A) <br />$\quad$ (c) Execute action A; observe resultant reward, R, and state, S’<br />$\quad$ (d) $Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma\; max_a Q(S’,a)-Q(S,A)]$<br />$\quad$ (e) $Model(S,A)\leftarrow R,S’$ (assuming deterministic environment)<br />$\quad$ (f) Repeat n times: <br /> $\quad\quad$ $S\leftarrow$ random previously observed state<br /> $\quad\quad$ $A\leftarrow$ random action previously taken in S <br /> $\quad\quad$ $R,S’\leftarrow Model(S,A)$ <br /> $\quad\quad$ $Q(S,A)\leftarrow Q(S,A) + \alpha [R + \gamma max_a Q(S’,a)-Q(S,A)]$</table></div><p>Model에서 추가적인 exeprience를 얻어 Value를 학습시키는 것은 적은 episode로도 빠르게 Value를 학습시킬 수 있을 지 모르나, Model의 정확성은 Real Experience가 충분히 많은 수로 확보되고 Model이 실제 Environment에 가까울 때에만 믿을 수 있다. 때로 초반에 잘못된 Model을 구성하도록 유도할 수도 있으며 장기적으로도 크게 유의미할것 같지는 않다.</p><h2 id="simulation-based-search">Simulation-Based Search</h2><h3 id="forward-search">Forward Search</h3><ul><li>Forward search algorithms select the best action by lookahead<li>They build a search tree with the current state $s_t$ at the root<li>Using a model of the MDP to look ahead<li>No need to solve whole MDP, just sub-MDP starting from now</ul><p>Simulation-Based Search</p><ul><li>Forward search paradigm using sample-based planning<li>Simulate episodes of experience from now with the model<li>Apply model-free RL to simulated episodes<li>Simulate episodes of experience from now with the model</ul>\[\mathsf{ \lbrace s^k_t, A^k_t, R^k_{t+1}, \dots, S^k_T \rbrace ^K_{k=1} \sim \mathcal{M}_v }\]<ul><li>Apply model-free RL to simulated episodes<ul><li>Monte-Carlo control $\rightarrow$ Monte-Carlo search<li>Sarsa $\rightarrow$ TD search</ul></ul><h3 id="monte-carlo-search">Monte-Carlo Search</h3><h4 id="simple-monter-carlo-search">Simple Monter-Carlo Search</h4><ul><li>Given a model $\mathcal{M}_v$ and a simulation policy $\pi$<li>For each action $\mathsf{a}\in\mathcal{A}$<ul><li>Simulate K episodes from current (real) state $\mathsf{s_t}$ <br /><center>$$ \mathsf{ \lbrace s_t, a, R^k_{t+1}, S^k_{t+1}, A^k_{t+1} \dots, S^k_T \rbrace ^K_{k=1} \sim \mathcal{M}_v,\pi } $$</center><li>Evaluate actions by mean return (Monte-Carlo evaluation) <br /><center>$$ \mathsf{ Q(s_t,a) = \frac{1}{K} \displaystyle\sum^K_{k=1} G_t \overset{P}{\rightarrow} q_\pi(s_t,a) } $$</center></ul><li>Select current (real) action with maximum value <br /><center>$$ \mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(s_t,a) } $$</center></ul><h4 id="monte-carlo-tree-search-evaluation">Monte-Carlo Tree Search (Evaluation)</h4><ul><li>Given a model $\mathcal{M}_v$<li>Simulate K episodes from current state $\mathsf{s_t}$ using current simulation policy $\pi$ <br /><center>$$ \mathsf{ \lbrace s_t, A^k_{t+1}, R^k_{t+1}, S^k_{t+1}, \dots, S^k_T \rbrace ^K_{k=1} \sim \mathcal{M}_v,\pi } $$</center><li>Build a search tree containing visited states and actions<li>Evaluate states Q(s,a) by mean return of episodes from s, a <br /><center>$$ \mathsf{ Q(s,a) = \frac{1}{N(s,a)} \displaystyle\sum^K_{k=1}\sum^T_{u=t} 1(S_u, A_u = s,a)G_u \overset{P}{\rightarrow} q_\pi(s_t,a) } $$</center><li>After search is finished, select current (real) action with maximum value in search tree <br /><center>$$ \mathsf{ a_t = \underset{a\in\mathcal{A}}{argmax}\; Q(s_t,a) } $$</center></ul><h4 id="monte-carlo-tree-search-simulation">Monte-Carlo Tree Search (Simulation)</h4><ul><li>MCTS, the simulation policy $\pi$ improves<li>Each simulation consists of two phases (in-tree, out-of-tree)<ul><li>Tree policy (improves): pick actions to maximise Q(S,A)<li>Default policy (fixed): pick actions randomly</ul><li>Repeat (each simulation)<ul><li>Evaluate states Q(S,A) by Monte-Carlo evaluation<li>Improve tree policy, e.g. by $\epsilon$-greedy(Q)</ul><li>Monte-Carlo control applied to simulated experience<li>Converges on the optimal search tree, $\mathsf{ Q(S,A) \rightarrow q_*(S,A)}$</ul><h4 id="position-evaluation-in-go">Position Evaluation in Go</h4><ul><li>How good is a position s?<li>Reward function (undiscounted):</ul>\[\begin{aligned} \mathsf{ R_t } &amp;= \mathsf{ 0 \text{ for all non-terminal steps } t&lt;T } \\ \mathsf{ R_T } &amp;= \begin{cases} 1 &amp; \text{ if Balck wins } \\ 0 &amp; \text{ if White wins } \end{cases} \end{aligned}\]<ul><li>Policy $\mathsf{\pi = \langle\pi_B , \pi_W\rangle}$ selects moves for both players<li>Value function (how good is position s):</ul>\[\begin{aligned} \mathsf{v_\pi(s)} &amp;= \mathsf{ \mathbb{E}_\pi [R_T \vert S=s] = \mathbb{P} [\text{Black wins } \vert S=s] } \\ \mathsf{v_*(s)} &amp;= \mathsf{ \underset{\pi_B}{max}\; \underset{\pi_W}{min}\; v_\pi(s) } \end{aligned}\]<h4 id="advantages-of-mc-tree-search">Advantages of MC Tree Search</h4><ul><li>Highly selective best-first search<li>Evaluates states dynamically (unlike e.g. DP)<li>Uses sampling to break curse of dimensionality<li>Works for “black-box” models (only requires samples)<li>Computationally efficient, anytime, parallelisable</ul><h3 id="temporal-difference-search">Temporal-Difference Search</h3><ul><li>Simulation-based search<li>Using TD instead of MC (bootstrapping)<li>MC tree search applies MC control to sub-MDP from now<li>TD search applies Sarsa to sub-MDP from now</ul><h4 id="mc-vs-td-search">MC vs. TD search</h4><ul><li>For model-free reinforcement learning, bootstrapping is helpful<ul><li>TD learning reduces variance but increases bias<li>TD learning is usually more efficient than MC<li>TD($\lambda$) can be much more efficient than MC</ul><li>For simulation-based search, bootstrapping is also helpful<ul><li>TD search reduces variance but increase bias<li>TD search is usually more efficient than MC search<li>TD($\lambda$) search can be much more efficient than MC search</ul></ul><h4 id="td-search">TD Search</h4><ul><li>Simulate episodes from the current (real) state $\mathsf{s_t}$<li>Estimate action-value function $\mathsf{Q(s,a)}$<li>For each step of simulation, update action-values by Sarsa</ul>\[\mathsf{ \Delta Q(S,A) = \alpha (R+ \gamma Q(S',A') - Q(S,A)) }\]<ul><li>Select actions based on action-values $\mathsf{Q(s,a)}<ul><li>e.g. $\epsilon$-greedy</ul><li>May also use function approximation for Q</ul><h4 id="dyna-2">Dyna-2</h4><ul><li>In Dyna-2, the agent stores two sets of feature weights<ul><li>Long-term memory<li>Short-term (working) memory</ul><li>Long-term memory is updated from real experience using TD learning<ul><li>General domain knowledgd that applies to any episod</ul><li>Short-term memory is updated from simulated experience using TD search<ul><li>Specific local knowledge about the currrent situation</ul><li>Over value function is sum of long and short-term memories</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 8 Integrating Learning and Planning - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-8/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 8 Integrating Learning and Planning - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-8/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 8 Integrating Learning and Planning - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-8/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-7/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 7 Policy Gradient</p></a> <a href="/posts/RL-course-Note-9/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 9 Exploration and Exploitation</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
