<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 2 Markov Decision Processes" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-2/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-2/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-11-27T10:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 2 Markov Decision Processes" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-2/","@type":"BlogPosting","headline":"RLcourse note - Lecture 2 Markov Decision Processes","dateModified":"2021-12-16T06:06:37+09:00","datePublished":"2021-11-27T10:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-2/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 2 Markov Decision Processes | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 2 Markov Decision Processes</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 2 Markov Decision Processes</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Nov 27, 2021, 10:00 AM +0900" >Nov 27, 2021<i class="unloaded">2021-11-27T10:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 16, 2021, 6:06 AM +0900" >Dec 16, 2021<i class="unloaded">2021-12-16T06:06:37+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3206 words">17 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/lfHX2hHRMVQ"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec2_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h2 id="2강-소감-및-생각">2강 소감 및 생각</h2><p><strong>결국 이렇게 정의된 MDP, Bellman equation을 푸는 것. 일반적인 모든 문제에서 Reward라는 것과 Action이라는 것으로 최대한 수학적으로 해를 구하기 쉽도록 학습 문제를 정의하기 위한 첫번째 단계인 듯하다. 최대한 Linear하게 만들고 싶은 것. 그러나 나는 AI 학습이 모든 것을 Linear하게 정의하려고 노력하는 것이 모든 상황을 커버할 수 있을지는 모르겠다. 물론 대부분의 수학적 접근은 Convolution과 Linear한 식의 combination으로 표현될 수 있다고 대충 느낌상 기억하고 있긴 하다만 그러면 Convolution과 Dense layer를 잘 구성해야 하며 수많은 Unrelated 변수가 생겨난다. 물론 그를 통해서 미처 파악하지 못한 변수를 이용해 추상적 접근을 모델링해내는 것이 가능할 수도 있다. 그러나 이 부분을 구현하기 위해서 Laplace transform과 거기서 표현되는 여러가지 수학적 접근을 구현할 수 있도록 Layer를 구현해야 할 것이다.</strong></p><h2 id="introduction-ot-mdps">Introduction ot MDPs</h2><ul><li><em>Markov decision processes</em> formally describe and envrionment for reinforcement learning<li>Where the envrionment is <em>fully observable</em><li>i.e. The current state completely characterise the process - state는 현재 상태 변수들로 markov 상태가 될 수 있도록 충분한 변수들을 갖추어야 함<li>Almost all RL problems can be formalised as MDPs 뭐 잘 설계해서 MDP 맞추란 소린데 누가 이런말 못함</ul><h3 id="markov-property">Markov Property</h3><p>“The future is independent of the past given the present” 문제를 자꾸 단정하고 싶어한다. 그게 속도나 효율면에서 맞긴 하지만 이게 다른 문제를 만들진 않을까? Markov로 만들수 없는 경우도 있을 것 같지만 복잡하다고 불가능하진 않을것 같은데, 혹은 간혹 더 간단한 모델을 만들 수도 있지 않을까</p><div class="table-wrapper"><table><thead><tr><th style="text-align: left">Definition<th style="text-align: right"> <tbody><tr><td style="text-align: left">A state \(S_t\) is Markov if and only if<td style="text-align: right">\(\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, ...,S_t]\)</table></div><ul><li>The state captures all relevant information from the history<li>Once the state is known, the history may be thrown away<li>흐음… 그보다는 environment의 information에 적절히 history data를 축적한 변수를 만들어야 하지 않을까. 그 변수를 전부 현재의 변수라고 설정하는 것 뿐 현재의 envrionment에서 관찰할 수 있는 것은 아니지 않으려나.</ul><h3 id="state-transition-matrix">State Transition Matrix</h3><p>For a Markov state <em>s</em> and successor state <em>s’</em>, the <em>state transition probability</em> is defined by,</p>\[\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t =s]\]<p>State transition matrix $\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s’$</p>\[\mathcal{P} = from \begin{bmatrix} \mathcal{P}_{11}&amp;\dots&amp;\mathcal{P}_{1n}\\ \vdots &amp; &amp; \\ \mathcal{P}_{n1}&amp;\dots&amp;\mathcal{P}_{nn} \end{bmatrix}\]<ul><li>state 의 갯수가 n개로 제한된 경우에만 사용하는 건가?<li>확률로 state 변화가 표현된 만큼 action이 개입할 여지가 없다. 결국 이 확률 $\mathcal{P}$ 는 이후 가변 성질의 policy로 대체되는 것 같다.</ul><h2 id="markov-process-1단계">Markov Process (1단계)</h2><p>A Markov process is a memoryless random process 마르코브 형태의 프로세스에 대한 설명, 확률만 가지고 state의 진행만을 보여주므로 agent, reward 이런건 없음. memoryless 이전 state과 필요없음. random, sampling 할 수 있다.</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>a <em>Markov</em> Process (or <em>Markov Chain</em>) is a tuple $\langle\mathcal{S,P}\rangle$<br />$\tiny{\blacksquare}\quad \normalsize{\mathcal{S}}\;$ is a (finite) set of states<br />$\tiny{\blacksquare}\quad\normalsize{\mathcal{P}}\;$ is a state transition probability matrix,<br /> \(\quad\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]\)</table></div><h2 id="markov-reward-process-mrp">Markov Reward Process (MRP)</h2><p>A Markov reward process is a Markov chain with values Reward와 Discount factor를 정의함.</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>a <em>Markov Reward</em> Process is a tuple $\langle\mathcal{S,P,}$<span style="color:red">$\mathcal{R,\gamma}$</span>$\rangle$<br />$\tiny{\blacksquare}\quad \normalsize{\mathcal{S}}\;$ is a (finite) set of states<br />$\tiny{\blacksquare}\quad\normalsize{\mathcal{P}}\;$ is a state transition probability matrix,<br /> \(\quad\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]\) <br /> $\tiny{\blacksquare}\quad$ <span style="color:red">$\normalsize{\mathcal{R}}\;$ is a reward function, \(\mathcal{R}_s=\mathbb{E}[R_{t+1} | S_t=s]\)</span><br />$\tiny{\blacksquare}\quad$ <span style="color:red">$\normalsize{\mathcal{\gamma}}\;$ is a discount factor, \(\gamma\in[0,1]\)</span></table></div><h3 id="return-g_t">Return $G_t$</h3><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>The return $G_t$ is the total discounted reward from tiem-step $t$.<br /><center>$$G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$$</center></table></div><ul><li>The discount $\gamma \in [0,1]$ is the present value of future rewards<li>The value of receiving reward $R$ after $k+1$ time-steps is $\gamma^k R$<li>This values immediate reward above delayed reward<ul><li>$\gamma$ close to 0 leads to “myopic” evaluation<li>$\gamma$ close to 1 leads to “far-sighted” evaluation</ul><li>단순 급수적인 형태로 감가를 시행하고 있지만 경우에 따라서 감가 함수를 별도로 함수로 만들어 사용할 수 있을 듯. 크게 필요한 경우가 있을 진 모르겠지만..<li>여기부터 이상해 $R_t$는 어디간거야 계산할 땐 다 쓰면서… 별로 중요한건 아닌데 거슬리네</ul><h3 id="reason-of-discount">Reason of discount</h3><ul><li>수학적으로 편리함<li>무한히 증가하는 return을 제거<li>state가 항상 terminate 될 것이 보장된다면 discount를 1로 해도 괜찮을 수 있음</ul><h3 id="value-function">Value Function</h3><p>The value function $v(s)$ gives the long-term value of state $s$ 기댓값(평균)</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>The <em>state</em> vlaue function $v(s)$ of an MRP is the expected return starting from state $s$<br /> <center>$$\mathsf{v}(s)=\mathbb{E}[G_t\;|\;S_t=s] $$</center></table></div><h2 id="bellman-equation-for-mrps">Bellman Equation for MRPs</h2><p>The value function can be decomposed into two parts: Value function을 학습시키기 위한 기본 논리 근데 이상한게 $R_{t+1}$ 써놓고 계산한거 보면 다 $R_t$ 쓴 것 같음. 그리고 예시를 보면 그게 맞는거 같기도 함. 식 중에서 중간에 뜬금없이 $\mathbb{E}[R_{t+1}]$을 $R_s$로 바꿔놓음 아주 지멋대로야.</p><ul><li>immediate reward $R_{t+1}$<li>discounted value of successor state $\gamma \mathsf{v}(S_{t+1})$</ul><p>\(\quad\quad\quad\quad\mathsf{v}(s) =\mathbb{E}\,[\,G_t\;\|\;S_t=s\,]\)<br /> \(\quad\quad\quad\quad\quad\quad=\mathbb{E}\,[\,R_{t+1}+\gamma R_{t+2} + \gamma ^2 R_{t+3} + \dots\;\|\;S_t=s\,]\)<br /> \(\quad\quad\quad\quad\quad\quad=\mathbb{E}\,[\,R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3} + \dots)\;\|\;S_t=s\,]\)<br /> \(\quad\quad\quad\quad\quad\quad=\mathbb{E}\,[\,R_{t+1}+\gamma G_{t+1}\;\|\;S_t=s\,]\)<br /> \(\quad\quad\quad\quad\quad\quad=\mathbb{E}\,[\,R_{t+1}+\gamma \mathsf{v} (S_{t+1})\;\|\;S_t=s\,]\)<br /> \(\quad\quad\quad\quad\mathsf{v}(s) =\mathcal{R}_s + \gamma \displaystyle\sum_{s' \in S} ^{}\mathcal{P}_{ss'} \mathsf{v} (s')\)</p><h3 id="bellman-equation-in-matrix-form">Bellman Equation in Matrix Form</h3><p>The Bellman equation can be expressed concisely using matrices.</p>\[\mathsf{v} = \mathcal{R} + \gamma \mathcal{P} \mathsf{v}\]<p>where $\mathsf{v}$ is a column vector with one entry per state</p>\[\begin{bmatrix} \mathsf{v} (1) \\ \vdots \\ \mathsf{v} (n) \end{bmatrix} = \begin{bmatrix} \mathcal{R}_1 \\ \vdots \\ \mathcal{R}_n \end{bmatrix} + \gamma \begin{bmatrix} \mathcal{P}_{11} &amp; \dots &amp; \mathcal{P}_{1n} \\ \vdots &amp; &amp; \\ \mathcal{P}_{n1} &amp; \dots &amp; \mathcal{P}_{nn} \end{bmatrix} \begin{bmatrix} \mathsf{v} (1) \\ \vdots \\ \mathsf{v} (n) \end{bmatrix}\]<p>각 state 1~n에서의 value function은 각 state의 reward와 dot (각 state로의 확률), (각 state의 value) * discount factor 의 합이다. 너무 흔한 형태의 식… 그러나 수치해석해야함…</p><h3 id="solving-the-bellman-equation">Solving the Bellman Equation</h3><ul><li>Linear equation</ul><p>\(\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \mathsf{v} = \mathcal{R+ \gamma P}\mathsf{v}\)<br /> \(\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\; (1 - \gamma \mathcal{P}) \mathsf{v} = \mathcal{R}\)<br /> \(\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \mathsf{v} = (1 - \gamma \mathcal{P})^{-1} \mathcal{R}\)</p><ul><li>Computational complexiy is $O(n^3)$ for n state<li>Direct solution only possible for small MRPs 근데 되겠냐<li>There are many iterative methods for large MRPs, e.g.<ul><li>Dynamic programming<li>Monte-Carlo evaluation<li>Temporal-Difference learning</ul></ul><h2 id="markov-decision-process-main">Markov Decision Process (main)</h2><p>드디어 Action으로 개입이 추가되었다. 근데 policy는 왜 빼놓고 하냐 논리적으로 문제가 생기잖아 이미 probability를 정의해 놓고서는 Action을 어떻게 해, 결국 예제에서는 Action을 정의하지 않는 자동 전이 state에서만 Probability를 사용한다. 물론 실제로 둘이 공존할수 있기는 하다. 그리고 Action을 만드는 애는 policy라서 이미 있는 것이다.</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>a <em>Markov Reward</em> Process is a tuple $\langle\mathcal{S, }$<span style="color:red">$\mathcal{A}$</span>$\mathcal{, P, R, \gamma \rangle}$<br />$\tiny{\blacksquare}\quad \normalsize{\mathcal{S}}\;$ is a finite set of states<br />$\tiny{\blacksquare}\quad$ <span style="color:red">$\normalsize{\mathcal{A}}\;$ is a finite set of actions</span><br />$\tiny{\blacksquare}\quad \normalsize{\mathcal{P}}\;$ is a state transition probability matrix,<br /> \(\quad\mathcal{P}^a_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s,\) <span style="color:red">\(A_t =a\)</span>\(]\) <br /> $\tiny{\blacksquare}\quad \normalsize{\mathcal{R}}\;$ is a reward function, \(\mathcal{R}^a_s=\mathbb{E}[R_{t+1} | S_t=s,\) <span style="color:red">\(A_t =a\)</span>\(]\)<br />$\tiny{\blacksquare}\quad \normalsize{\mathcal{\gamma}}\;$ is a discount factor, \(\gamma\in[0,1]\)</table></div><h2 id="policies">Policies</h2><p>Policy는 stochastic으로 시작해 확률을 정의하는 것처럼 해놓았지만 결국엔 deterministic으로 은근슬쩍 바뀐다. 물론 내부 policy 정의는 양쪽을 다 가다가 우선순위 높은 놈으로 바뀌는 식이니 둘다 맞는 말이긴 하다.</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>A policy $\pi$ is a distribution over actions given states.<br /><centor>$$\pi (a| s) = \mathbb{P} [A_t = a | S_t = s]$$</centor></table></div><ul><li>A policy fully defines the behaviour of an agent<li>MDP policies depend on the current state (not the history)<li>Policies are stationary (time-independent)</ul><p>$\quad A_t ~ \pi(. | S_t), \forall t &gt; 0$</p><ul><li>Given an MDP $\mathcal{M = \langle S, A, P ,R ,\gamma \rangle}$ and a policy $ \pi$<li>The state sequence $\mathcal{S_1, S_2, \dots }$ is a Markov process $\mathcal{\langle S, P^{\pi} \rangle}$<li>The state and reward sewuence $\mathcal{S_1, R_2, S_2, \dots}$ is a Markov reward process $\mathcal{\langle S, P^{\pi}, R^{\pi}, \gamma \rangle }$<li>슬쩍 A를 뺐지만 policy에 포함된 것이므로 없어진 게 아니다 바뀐것 없다<li>where</ul><center> $$ \mathcal{P^{\pi}_{s, s'}= \displaystyle\sum_{a \in A} \pi (a \| s) P^a_{ss'}}$$ $$ \mathcal{R^{\pi}_{s}= \displaystyle\sum_{a \in A} \pi (a \| s) R^a_{s}}$$</center><h2 id="value-function-2">Value Function 2</h2><p>각 state와 action의 value를 정의한다.</p><p>The state-value function $\mathsf{v_{\pi} (s)} $ of an MDP is the expected return starting from state s, and then following policy $\pi$</p>\[\mathsf{v_{\pi} (s) = \mathbb{E}_{\pi} [G_t | S_t = s]}\]<p>The action-value function $\mathsf{q_{\pi} (s,a)}$ is the expected return starting from state s, taking action a, and then following policy $\pi$</p>\[\mathsf{q_{\pi} (s,a) = \mathbb{e}_{\pi} [G_t | S_t = s, A_t = a] }\]<h3 id="bellman-expectation-equation">Bellman Expectation Equation</h3><p>필요에 따라 연쇄적으로 대충 정의할 수 있다. Action에 따라서 따라오는 State도 stochastic할 수 있다.</p><p>The state-value function can again be decomposed into immediate reward plus discounted value of successor state</p>\[\mathsf{v_{\pi} (s) = \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s]}\]<p>The action-value function can similarly be decomposed</p>\[\mathsf{q_{\pi} (s,a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi} (S_{t+1}, A_{t+1}) | S_t = s, A_t=a]}\]<h4 id="bellman-expectation-equation-matrix-form">Bellman Expectation Equation Matrix Form</h4><p>The Bellman expectation equation can be expressed concisely using the induced MRP</p>\[\mathsf{v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}}\]<p>with direct solution</p>\[\mathsf{v_{\pi} = (1- \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}}\]<h3 id="optimal-value-function">Optimal Value Function</h3><p>The optimal state-value function $\mathsf{v_* (s)}$ is the maximum value function over all policies</p>\[\mathsf{v_* (s) = \displaystyle\max_{\pi} v_{\pi} (s)}\]<p>The optimal action-value function $\mathsf{a_* (s,a)}$ is the maximum action-value function over all policies</p>\[\mathsf{q_* (s,a) = \displaystyle\max_{\pi} q_{\pi} (s,a)}\]<ul><li>The optimal value function specifies the best possible performance in the MDP<li>An MDP is “solved” when we know the optimal value fn</ul><h3 id="optimal-policy">Optimal Policy</h3><p>증명생략 - 하여튼 존재하고 반드시 그렇다. 흠 잘못 수렴되는 경우가 발생할 수 있을 것 같은데..</p><p>Define a partial ordering over policies</p>\[\mathsf{\pi \ge \pi ' \quad if \quad v_{\pi} (s) \ge v_{\pi '} (s), \forall s}\]<div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>For any Markov Decision Process<br />$\bullet\;$ There exists an optimal policy $\pi$, that is better than or equal to all other policies, $\pi _* \geq \pi, \forall \pi $<br />$\bullet\;$ All optimal policies achieve the optimal value function, \(\mathsf{v_{\pi_*} (s) = v_* (s)}\)<br />$\bullet\;$ All optimal policies achieve the optimal action-value function, \(\mathsf{q_{\pi_*} (s,a) = q_* (s,a)}\)</table></div><h4 id="finding-an-optimal-policy">Finding an Optimal Policy</h4><p>An optimal policy can be found by maximising over \(\mathsf{q_* (s,a)}\)</p>\[\mathsf{\pi_*(a\vert s)} = \begin{cases} \mathsf{1\quad if \; a = \underset{a\in\mathcal{A}}{argmax} \; q_* (s,a)} \\ \mathsf{0 \quad otherwise} \end{cases}\]<ul><li>There is always a deterministic optimal policy for any MDP<li>If we know $\mathsf{q_* (s,a)}$, we immediately have the optimal policy<li>당연하지만 optimal이 되면 deterministic이 된다.</ul><h3 id="bellman-optimality-equation">Bellman Optimality Equation</h3><p>The optimal value functions are recursively related by the Bellman optimality equations:</p>\[\mathsf{v_* (s) = \underset{a}{max} \;q_* (s,a)}\] \[\mathsf{q_* (s,a) = \mathcal{R}^a_s + \gamma \displaystyle\sum_{s'\in S} \mathcal{P}^a_{ss'} v_* (s')}\] \[\mathsf{v_* (s) = \underset{a}{max} \; \mathcal{R}^a_s + \gamma \displaystyle\sum_{s' \in S} \mathcal{P}^a_{ss'}v_* (s')}\] \[\mathsf{q_* (s,a) = \mathcal{R}^a_s + \gamma \displaystyle\sum_{s' \in S} \mathcal{P}^a_{ss'} \underset{a'}{max} \; q_* (s', a')}\]<h4 id="solving-the-bellman-optimality-equation">Solving the Bellman Optimality Equation</h4><ul><li>수치해석 하라는 소리. 오랜만에 복습해야겠는 걸 잘 기억이안나네 쉽긴한데<li>Bellman Optimality Equation is non-linear<li>No closed form solution (in general)<li>Many iterative solution methods<ul><li>Value Iteration<li>Policy Iteration<li>Q-learning<li>Sarsa</ul></ul><h2 id="extensions-to-mdps">Extensions to MDPs</h2><p>여기부턴 강의는 없다. PPT만 있다.</p><ul><li>Infinite and continuous MDPs<li>Partially observable MDPs<li>Undiscounted, average reward MDPs</ul><h3 id="infinite-mdps">Infinite MDPs</h3><p>Continuous한 MDP, 물리적상태 같은건가 HJB equation 다시한번 봐야겠다</p><p>The following extensions are all possible:</p><ul><li>Countably infinite state and/or action spaces<ul><li>Straightforward</ul><li>Continuous state and/or action spaces<ul><li>Closed form for linear quadratic model(LQR)</ul><li>Continuous Time<ul><li>Requires partial differential equations<li>Hamilton-Jaccobian-Bellman (HJB) equation<li>Limiting case of Bellman equation as time-step $\rightarrow 0$</ul></ul><h3 id="pomdps">POMDPs</h3><p>실제 모든것이 관측 가능하지 않은 문제에 대한 정의로 observation이 들어간다. 실제로 이런게 많을것 같은데 정의를 어떻게 할 것인가</p><p>A Partially Observab;e Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions.</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>A POMDP is a tuple $\mathcal{\langle S, A, \textcolor{red}{O}, P, R,\textcolor{red}{Z}, \gamma \rangle}$<br />$\bullet\;\; \mathcal{S}\;$ is a finite set of states<br />$\bullet\;\; \mathcal{A}\;$ is a finite set of actions<br />$\bullet\;\; \textcolor{red}{\mathcal{A}}\;$ is a finite set of observation<br />$\bullet\;\; \mathcal{P}\;$ is a state transition probability matrix<br />\(\quad\,\mathsf{\mathcal{P}^a_{ss'} = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] }\)<br />$\bullet\;\; \mathcal{R}\;$ is a reward function<br />\(\quad\,\mathsf{\mathcal{R}^a_s = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] }\)<br />$\bullet\;\; \mathcal{\textcolor{red}{Z}}\;$ is an observation function<br />\(\quad\,\mathsf{\mathcal{Z}^a_{s'o} = \mathbb{P} [O_{t+1} = o \vert S_{t+1} = s', A_t = a] }\)<br />$\bullet\;\; \mathcal{\gamma}\;$ is a discount factor $\gamma \in [0, 1]$</table></div><h3 id="belief-states">Belief States</h3><p>현재 state가 어딘지 모르고 history에 따라 확률로 정의한다는 뜻인가? 흐음 어떤 경우인지 아직 모르겠다.</p><p>A history $H_t$ is a sequence of actions, observations and rewards</p>\[\mathsf{H_t = A_0, O_1, R_1, \dots , A_{t-1}, O_t, R_t}\]<p>A belief state $b(h) is a probablity distribution over states, conditioned on the history h</p>\[\mathsf{b(h) = (\mathbb{P} [ S_t = s^1 \vert H_t = h ], \dots , \mathbb{P}[S_t = s^n \vert H_t = h])}\]<h3 id="reductinos-of-pomdps">Reductinos of POMDPs</h3><p>State tree로 표현하는게 뭘까 흠 bellman equation으로 표현하기 어려운 경우가 있는건가</p><ul><li>The history $\mathsf{H_t}$ satisfies the Markov property<li>The belief state $\mathsf{b(H_t)}$ satisfies the Markov property<li>A POMDP can be reduced to an (infinite) history tree<li>A POMDP can be reduced to an (infinite) belief state tree</ul><h3 id="ergodic-markov-process">Ergodic Markov Process</h3><p>각 policy에 대해서 각 state에 나타나는 확률적 분포에 대한 이야기같다. 나중에 다시 생각해볼까..</p><p>An ergodic Markov process is</p><ul><li>Recurrent: each state is visted an infinite number of times<li>Aperiodic: each state is visited without any systematic period</ul><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>An ergodic Markov process has a limiting stationary distribution $\mathsf{d^{\pi}(s)}$ with the property<br /><center>$$ \mathsf{d^{\pi} (s) = \displaystyle\sum_{s' \in S} d^{\pi} (s') \mathcal{P}_{s's}}$$</center></table></div><h3 id="ergodic-mdp">Ergodic MDP</h3><p>그렇게 state의 확률 분포가 수렴하는 형태의 MDP에 대해서 policy에 의한 각 타임 step의 평균적인 reward가 존재한다 흠… 이게 마이너스면 time이 흘러갈수록 reward는 계속 낮아지고 그런거일 뿐 아닌가…? 이 평균은 어따쓴담</p><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>An MDP is ergodic if the Markov chain induced by any policy is ergodic</table></div><p>For any policy $\pi$, and ergodic MDP has an average reward per time-step $\rho ^{\pi}$ that is independent of start state.</p>\[\mathsf{\rho^{\pi} = \underset{T\rightarrow \infty}{lim} \; {1\over T}\mathbb{E} \left[\displaystyle\sum^T_{t=1} R_t\right]}\]<h3 id="average-reward-value-function">Average Reward Value Function</h3><p>Ergodic 내용을 적용한 Value Function</p><ul><li>The value function of an undiscounted, ergodic MDP can be expressed in terms of average reward.<li>$\mathsf{\tilde{v}_{\pi} (s)$ is the extra reward due to starting from state s,</ul>\[\mathsf{\tilde{v}_{\pi} (s) = \mathbb{E}_{\pi} \left[ \displaystyle\sum^{\infty}_{k=1} (R_{t+k} - \rho^{\pi})\;\vert\;S_t =s \right ]}\]<p>There is a corresponding average reward Bellman equation,</p>\[\mathsf{\tilde{v}_{\pi} (s) = \mathbb{E}_{\pi} \left[(R_{t+1} - \rho^{\pi}) + \displaystyle\sum^{\infty}_{k=1} (R_{t+k} - \rho^{\pi})\;\vert\;S_t =s \right ]}\] \[\quad\quad = \mathsf{\mathbb{E}_{\pi} \left[(R_{t+1} - \rho^{\pi}) + \tilde{v}_{\pi} (s+1)\;\vert\;S_t =s \right ]}\]</div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 2 Markov Decision Processes - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 2 Markov Decision Processes - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 2 Markov Decision Processes - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-2/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-1/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 1 Introduction to Reinforcement Learning</p></a> <a href="/posts/RL-course-Note-3/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 3 Planning by Dynamic Programming</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
