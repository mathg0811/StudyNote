<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 7 Policy Gradient" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-7/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-7/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-24T01:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 7 Policy Gradient" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-7/","@type":"BlogPosting","headline":"RLcourse note - Lecture 7 Policy Gradient","dateModified":"2022-01-04T18:50:40+09:00","datePublished":"2021-12-24T01:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-7/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 7 Policy Gradient | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 7 Policy Gradient</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 7 Policy Gradient</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Dec 24, 2021, 1:00 AM +0900" >Dec 24, 2021<i class="unloaded">2021-12-24T01:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 4, 2022, 6:50 PM +0900" >Jan 4<i class="unloaded">2022-01-04T18:50:40+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3397 words">18 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/KHZVXao4qXs"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec7_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h2 id="introduction-of-policy-gradient">Introduction of Policy Gradient</h2><h3 id="7강-소감">7강 소감</h3><p>마지막으로 제일 어려운 부분이라고 하는데 사실 진짜 어렵다기보다는 논리적으로 중요한 연결점들을 다 생략해버리고 이게 결국 이렇게 된다라는 결과적인 내용만 정리한 데다가 notation 은 이상하게 작성되고 시도때도없이 변하니 이해하기 어려운 게 당연하다. 그래도 컨셉은 거의 다 이해했지만 베이스가 되는 논리가 많이 빠져있으니 제대로 기억해낼지는 모르겠다. 나머지 강의에서도 크게 보충이 안될것 같고 결국엔 무언가 해보던지 좀더 심화 강의를 들어보던지 책을 독학하던지 해야 한다. 물론 이정도 지식이면 실제 활용에도 문제 없을 정도는 될것같지만 그래도 찝찝한게 싫은게 나니까 어쩔수없다.</p><h3 id="policy-based-reinforcement-learning">Policy-Based Reinforcement Learning</h3><ul><li>In the last lecture we approximated the value or action-value function using parameters $\theta$<li>이번엔 $\theta$ notation이 변수로 들어가지 않고 underbar로 들어갔다. 의미는 똑같다</ul>\[\begin{aligned} \mathsf{V_\theta (s)} &amp;\approx \mathsf{V^\pi (s)} \\ \mathsf{Q_\theta (s,a)} &amp;\approx \mathsf{Q^\pi (s,a)} \end{aligned}\]<ul><li>A policy was generated directly from the value function<ul><li>e.g. using $\epsilon$-greedy</ul><li>In this lecture we will directly parametrise the policy</ul>\[\mathsf{ \pi_\theta (s,a) = \mathbb{P} [a\vert s, \theta] }\]<ul><li>We will focus again on model-free reinforcement learning</ul><h3 id="value-based-and-policy-based-rl">Value-Based and Policy-Based RL</h3><ul><li>Value Based<ul><li>Learnt Value Function<li>Implicit policy (e.g. $\epsilon$-greedy)</ul><li>Policy Based<ul><li>No Value Function<li>Learnt Policy</ul><li>Actor-Critic<ul><li>Learnt Value Function<li>Learnt Policy</ul></ul><h3 id="advantages-of-policy-based-rl">Advantages of Policy-Based RL</h3><p>Advantages:</p><ul><li>Better convergence properties<li>Effective in high-dimensional or continuous action spaces<li>Can learn stochastic policies<li>왜 Value based는 Stochastic이 안된다고 하는걸까 걍 하면되지</ul><p>Disadvantages:</p><ul><li>Typically converge to a local rather than global optimum<li>Evaluating a policy is typically inefficient and high variance<li>이거에 대한 부분은 차후에 고민해보는걸로</ul><h3 id="policy-search">Policy Search</h3><h4 id="policy-objective-functions">Policy Objective Functions</h4><ul><li>Goal : given policy $\mathsf{\pi_\theta (s,a)}$ with parameters $\theta$, find best $\theta$<li>But how do we measure the quality of a policy $\pi_\theta$?<li>In episodic environments we can use the start value</ul>\[\mathsf{ J_1 (\theta) = V^{\pi_\theta}(s_1) = \mathbb{E}_{\pi_\theta} [v_1] }\]<ul><li>In continuing envrionments we can use the average value</ul>\[\mathsf{ J_{avV} (\theta) = \displaystyle\sum_s d^{\pi_\theta} (s) V^{\pi_\theta} (s) }\]<ul><li>Or the average reward per time-step</ul>\[\mathsf{ J_{avR} (\theta) = \displaystyle\sum_s d^{\pi_\theta} (s) \displaystyle\sum_a \pi_\theta (s,a) \mathcal{R}^a_s }\]<ul><li>where $\mathsf{d^{\pi_\theta} (s) }$ is stationary distribution of Markov chain for $\pi_\theta$<li>Value 안쓴다더니 Value Function만 안쓰고 Value는 쓰는건가 approximation 만 안하는거네, 근데 state Value는 빼고 Action reward 만 가져다 쓰는</ul><h4 id="policy-optimisation">Policy Optimisation</h4><ul><li>Policy based reinforcement learning is an optimisation problem<li>Find $\theta$ that maximises $\mathsf{J(\theta)}$<li>Some approaches do not use gradient<ul><li>Hill climbing<li>Simplex / amoeba / Nelder Mead<li>Genetic algorithms</ul><li>Greater efficiency often possible using gradient<ul><li>Gradient descent<li>Conjugate gradient<li>Quasi-newton</ul><li>We focus on gradient descent, many extensions possible<li>And on methods that exploit sequential structure</ul><h2 id="finite-difference-policy-gradient">Finite Difference Policy Gradient</h2><h3 id="policy-gradient">Policy Gradient</h3><ul><li>Let $\mathsf{J(\theta)}$ be a policy objective function<li>Policy gradient algorithms search for a local maximum in $\mathsf{J(\theta)}$ by ascending the gradient of the policy, w.r.t parameters $\theta$</ul>\[\Delta \theta = \alpha \nabla _\theta \mathsf{J}(\theta)\]<ul><li>Where $\nabla _\theta \mathsf{J}(\theta)$ is the policy gradient</ul><p>사실 이 방법의 local maximise 방법은 적절한 알파값을 찾는게 쉽지 않고 그래프 형태에 따라 달라지기 때문에 사실 그렇게 좋은 방법이라고 하긴 어렵다. 물론 최대한 작은 숫자를 넣어주면 안정적이기는 하지만 그런 만큼 느려지기도 한다. 최대한 간단한 방법의 접근이긴한데 나중에 보충하려나</p>\[\mathsf{ \nabla _\theta J(\theta) } = \left( \begin{array} \; \mathsf{ \frac{\partial J(\theta)}{\partial \theta _1} } \\ \vdots \\ \mathsf{ \frac{\partial J(\theta)}{\partial \theta _n} } \end{array} \right)\]<ul><li>and $\alpha$ is a step-size parameter</ul><p>조금 더 좋은 수치해석 접근방법이 있었던거 같은데 수치해석 다시 찾아볼까..</p><h3 id="computing-gradients-by-finite-differences">Computing Gradients By Finite Differences</h3><ul><li>To evaluate policy gradient of $\mathsf{\pi_\theta (s,a)}$<li>For each dimension $\mathsf{k\in [1,n]}$<ul><li>Estimate kth partial derivative of objective function w.r.t. $\theta$<li>By perturbing $\theta$ by small amount $\epsilon$ in kth dimension <br /><center>$$ \mathsf{ \frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon u_k) - J(\theta)}{ \epsilon} } $$</center> <br /> where $\mathsf{u_k}$ is unit vector with 1 in kth component, 0 elsewhere</ul><li>Uses n evaluations to compute policy gradient in n dimensions<li>Simple, noisy, inefficient - but sometimes effective<li>Works for arbitrary policies, even if policy is not differentiable</ul><p>하나씩 조금씩 바꿔본다는 그런 방법</p><h2 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</h2><h3 id="likelihood-ratios">Likelihood Ratios</h3><h4 id="score-function">Score Function</h4><ul><li>We now compute the policy gradient analytically<li>Assume policy $\pi_\theta$ is differentiable whenever it is non-zero<li>and we know the gradient $\nabla_\theta \pi_\theta \mathsf{(s,a)}$<li>Likelihood ratios exploit the following identity</ul>\[\begin{aligned} \nabla_\theta \pi_\theta \mathsf{(s,a)} &amp;= \mathsf{ \pi_\theta (s,a) \frac{ \nabla_\theta \pi_\theta (s,a) }{\pi_\theta (s,a)} } \\ &amp;= \mathsf{ \pi_\theta (s,a) \nabla_\theta \log \pi_\theta (s,a) } \end{aligned}\]<ul><li>The score function is $\nabla_\theta \log \pi_\theta \mathsf{(s,a)}$</ul><p>여기서 Policy를 approximation, Score function 이라는 이름의 의미는 이 값이 policy의 각 확률에 대해서 gradient 보정 방향마다의 보정 크기 비율을 곱해주는 값이기 때문인 듯 Gradient = policy * score</p><h4 id="softmax-policy">Softmax Policy</h4><ul><li>We will use a softmax policy as a running example<li>Weight actions using linear combination of features $ \phi \mathsf{(s,a)^T} \theta $<li>Probability of action is proportional to exponentiated weight</ul>\[\mathsf{ \pi_\theta (s,a) \propto e^{\phi (s,a)^T \theta} }\]<ul><li>The score function is</ul>\[\mathsf{ \nabla_\theta \log \pi_\theta (s,a) = \phi (s,a) - \mathbb{E}_{\pi_\theta}[\phi(s,\cdot)] }\]<p>과정이 안적혀있어서 나중에 찾아봐야</p><h4 id="gaussian-policy">Gaussian Policy</h4><ul><li>In continuous action spaces, a Gaussian policy is natural<li>Mean is a linear combination of state features $\mathsf{\mu(s) = \phi(s)^T\theta}$<li>Variance may be fixed $\rho^2$, or can also parametrised<li>Policy is Gaussian, $\mathsf{a \sim \mathcal{N}(\mu(s),\rho^2)}$<li>The scroe function is</ul>\[\mathsf{ \nabla_\theta \log \pi_\theta (s,a) = \frac{ (a-\mu(s))\phi(s) }{ \rho^2 } }\]<p>Continuous한 action을 선택해야하여 한 방법으로 Gaussain 형태를 사용한다</p><h3 id="policy-gradient-theorem">Policy Gradient Theorem</h3><h4 id="one-step-mdps">One-Step MDPs</h4><ul><li>Consider a simple class of one-step MDPs<ul><li>Starting in state $\mathsf{s\sim d(s)}$<li>Terminating after on time-step with reward $\mathsf{r=\mathcal{R}_{s,a}}$</ul><li>Use likelihood ratios to compute the policy gradient</ul>\[\begin{aligned} \mathsf{J(\theta)} &amp;= \mathbb{E}_{\pi_\theta} [\mathsf{r}] \\ &amp;= \mathsf{\displaystyle\sum_{s\in \mathcal{S}} d(s) \displaystyle\sum_{a\in\mathcal{A}} \pi_\theta (s,a) \mathcal{R}_{s,a} } \\ \mathsf{\nabla_\theta J(\theta)} &amp;= \mathsf{\displaystyle\sum_{s\in \mathcal{S}} d(s) \displaystyle\sum_{a\in\mathcal{A}} \pi_\theta (s,a) \nabla_\theta \log \pi_\theta (s,a) \mathcal{R}_{s,a} } \\ &amp;= \mathsf{\mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta (s,a) r]} \end{aligned}\]<p>이거 그냥 Value Function 아니냐고</p><p>근데 왜 one step return에 next step state value는 은근슬쩍 빠지고 action reward만 남아있는가 자꾸 이랬다 저랬다 할거야? Terminateing state value가 0이라고 써주던지.. 만약에 state value 포함하면 sigma도 한개 더 들어가야함</p><h4 id="policy-gradient-theorem--">Policy Gradient Theorem -</h4><ul><li>The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs<li>Replaces instantaneous reward r with long-term value $\mathsf{Q^\pi(s,a)}$ - r은 action reward 였는데 멋대로 Q action value로 바꾸기 있냐<li>Policy gradient theorem applies to start state objective, average reward and average value objective</ul><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>For any differentiable policy $\mathsf{\pi_\theta(s,a)}$,<br /> for any of the policy objective functions $\mathsf{J = J_1, J_{avR} \text{, or} \frac{1}{1-\gamma}J_{avR}}$, <br /> the policy gradient is<br /><center>$$ \mathsf{ \nabla_\theta J(\theta) = {\color{Red} \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta (s,a) Q^{\pi_\theta}(s,a)]} } $$</center></table></div><p>하여튼 score와 action value 곱의 기대값이다. 증명은 생략되어 있으므로 나중에 책을 찾아본다, 사실상 sarsa나 Q-learning을 parameterise한 것인듯</p><h4 id="monte-carlo-policy-gradient-reinforce">Monte-Carlo Policy Gradient (REINFORCE)</h4><ul><li>Update parameters by stochastic gradient ascent<li>Using policy gradient theorem<li>Using return $\mathsf{v_t}$ as an unbiased sample of $\mathsf{Q^{\pi_\theta} (s_t,a_t)}$</ul>\[\mathsf{\Delta \theta_t = \alpha \nabla_\theta \log \pi_\theta (s_t,a_t) v_t }\]<div class="table-wrapper"><table><tbody><tr><td>function REINFORCE <br /> $\quad$ Initialise $\theta$ arbitrarily <br /> $\quad$ for each episode $\mathsf{ \lbrace s_q,a_q,r_2, \dots, s_{T-1}, a_{T-1}, r_T \rbrace \sim \pi_\theta }$ do <br />$\quad\quad$ for $\mathsf{t=1}$ to $\mathsf{T-1}$ do <br />$\quad\quad\quad$ $\mathsf{ \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta (s_t,a_t) v_t }$ <br />$\quad\quad$ end for <br />$\quad$ end for <br />$\quad$ return $\theta$ <br /> end function</table></div><p>에피소드 중 각 step에 대해서 score 곱하기 return으로 parameter를 업데이트한다. 오래전에 나온 알고리즘이라 딱 이 알고리즘을 Reinforce 알고리즘이라고 한다. 알파고1 에 쓰인 알고리즘</p><p>improvement가 부드럽지만 variance가 커서 느리다</p><h2 id="actor-critic-policy-gradient">Actor-Critic Policy Gradient</h2><h3 id="reducing-variance-using-a-critic">Reducing Variance Using a Critic</h3><ul><li>Monte-Carlo policy gradient still has high variance<li>We use a critic to estimate the action-value function,</ul>\[\mathsf{ Q_w (s,a) \approx Q^{\pi_\theta} (s,a)}\]<ul><li>Actor-critic algorithms maintain two sets of parameters<ul><li>Critic - Updates action-value function parameters w<li>Actor - Updates policy parameters $\theta$, in direction suggested by critic</ul><li>Actor-critic algorithms follow an approximate policy gradient</ul>\[\begin{aligned} \mathsf{\nabla_\theta J(\theta)} &amp;\approx \mathsf{\mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta (s,a) Q_w(s,a)]} \\ \Delta \theta &amp;= \mathsf{ \alpha \nabla_\theta \log \pi_\theta (s,a) Q_w(s,a) } \end{aligned}\]<p>J 는 average return for time step인데 왜 return 값의 sample로 parameter를 업데이트 하는 부분 이거 자꾸 신경쓰이긴 한데.. alpha값 꽤 중요하겠다.</p><h3 id="estimating-the-action-value-function">Estimating the Action-Value Function</h3><ul><li>The critic is solving a familiar probelm: policy evaluation<li>How good is policy $\pi_\theta$ for current parameters $\theta$?<li>This problem was explored in previous two lectures, e.g.<ul><li>Monte-Carlo policy evaluation<li>Temporal-Difference learning<li>TD($\lambda$)</ul><li>Could also use e.g. least-squares policy evaluation</ul><h3 id="action-value-actor-critic">Action-Value Actor-Critic</h3><ul><li>Simple actor-critic algorithm based on action-value critic<li>Using linear value fn approx. $\mathsf{Q_w(s,a) =\phi(s,a)^T w}$<ul><li>Critic - Update w by linear TD(0)<li>Actor - Updates $\theta$ by policy gradient</ul></ul><div class="table-wrapper"><table><tbody><tr><td>function QAC <br /> $\quad$ Initialise $\mathsf{s}, \theta$ <br /> $\quad$ Sample $\mathsf{ a \sim \pi_\theta }$ <br /> $\quad$ For each step do <br />$\quad\quad$ Sample reward $\mathsf{r=\mathcal{R}^a_s}$; sample transition $\mathsf{s’ \sim \mathcal{P}^a_s}$ <br />$\quad\quad$ Sample action $\mathsf{a’ \sim \pi_\theta (s’, a’)}$ <br />$\quad\quad$ $\mathsf{ \delta = r + \gamma Q_w (s’,a’) - Q_w (s,a)}$ <br />$\quad\quad$ $\mathsf{ \theta = \theta + \alpha \nabla_\theta \log \pi_\theta (s,a) Q_w(s,a)}$ <br />$\quad\quad$ $\mathsf{ w \leftarrow w + \beta \delta \phi (s,a) }$ <br />$\quad\quad$ $\mathsf{ a \leftarrow a’, s \leftarrow s’ }$ <br />$\quad$ end for <br /> end function</table></div><p>policy와 Value를 둘다 동시에 학습한다는 건데 결국 하나씩 하나씩 조립한 것. TD error를 구하고 policy 업데이트하고 linear model에 error 곱해서 Q parameter weight 업데이트한다.</p><h3 id="compatible-function-approximation">Compatible Function Approximation</h3><h4 id="bias-in-actor-critic-algorithms">Bias in Actor-Critic Algorithms</h4><ul><li>Approximatin the policy gradient introduces bias<li>A biased policy gradient may not find the right solution<ul><li>e.g. if $\mathsf{Q_w(s,a)}$ uses aliased features, can we solve gridworld example?</ul><li>Luckily, if we choose value function approximation carefully<li>Then we can avoid introducing any bias<li>i.e. We can still follow the exact policy gradient</ul><p>Compatible Function Approximation</p><div class="table-wrapper"><table><thead><tr><th>Theorem (Compatible Function Approximation Theorem)<tbody><tr><td>If the following two conditions are satisfied:<br /> 1. Value function approximator is compatible to the policy<br /><center>$$ \mathsf{ \nabla _w Q_w(s,a) = \nabla_\theta \log \pi_\theta (s,a) } $$</center><br />2. Value function parameters w minimise the mean-squared error <br /><center>$$ \mathsf{ \epsilon = \mathbb{E}_{\pi_\theta} [(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2] } $$</center><br /> Then the policy gradient is exact,<br /><center>$$\mathsf{ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log_\theta \pi_\theta(s,a) Q_w (s,a)]} $$</center></table></div><p>왜 에러가 parameterised policy의 action Value와 parameterised Action Value의 차이가 되어있을까. 설명이 없어도 너무 없다. improved Value 와 이전 단계 Value의 에러가 되어야 하는 거 아닌가 근데 이부분 강의에서 안다루고 대충넘어감</p><h4 id="proof-of-compatible-function-approximation-therorem">Proof of Compatible Function Approximation Therorem</h4><p>If w is chosen to minimise mean-squared error, gradient of $\epsilon$ w.r.t. w must be zero,</p>\[\begin{aligned} \mathsf{ \nabla_w \epsilon } &amp;= 0 \\ \mathsf{ \mathbb{E}_{\pi_\theta} [(Q^{\theta}(s,a)-Q_w(s,a))\nabla_w Q_w (s,a)] } &amp;= 0 \\ \mathsf{ \mathbb{E}_{\pi_\theta} [(Q^{\theta}(s,a)-Q_w(s,a)) \nabla_\theta \log \pi_\theta (s,a)] } &amp;= 0 \\ \mathsf{ \mathbb{E}_{\pi_\theta} [Q^{\theta}(s,a) \nabla_\theta \log \pi_\theta (s,a)] } &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} [Q_w(s,a) \nabla_\theta \log \pi_\theta (s,a)] } \end{aligned}\]<p>So $\mathsf{Q_w(s,a)}$ can be substituted directly into policy gradient,</p>\[\mathsf{ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log_\theta \pi_\theta(s,a) Q_w (s,a)] }\]<h3 id="advantage-function-critic">Advantage Function Critic</h3><h4 id="reducing-variance-using-a-baseline">Reducing Variance Using a Baseline</h4><ul><li>We subtract a baseline function B(s) from the policy gradient<li>This can reduce variance, without changing expectation</ul>\[\begin{aligned} \mathsf{ \mathbb{E}_{\pi_\theta} [\nabla_\theta \log_\theta \pi_\theta(s,a) B(s)] } &amp;= \mathsf{ \displaystyle\sum_{s\in\mathcal{S}} d^{\pi_\theta}(s) \displaystyle\sum_{a} \nabla_\theta \pi_\theta(s,a)B(s) } \\ &amp;= \mathsf{ \displaystyle\sum_{s\in\mathcal{S}} d^{\pi_\theta}B(s) \nabla_\theta \displaystyle\sum_{a\in\mathcal{A}} \pi_\theta(s,a) } \\ &amp;= 0 \end{aligned}\]<ul><li>A good baseline is the state value function $\mathsf{B(s) = V^{\pi_\theta}(s)}$<li>So we can rewrite the policy gradient using the advantage function $\mathsf{A^{\pi_\theta}(s,a)}$</ul>\[\begin{aligned} \mathsf{A^{\pi_\theta}(s,a)} &amp;= \mathsf{ Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s) } \\ \mathsf{ \nabla_\theta J(\theta)} &amp;= {\color{Red} \mathsf{ \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s,a) A^{\pi_\theta}(s,a)] }} \end{aligned}\]<p>그냥 action과 관계없는 state value 를 빼면 평균적으로 영향이 0에 수렴한다는 뜻이라서 충분한 수를 sample할 때는 괜찮다는 것</p><h4 id="estimating-the-advantage-function">Estimating the Advantage Function</h4><ul><li>The advantage function can significantly reduce variance of policy gradient<li>So the critic should really estimate the advantage function<li>For example, by estimating both $\mathsf{V^{\pi_\theta}(s)}$ and $\mathsf{Q^{\pi_\theta}(s,a)}$<li>Using two function approximators and two parameter vectors,</ul>\[\begin{aligned} \mathsf{ V_v(s) } &amp;\approx \mathsf{ V^{\pi_\theta}(s) } \\ \mathsf{ Q_w(s,a) } &amp;\approx \mathsf{ Q^{\pi_\theta}(s,a) } \\ \mathsf{ A(s,a) } &amp;= \mathsf{ Q_w(s,a)-V_v(s) } \end{aligned}\]<ul><li>And updating both value functions by e.g. TD learning<li>For the true value function $\mathsf{V^{\pi_\theta}(s)}$, the TD error $\delta^{\pi_\theta}$</ul>\[\mathsf{ \delta^{\pi_\theta} = r+\gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s) }\]<ul><li>is an unbiased estimate of the advantage function</ul>\[\begin{aligned} \mathsf{ \mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta} \vert s,a ] } &amp;= \mathsf{ \mathbb{E}_{\pi_\theta}[ r+\gamma V^{\pi_\theta}(s') \vert s,a ] - V^{\pi_\theta}(s) } \\ &amp;= \mathsf{ Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s) } \\ &amp;= \mathsf{ A^{\pi_\theta}(s,a) } \end{aligned}\]<ul><li>So we can use the TD error to compute the policy gradient</ul>\[\mathsf{ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[ \nabla_\theta \log \pi_\theta(s,a) \delta^{\pi_\theta} ] }\]<ul><li>In practice we can use an approximate TD error</ul>\[\mathsf{ \delta_v = r+\gamma V_v(s') - V_v(s) }\]<ul><li>This approach only requires one set of critic parameters v</ul><p>여기까지 하면 State value와 Action Value, Policy까지 전부 parameterise하는 것이었지만. Advantage는 TD error와 같고 이걸로 policy gradient를 계산할 수 있다. 근데 Q가 아니라 V를 parameterise 한다는 건 dimension이 하나 줄어든다는 건데 이거에 관해서 전에 model free가 불가능하다고 했던거 같은데 이거 한번 다시 맞춰봐야겠다.</p><h3 id="eligibility-traces">Eligibility Traces</h3><h4 id="critics-at-different-time-scales">Critics at Different Time-Scales</h4><ul><li>Critic can estimate value function $\mathsf{V_\theta(s)}$ from many targets at different time-scales<ul><li>For MC, the target is the return $\mathsf{v_t}$ <br /><center>$$ \mathsf{ \Delta\theta = \alpha({\color{Red} v_t} - V_\theta(s))\phi(s) } $$ </center><li>For TD(0), the target is the TD target $\mathsf{ r+\gamma V(s’) }$ <br /><center>$$ \mathsf{ \Delta\theta = \alpha({\color{Red} r+\gamma V(s') } - V_\theta(s))\phi(s) } $$ </center><li>For forward-view TD(\lambda), the target is the $\lambda$-return $\mathsf{v^\lambda_t}$ <br /><center>$$ \mathsf{ \Delta\theta = \alpha({\color{Red} v^\lambda_t} - V_\theta(s))\phi(s) } $$ </center><li>For backward-view TD, we use eligibility traces <br /><center>$$\begin{aligned} \mathsf{ \delta_t } &amp;= \mathsf{ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) } \\ \mathsf{ e_t } &amp;= \mathsf{ \gamma\lambda e_{t-1} + \phi(s_t) } \\ \mathsf{ \Delta \theta } &amp;= \mathsf{ \alpha\delta_t e_t } \end{aligned}$$ </center></ul></ul><h4 id="actors-at-different-time-scales">Actors at Different Time-Scales</h4><ul><li>The policy gradient can also be estimated at many time-scales</ul>\[\mathsf{ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[ \nabla_\theta \log \pi_\theta(s,a) {\color{Red} A^{\pi_\theta}(s,a) } ] }\]<ul><li>Monte-Carlo policy gradient uses error from complete return</ul>\[\mathsf{ \Delta\theta = \alpha({\color{Red} v_t} - V_v(s_t)) \nabla_\theta \log \pi_\theta(s_t,a_t) }\]<ul><li>Actor-critic policy gradient uses the one-step TD error</ul>\[\mathsf{ \Delta\theta = \alpha({\color{Red} r+\gamma V_v(s_{t+1})} - V_v(s_t)) \nabla_\theta \log \pi_\theta(s_t,a_t) }\]<h4 id="policy-gradient-with-eligibility-traces">Policy Gradient with Eligibility Traces</h4><ul><li>Just like forward-view TD($\lambda$), we can mix over time-scales</ul>\[\mathsf{ \Delta\theta = \alpha({\color{Red} v^\lambda_t} - V_v(s_t)) \nabla_\theta \log \pi_\theta(s_t,a_t) }\]<ul><li>where $\mathsf{ v^\lambda_t - V_v(s_t)}$ is a biased estimate of advantage fn<li>Like backward-view TD($\lambda$), we can also use eligibility traces<ul><li>By equivalence with TD($\lambda$), substituting $\mathsf{\phi(s) = \nabla_\theta \log \pi_\theta(s,a)}$</ul></ul>\[\begin{aligned} \mathsf{ \delta } &amp;= \mathsf{ r_{t+1} + \gamma V_v(s_{t+1}) - V_v(s_t) } \\ \mathsf{ e_{t+1} } &amp;= \mathsf{ \lambda e_t + \nabla_\theta \log \pi_\theta(s,a) } \\ \mathsf{ \Delta \theta } &amp;= \mathsf{ \alpha\delta_t e_t } \end{aligned}\]<ul><li>This update can be applied online, to incomplete sequences</ul><h3 id="natural-policy-gradient">Natural Policy Gradient</h3><h4 id="alternative-policy-gradient-directions">Alternative Policy Gradient Directions</h4><ul><li>Gradient ascent algorithms can follow any ascent direction<li>A good ascent direction can significantly speed convergence<li>Also, a policy can often be reparametrised without changing action probabilities<li>For example, increasing score of all actions in a softmax policy<li>The vanilla gradient is sensitive to these reparametrisations</ul><p>Natural Policy Gradient</p><ul><li>The natural policy gradient is parametrisation independent<li>It finds ascent direction that is closest to vanilla gradient, when changing policy by a small, fixed amount</ul>\[\mathsf{ \nabla^{nat}_\theta \pi_\theta (s,a) = G^{-1}_\theta \nabla_\theta \pi_\theta(s,a)}\]<ul><li>where $\mathsf{G_\theta}$ is the Fisher information matrix</ul>\[\mathsf{ G_\theta = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a)\nabla_\theta \log \pi_\theta(s,a)^T \right] }\]<h4 id="natural-actor-critic">Natural Actor-Critic</h4><ul><li>Using compatible function approximation,</ul>\[\mathsf{ \nabla_w A_w (s,a) = \nabla_\theta\log\pi_\theta(s,a) }\]<ul><li>So the natural policy gradient simplifies,</ul>\[\begin{aligned} \mathsf{ \nabla_\theta J(\theta) } &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a) A^{\pi_\theta}(s,a) \right] } \\ &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a)\nabla_\theta \log \pi_\theta(s,a)^T w \right] } \\ &amp;= \mathsf{ G_\theta w } \\ \mathsf{ \nabla^{nat}_\theta J(\theta) } &amp;= \mathsf{ w } \\ \end{aligned}\]<ul><li>i.e. update actor parameters in direction of critic parameters</ul><h2 id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</h2><ul><li>The policy gradient has many equivalent forms</ul>\[\begin{alignat*}{3} \mathsf{ \nabla_\theta J(\theta) } &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a)\; {\color{Red} v_t } \right] } \quad &amp; \text{REINFORCE} \\ &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a) \;{\color{Red} Q^w(s,a) } \right] } \quad &amp; \text{Q Actor-Critic} \\ &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a) \; {\color{Red} A^w(s,a) } \right] } \quad &amp; \text{Advantage Actor-Critic} \\ &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a) \;{\color{Red} \delta } \right] } \quad &amp; \text{TD Actor-Critic} \\ &amp;= \mathsf{ \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(s,a) \;{\color{Red} \delta e } \right] } \quad &amp; \text{TD($\lambda$) Actor-Critic} \\ \mathsf{ G^{-1}_\theta\nabla_\theta J(\theta) } &amp;= \mathsf{ w } &amp; \text{Natural Actor-Critic} \end{alignat*}\]<ul><li>Each leads a stochastic gradient ascent algorithm<li>Critic uses policy evaluation (e.g. MC or TD learning) to estimate $\mathsf{Q^\pi (s,a), A^\pi(s,a) \text{ or } V^\pi(s)}$</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 7 Policy Gradient - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-7/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 7 Policy Gradient - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-7/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 7 Policy Gradient - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-7/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-6/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 6 Value Function Approximation</p></a> <a href="/posts/RL-course-Note-8/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 8 Integrating Learning and Planning</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
