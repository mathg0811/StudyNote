<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 6 Value Function Approximation" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-6/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-6/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-20T17:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 6 Value Function Approximation" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-6/","@type":"BlogPosting","headline":"RLcourse note - Lecture 6 Value Function Approximation","dateModified":"2021-12-23T20:53:36+09:00","datePublished":"2021-12-20T17:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-6/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 6 Value Function Approximation | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 6 Value Function Approximation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 6 Value Function Approximation</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Dec 20, 2021, 5:00 PM +0900" >Dec 20, 2021<i class="unloaded">2021-12-20T17:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 23, 2021, 8:53 PM +0900" >Dec 23, 2021<i class="unloaded">2021-12-23T20:53:36+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2570 words">14 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/UoPei5o4fps"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec6_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h2 id="introduction-of-value-function-approximation">Introduction of Value Function Approximation</h2><p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p><h3 id="6강-소감">6강 소감</h3><p>6강은 어느정도 이해했지만 좀 찜찜한 부분들도 있다 슬슬 실전에 사용되는 내용에 많아지는 만큼 세심한 수식들이 많은데 나중에 다시한번 봐야할듯</p><h3 id="value-function-approximation">Value Function Approximation</h3><ul><li>So far we gave represented value function by a lookup table<ul><li>Every state s has an entry $\mathsf{V(s)}$<li>Or every state-action pair s, a has an entry $\mathsf{Q(s,a)}$</ul><li>Probelm with large MDPs:<ul><li>There are too many states and/or actions to store in memory<li>It is too slow to learn the value of each state individually</ul><li>Solution for large MDPs:<ul><li>Estimate value function with function approximation <br /><center>$$ \begin{aligned} \mathsf{\hat{v}(s,w)} &amp;\approx \mathsf{v_\pi(s)} \\ \mathsf{or \;\hat{q}(s,a,w)} &amp;\approx \mathsf{q_\pi(s,a)} \end{aligned}$$</center><li>Generalise from seen states to unseen states<li>Update parameter w using MC or TD learning</ul></ul><p>Function Approximators</p><ul><li>Linear combinations of features<li>Neural network<li>Decision tree<li>Nearest neighbour<li>Fourier / wavelet bases<li>…</ul><p>We require a training method that is suitable for non-stationary, non-iid data</p><h2 id="incremental-methods">Incremental Methods</h2><h3 id="gradient-descent">Gradient Descent</h3><ul><li>Let $\mathsf{J(w)}$ be a differentiable function of parameter vector w<li>Define the gradient of $\mathsf{J(w)}$ to be <br /><center>$$ \mathsf{\nabla _w J(w)} = \left( \begin{array} \mathsf{ \frac{\partial J(w)}{\partial w_1} } \\ \vdots \\ \mathsf{ \frac{\partial J(w)}{\partial w_n} } \end{array} \right) $$ </center><li>To find a local minimum of $\mathsf{J(w)}$<li>Adjust w in direction of -ve gradient <br /><center> $$ \mathsf{ \Delta w = -\frac{1}{2} \alpha \nabla _w J(w) } $$</center> where $\alpha$ is a step-size parameter</ul><p>Gradient의 minus값으로 향하는 vector이므로 local minimum을 찾게 됨</p><h4 id="value-funcion-approx-by-stochastic-gradient-descent">Value Funcion Approx. By Stochastic Gradient Descent</h4><ul><li>Goal: find parameter vector w minimising mean-squared error between approximate value fn $\mathsf{\hat{v}(s,w)}$ and true value fn $\mathsf{v_\pi(s)}$</ul>\[\mathsf{ J(w) = \mathbb{E}_\pi [(v_\pi(S) - \hat{v} (S,w))^2] }\]<ul><li>Gradient descent finds a local minimum</ul>\[\begin{aligned} \Delta \mathsf{w} &amp;= \mathsf{-\frac{1}{2}\alpha \nabla _w J(w) } \\ &amp;= \mathsf{ \alpha \mathbb{E}_\pi [(v_\pi(S) - \hat{v}(S,w))\nabla _w \hat{v} (S,w)] } \end{aligned}\]<ul><li>Stochastic gradient descent samples the gradient</ul><p>Error 의 Gradient descent를 취할 때 Target 은 fixed여야 한다</p>\[\mathsf{ \Delta w = \alpha (v_\pi(S) - \hat{v}(S,w))\nabla _w \hat{v} (S,w) }\]<ul><li>Expected update is equal to full gradient update</ul><h3 id="linear-function-approximation">Linear Function Approximation</h3><h4 id="feature-vectors">Feature Vectors</h4><ul><li>Represent state by a feature vector</ul>\[\mathsf{ x(S)} = \left( \begin{array}\, \mathsf{x_1(S)} \\ \vdots \\ \mathsf{x_n(S)} \end{array} \right)\]<ul><li>For example:<ul><li>Distance of robot from landmarks<li>Trends in the stock market<li>Piece and pawn configurations in chess</ul></ul><p>information의 집합으로 결국 가장 중요한 input임</p><h4 id="linear-value-function-approximation">Linear value Function Approximation</h4><ul><li>Represent value function by a linear combination of feature</ul>\[\mathsf{ \hat{v}(S,w) = x(S)^T w=\displaystyle\sum^n_{j=1}x_j(S)w_j }\]<ul><li>Objective function is quadratic in parameters $\mathsf{w}$</ul>\[\mathsf{ J(w) = \mathbb{E}_\pi \left[ (v_\pi(S)-x(S)^T w)^2\right] }\]<ul><li>Stochastic gradient descent converges on global optimum<li>Update rule is particularly simple</ul>\[\begin{aligned} \mathsf{\nabla _w \hat{v}(S,w)} &amp;= \mathsf{x(S)} \\ \mathsf{\Delta w} &amp;= \mathsf{ \alpha (v_\pi(S) - \hat{v}(S,w))x(S) } \end{aligned}\]<p>Update - step-size $\times$ prediction error $\times$ feature value</p><p>Linear한 경우를 굳이 보여주는 이유는 단순한 문제들에 대하여 문제를 Linear하게 설정해내서 해결해내는 경우가 많기 때문일듯</p><h4 id="table-lookup-features">Table Lookup Features</h4><ul><li>Table lookup is a special case of linear value function approximation<li>Using table lookup features</ul>\[\mathsf{ x^{table}(S)} = \left( \begin{array}\, \mathsf{1(S=s_1)} \\ \vdots \\ \mathsf{1(S=s_n)} \end{array} \right)\]<ul><li>Parameter vector $\mathsf{w}$ gives value of each individual state</ul>\[\mathsf{\hat{v}(S,w)} = \left( \begin{array}\, \mathsf{1(S=s_1)} \\ \vdots \\ \mathsf{1(S=s_n)} \end{array} \right) \cdot \left( \begin{array}\, \mathsf{w_1} \\ \vdots \\ \mathsf{w_n} \end{array} \right)\]<p>이전 강의까지 하던 table lookup 방식의 연장선이라는 것</p><h3 id="incremental-prediction-algorithm">Incremental Prediction Algorithm</h3><ul><li>Have assumed true value function $\mathsf{v_\pi(s)}$ given by supervisor<li>But in RL there is no supervisor, only rewards<li>In practice, we substitute a target for $\mathsf{v_\pi(s)}$<ul><li>For MC, the target is the return $\mathsf{G_t}$ <br /><center>$$ \mathsf{ \Delta w = \alpha({\color{red}G_t} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } $$ </center><li>For TD(0), the target is the TD target $\mathsf{R_{t+1} + \gamma\hat{v}(S_{t+1},w)}$ <br /><center>$$ \mathsf{ \Delta w = \alpha({\color{red}R_{t+1} + \gamma\hat{v}(S_{t+1},w)} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } $$ </center><li>For TD($\lambda$), the target is the $\lambda$-return $\mathsf{G^\lambda_t}$ <br /><center>$$ \mathsf{ \Delta w = \alpha({\color{red}G^\lambda_t} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } $$ </center></ul></ul><p>target을 고르는 prediction 방법들<br /> E가 왜 feature를 더하는지도 생각해봐야</p><h4 id="monte-carlo-with-value-function-approximation">Monte-Carlo with Value Function Approximation</h4><ul><li>Return $\mathsf{G_t}$ is an unbiased, noisy sample of true value $\mathsf{v_\pi(S_t)}$<li>Can therefore apply supervised learning to “training data”:</ul>\[\mathsf{ \langle S_1, G_1\rangle,\langle S_2,G_2\rangle, \dots , \langle S_T,G_T\rangle }\]<ul><li>For example, using linear Monte-Carlo policy evaluation</ul>\[\begin{aligned} \mathsf{\Delta w} &amp;= \mathsf{ \alpha({\color{red}G_t} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } \\ &amp;= \mathsf{ \alpha(G_t - \hat{v}(S_t,w))x(S_t) } \end{aligned}\]<ul><li>Monte-Carlo evaluation converges to a local optimum<li>Even when using non-linear value function approximation</ul><h4 id="td-learning-with-value-function-approximation">TD Learning with Value Function Approximation</h4><ul><li>The TD-target $\mathsf{R_{t+1} + \gamma \hat{v}(S_{t+1},w)}$ is biased sample of true value $\mathsf{v_\pi(S_t)}$<li>Can still apply supervised learning to “training data”:</ul>\[\mathsf{ \langle S_1, R_2 + \gamma\hat{v}(S_2,w) \rangle,\langle S_2,R_3 + \gamma\hat{v}(S_3,w) \rangle, \dots , \langle S_T,R_T\rangle }\]<ul><li>For example, using linear TD(0)</ul>\[\begin{aligned} \mathsf{\Delta w} &amp;= \mathsf{ \alpha({\color{red}R + \gamma\hat{v}(S',w)} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } \\ &amp;= \mathsf{ \alpha\delta x(S) } \end{aligned}\]<ul><li>Linear TD(0) convergs (close) to global optimum</ul><h4 id="tdlambda-with-value-function-approximation">TD($\lambda$) with Value Function Approximation</h4><ul><li>The $\lambda$-return $\mathsf{G^\lambda_t}$ is also a biased sample of true value $\mathsf{v_\pi(s)}$<li>Can again apply supervised learning to “training data”:</ul>\[\mathsf{ \langle S_1, G^\lambda_1\rangle,\langle S_2,G^\lambda_2\rangle, \dots , \langle S_{T-1},G^\lambda_{T-1}\rangle }\]<ul><li>Forward view linear TD($\lambda$)</ul>\[\begin{aligned} \mathsf{\Delta w} &amp;= \mathsf{ \alpha({\color{red}G^\gamma_t} - \hat{v}(S_t,w))\nabla _w\hat{v}(S_t,w) } \\ &amp;= \mathsf{ \alpha({\color{red}G^\gamma_t} - \hat{v}(S_t,w))x(S_t) } \end{aligned}\]<ul><li>Backward view linear TD($\lambda$)</ul>\[\begin{aligned} \mathsf{\delta _t} &amp;= \mathsf{ R_{t+1} +\gamma hat{v}(S_{t+1},w) - hat{v}(S_t,w) } \\ \mathsf{E_t } &amp;= \mathsf{\gamma\lambda E_{t-1} +x(S_t) } \\ \mathsf{\Delta w} &amp;= \alpha\delta _t E_t \end{aligned}\]<p>이거 E는 모든 x에 대해서 정의되는거같은데 <S> 또는 &lt;S,A&gt; 로<br /> target은 왜 Gradient에 포함하지 않는가에 대해서 Time-reversal 이라고 설명하는데 이건 좀 고민을 해봐야겠다. 실제로 해봐도 그렇게 하면 결과가 나오지 않는다고 하는데.... <br /> Target 도 gradient를 취할 경우 %\lambda$가 크면 값이 아주 작아지거나 뒤바뀌기도 하는데 그러면 아예 의미가 달라져서 잘못된 접근이 되긴 한다.</S></p><p>Forward view and backward view linear TD($\lambda$) are equivalent</p><h3 id="incremental-control-algorithm">Incremental Control Algorithm</h3><h4 id="control-with-value-function-approximation">Control with Value Function Approximation</h4><p>Policy evaluation - Approximate policy evaluation $\mathsf{\hat{q}(\cdot, \cdot, w) \approx q_\pi}$ Policy improvement $\epsilon$-greedy policy improvement</p><h4 id="action-value-function-approximation">Action-Value Function Approximation</h4><ul><li>Approximate the action-value function</ul>\[\mathsf{ \hat{q}(S,A,w) \approx q_\pi(S,A) }\]<ul><li>Minimise mean-squred error between approximate action-value fn $\mathsf{\hat{q}(S,A,w)}$ and true action-value fn $\mathsf{q_\pi (S,A)}$</ul>\[\mathsf{ J(w) = \mathbb{E}_\pi [(q_\pi(S,A) - \hat{q}(S,A,w))^2] }\]<ul><li>Use stochastic gradient descent to find a local minimum</ul>\[\begin{aligned} \mathsf{-\frac{1}{2}\nabla _w J(w)} &amp;= \mathsf{(q_\pi(S,A)-\hat{q}(S,A,w)) \Delta _w \hat{q}(S,A,w)} \\ \mathsf{\Delta w} &amp;= \mathsf{\alpha(q_\pi(S,A)-\hat{q}(S,A,w)) \Delta _w \hat{q}(S,A,w)} \end{aligned}\]<h4 id="linear-action-value-function-approximation">Linear Action-Value Function Approximation</h4><ul><li>Represent state and action by a feature vector</ul>\[\mathsf{x(S,A)} = \left( \begin{array} \; \mathsf{x_1(S,A)} \\ \vdots \\ \mathsf{x_n(S,A)} \end{array} \right)\]<ul><li>Represent action-value fn by linear combination of features</ul>\[\mathsf{ \hat{q}(S,A,w) = x(S,A)^T w = \displaystyle\sum^n_{j=1} x_j (S,A)w_j }\]<ul><li>Stochastic gradient descent update</ul>\[\begin{aligned} \mathsf{ \nabla _w \hat{q}(S,A,w) } &amp;= \mathsf{ x(S,A)} \\ \mathsf{\Delta w} &amp;= \mathsf{\alpha(q_\pi(S,A)-\hat{q}(S,A,w))x(S,A)} \end{aligned}\]<h4 id="incremental-control-algorithms">Incremental Control Algorithms</h4><ul><li>Like prediction, we must substitute a target for $\mathsf{q_\pi(S,A)}$<ul><li>For MC, the target is the return $\mathsf{G_t}$ <br /><center>$$ \mathsf{ \Delta w = \alpha ( {\color{red}G_t} - \hat{q}(S_t,A_t,w))\nabla _w \hat{q}(S_t,A_t,w) } $$ </center><li>For TD(0), the target is the TD target $\mathsf{R_{t+1} + \gamma Q(S_{t+1}A_{t+1})}$ <br /><center>$$ \mathsf{ \Delta w = \alpha ( {\color{red}R_{t+1} + \gamma Q(S_{t+1}A_{t+1})} - \hat{q}(S_t,A_t,w))\nabla _w \hat{q}(S_t,A_t,w) } $$ </center><li>For forward-view TD($\lambda$), target is the action-value $\lambda$-return <br /><center>$$ \mathsf{ \Delta w = \alpha ( {\color{red}q^\lambda_t} - \hat{q}(S_t,A_t,w))\nabla _w \hat{q}(S_t,A_t,w) } $$ </center><li>For backward-view TD($\lambda$), equialent update is <br /><center>$$ \begin{aligned} \mathsf{\delta _t} &amp;= \mathsf{ R_{t+1} +\gamma hat{v}(S_{t+1},A_{t+1},w) - hat{v}(S_t,A_t,w) } \\ \mathsf{E_t } &amp;= \mathsf{\gamma\lambda E_{t-1} +\nabla _w \hat{v}(S_t,A_t,w) } \\ \mathsf{\Delta w} &amp;= \alpha\delta _t E_t \end{aligned} $$</center></ul></ul><h3 id="convergence">Convergence</h3><h4 id="convergence-of-prodiction-algorithms">Convergence of Prodiction Algorithms</h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">On/Off-Policy<th style="text-align: center">Algorithm<th style="text-align: center">Table Lookup<th style="text-align: center">Linear<th style="text-align: center">Non-Linear<tbody><tr><td style="text-align: center">On-Policy<td style="text-align: center">MC <br /> TD(0) <br /> TD($\lambda$)<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> X <br /> X<tr><td style="text-align: center">Off-Policy<td style="text-align: center">MC <br /> TD(0) <br /> TD($\lambda$)<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> X <br /> X<td style="text-align: center">$\checkmark$ <br /> X <br /> X</table></div><h4 id="gradient-temporal-difference-learning">Gradient Temporal-Difference Learning</h4><ul><li>TD does not follow the gradient of any objective function<li>This is why TD can diverge when off-policy or using non-linear function approximation<li>Gradient TD follows true gradient of projected Bellman error</ul><div class="table-wrapper"><table><thead><tr><th style="text-align: center">On/Off-Policy<th style="text-align: center">Algorithm<th style="text-align: center">Table Lookup<th style="text-align: center">Linear<th style="text-align: center">Non-Linear<tbody><tr><td style="text-align: center">On-Policy<td style="text-align: center">MC <br /> TD(0) <br /> Gradient TD<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> X <br /> $\checkmark$<tr><td style="text-align: center">Off-Policy<td style="text-align: center">MC <br /> TD(0) <br /> Gradient TD<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> X <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> X <br /> $\checkmark$</table></div><p>Gradient TD 가 뭔진 알아서 찾아보라는 건가</p><h4 id="convergence-of-control-algorithms">Convergence of Control Algorithms</h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Algorithm<th style="text-align: center">Table Lookup<th style="text-align: center">Linear<th style="text-align: center">Non-Linear<tbody><tr><td style="text-align: center">Monte-Carlo Control <br /> Sarsa <br /> Q-learning <br /> Gradient Q-learning<td style="text-align: center">$\checkmark$ <br />$\checkmark$ <br />$\checkmark$ <br />$\checkmark$<td style="text-align: center">($\checkmark$) <br />($\checkmark$) <br />X <br />$\checkmark$<td style="text-align: center">X <br /> X <br /> X <br /> X</table></div><p>$(\checkmark) =$ chatters around near-optimal value function</p><h2 id="batch-methods">Batch Methods</h2><p>Batch Reinforcement Learning</p><ul><li>Gradient descent is simple and appealing<li>But it is not sample efficient<li>Batch methods seek to find the best fitting value function<li>Given the agent’s experience (“training data”)</ul><h3 id="least-squares-prediction">Least Squares Prediction</h3><ul><li>Given value function approximation $\mathsf{\hat{v}(s,w) \approx v_\pi(s)}$<li>And experience $\mathcal{D}$ consisting of &lt;state, value&gt; pairs</ul>\[\mathsf{ {\cal D} = \lbrace \langle s_1,v_1^\pi \rangle, \langle s_2,v_2^\pi \rangle , \dots, \langle s_T,v_T^\pi \rangle \rbrace }\]<ul><li>Which parameters $\mathsf{w}$ give the best fitting value fn $\mathsf{\hat{v}(s,w)}$?<li>Least squares algorithms find parameter vector w minimising sum-squared error between $\mathsf{\hat{v}(s_t,w)}$ and target values $\mathsf{v^\pi_t}$</ul>\[\begin{aligned} \mathsf{ LS(w) } &amp;= \mathsf{ \displaystyle\sum^T_{t=1} (v^\pi_t - \hat{v}(s_t, w))^2 } \\ &amp;= \mathsf{ \mathbb{E}_\mathcal{D} [(v^\pi - \hat{v}(s,w))^2] } \end{aligned}\]<h4 id="stochastic-gradient-descent-with-experience-replay">Stochastic Gradient Descent with Experience Replay</h4><p>Given experience consisting of &lt;state, value&gt; pairs</p>\[\mathsf{ \mathcal{D} = \lbrace \langle s_1, v^\pi_1 \rangle , \langle s_2, v^\pi_2 \rangle , \dots , \langle s_T, v^\pi_T \rangle \rbrace }\]<p>Repeat:</p><ol><li>Sample state, value from experience <br /><center>$$ \mathsf{ \langle s, v^\pi \rangle \sim \mathcal{D} } $$</center><li>Apply stochastic gradient descent update <br /><center>$$ \mathsf{ \Delta w = \alpha (v^\pi - \hat{v} (s,w)) \nabla _w \hat{v} (s,w) } $$</center></ol><p>Converges to least squares solution</p>\[\mathsf{ w^\pi = \underset{w}{argmin} \; LS(w) }\]<h4 id="experience-replay-in-deep-q-networks-dqn">Experience Replay in Deep Q-Networks (DQN)</h4><p>DQN uses experience replay and <font color="red">fixed Q-targets</font></p><ul><li>Take action $\mathsf{a_t}$ according to $\epsilon$-greedy policy<li>Store transition $\mathsf{(s_t, a_t, r_{t+1}, s_{t+1})}$ in replay memory $\mathcal{D}$<li>Sample random mini-batch of transitions $\mathsf{(s, a, r, s’)}$ from $\mathcal{D}$<li>Compute !-learning targets w.r.t. old, fixed parameters $w^-$<li>Optimise MSE between Q-network and Q-learning targets</ul>\[\mathsf{ \mathcal{L}_i(w_i) = \mathbb{E}_{s,a,r,s' \sim \mathcal{D}_i} \left[ \left( r+\gamma \,\underset{a'}{max} \; Q(s', a'; w^-_i ) - Q(s,a,;w_i) \right)^2 \right] }\]<ul><li>Using variant of stochastic gradient descent</ul><h4 id="dqn-in-atari">DQN in Atari</h4><ul><li>End-toend learning of values Q(s,a) from pixels s<li>Input state s is stack of raw pixels from last 4 frames<li>Output is $\mathsf{Q(s,a)}$ for 18 joystick/button positions<li>Reward is change in score for that step</ul><h4 id="linear-least-squares-prediction">Linear Least Squares Prediction</h4><ul><li>Experience replay finds least squares solution<li>But it may take many iterations<li>Using linear value funciton approximation $\mathsf{\hat{v}(s,w) = x(s)^T w}$<li>We can solve the least squares solution directly<li>At minimum of $\mathsf{LS(w)}$, the expected update must be zero</ul>\[\begin{aligned} \mathbb{E}_\mathcal{D} [\Delta w] &amp;= 0 \\ \mathsf{ \alpha \displaystyle\sum^T_{t=1} x(s_t) (v^\pi_t - x(s_t)^T w ) } &amp;= 0 \\ \mathsf{\displaystyle\sum^T_{t=1} x(s_t) v^\pi_t} &amp;= \mathsf{ \displaystyle\sum^T_{t=1} x(s_t)x(s_t)^T w } \\ \mathsf{w} &amp;= \mathsf{ \left( \displaystyle\sum^T_{t=1} x(s_t)x(s_t)^T \right)^{-1} \displaystyle\sum^T_{t=1} x(s_t)v^\pi_t } \end{aligned}\]<ul><li>For N features, direct solution time is $\mathsf{O(N^3)}$<li>Incremental solution time is $\mathsf{O(N^2)}$ using Shermann-Morrison</ul><h4 id="linear-least-squares-prediction-algorithms">Linear Least Squares Prediction Algorithms</h4><ul><li>We do not know true value $\mathsf{v^\pi_t}$<li>In practice, our “training data” must use noisy or biased samples of $\mathsf{v^\pi_t}$</ul>\[\begin{alignat*}{2} &amp;\color{ProcessBlue}{\text{LSMC}}\; \; &amp;&amp; \text{Least Squares Monte-Carlo uses return} \\ &amp; \; &amp;&amp; \mathsf{v^pi_t \approx \color{Red}{G_t}} \\ &amp;\color{ProcessBlue}{\text{LSTD}}\; \; &amp;&amp; \text{Least Squares Temporal-Difference uses TD target} \\ &amp; \; &amp;&amp; \mathsf{v^pi_t \approx \color{Red}{R_{t+1} + \gamma \hat{v} (S_{t+1}, w)}} \\ &amp;\color{ProcessBlue}{\text{LSTD} (\lambda)}\; \; &amp;&amp; \text{Least Squares TD($\lambda$) uses $\lambda$ -return} \\ &amp; \; &amp;&amp; \mathsf{v^pi_t \approx \color{Red}{G^\lambda_t}} \\ \end{alignat*}\]<ul><li>In each case solve directly for fixed point of MC / TD / TD($\lambda$)</ul>\[\begin{align*} \color{ProcessBlue}{\text{LSMC}} &amp;&amp; \; 0\; &amp; \mathsf{= \displaystyle\sum^T_{t=1} \alpha (G_t - \hat{v}(S_t,w))x(S_t)} \\ &amp;&amp; \; w\; &amp; \mathsf{= \left( \displaystyle\sum^T_{t=1} x(S_t) x(S_t)^T \right)^{-1} \displaystyle\sum^T_{t=1}x(S_t)G_t } \\ \color{ProcessBlue}{\text{LSTD}} &amp;&amp; \; 0\; &amp; \mathsf{= \displaystyle\sum^T_{t=1} \alpha (R_{t+1} + \gamma \hat{v} (S_{t+1}, w) - \hat{v}(S_t,w))x(S_t)} \\ &amp;&amp; \; w\; &amp; \mathsf{= \left( \displaystyle\sum^T_{t=1} x(S_t)( x(S_t) - \gamma x(S_{t+1}))^T \right)^{-1} \displaystyle\sum^T_{t=1}x(S_t)R_{t+1} } \\ \color{ProcessBlue}{\text{LSTD} (\lambda)} &amp;&amp; \; 0\; &amp; \mathsf{= \displaystyle\sum^T_{t=1} \alpha \delta _t E_t} \\ &amp;&amp; \; w\; &amp; \mathsf{= \left( \displaystyle\sum^T_{t=1} E_t( x(S_t) - \gamma x(S_{t+1}) )^T \right)^{-1} \displaystyle\sum^T_{t=1} E_t R_{t+1} } \\ \end{align*}\]<h4 id="convergence-of-linear-least-squares-prediction-algorithms">Convergence of Linear Least Squares Prediction Algorithms</h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">On/Off-Policy<th style="text-align: center">Algorithm<th style="text-align: center">Table Lookup<th style="text-align: center">Linear<th style="text-align: center">Non-Linear<tbody><tr><td style="text-align: center">On-Policy<td style="text-align: center">MC <br /> LSMC <br /> TD <br /> LSTD<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> - <br /> X <br /> -<tr><td style="text-align: center">Off-Policy<td style="text-align: center">MC <br /> LSMC <br /> TD <br /> LSTD<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> X <br /> $\checkmark$<td style="text-align: center">$\checkmark$ <br /> - <br /> X <br /> -</table></div><h3 id="least-squares-control">Least Squares Control</h3><h4 id="least-squares-policy-iteration">Least Squares Policy Iteration</h4><p>Policy evaluation - Policy evaluation by least squares Q-learning <br /> Policy improvement Greedy policy improvement</p><h4 id="least-squares-action-value-function-approximation">Least Squares Action-Value Function Approximation</h4><ul><li>Approximate action-value function $\mathsf{q_\pi(s,a)}$<li>using linear combination of features $\mathsf{x(s,a)}$</ul>\[\mathsf{ \hat{q}(s,a,w) = x(s,a)^T w \approx q_\pi(s,a) }\]<ul><li>Minimise least squares error between $\mathsf{\hat{q}(s,a,w)}$ and $\mathsf{q(s,a)}$<li>from experience generated using policy $\pi$<li>consisting of &lt;(state, action), value&gt; pairs</ul>\[\mathsf{ \mathcal{D} = \lbrace \langle (s_1,a_1), v^\pi_1\rangle, \langle (s_2,a_2), v^\pi_2\rangle, \dots, \langle (s_T,a_T), v^\pi_T\rangle \rbrace }\]<h4 id="least-square-control">Least Square Control</h4><ul><li>For policy evaluation, we want to efficiently use all experience<li>For control, we also want to improve the policy<li>This experience is generated from many policies<li>So to evaluate $\mathsf{q_\pi(S,A)}$ we must learn off-policy<li>We use the same idea as Q-learning:<ul><li>Use experience generated by old policy <br />\(\mathsf{ S_t, A_t, R_{t+1}, S_{t+1} \sim \pi_{old} }\)<li>Consider alternative successor action $\mathsf{A’ = \pi_{new}(S_{t+1})}$<li>Update $\mathsf{\hat{q}(S_t,A_t,w)}$ towards value of alternative action <br />\(\mathsf{ R_{t+1} + \gamma \hat{q}(S_{t+1}, A', w) }\)</ul></ul><h4 id="least-squares-q-learning">Least Squares Q-Learning</h4><ul><li>Consider the following linear Q-learning update</ul>\[\begin{aligned} \delta &amp;= \mathsf{ R_{t+1} + \gamma \hat{q} (S_{t+1}, \pi(S_{t+1}),w) - \hat{q}(S_t,A_t,w) } \\ \Delta \mathsf{w} &amp;= \mathsf{ \alpha \delta x(S_t, A_t) } \end{aligned}\]<ul><li>LSTDQ algorithm: solve for total update = zero</ul>\[\begin{aligned} 0 &amp;= \mathsf{ \displaystyle\sum^T_{t=1} \alpha(R_{t+1} + \gamma \hat{q} (S_{t+1}, \pi(S_{t+1}),w) - \hat{q}(S_t,A_t,w)) x(S_t,A_t) } \\ \mathsf{w} &amp;= \mathsf{ \left( \displaystyle\sum^T_{t=1} x(S_t,A_t)(x(S_t,A_t) - \gamma x(S_{t+1}, \pi(S_{t+1})))^T \right)^{-1} \displaystyle\sum^T_{t=1} x(S_t, A_t)R_{t+1} } \end{aligned}\]<h4 id="least-squares-policy-iteration-algorithm">Least Squares Policy Iteration Algorithm</h4><ul><li>The following pseudocode uses LSTDQ for policy evaluation<li>It repeatedly re-evaluates experience $\mathcal{D}$ with different policies</ul><div class="table-wrapper"><table><tbody><tr><td>function LSPI-TD($\mathcal{D}, \pi_0$) <br /> $\quad$ $\pi’ \leftarrow \pi_0$ <br /> $\quad$ Repeat <br />$\quad\quad$ $\pi \leftarrow \pi’$ <br />$\quad\quad$ \(\mathsf{ Q \leftarrow LSTDQ(\pi, \mathcal{D})}\) <br />$\quad\quad$ for all $\mathsf{s} \in \mathcal{S}$ do <br />$\quad\quad\quad$ $ \mathsf{ \pi’(s) \leftarrow \underset{a\in\mathcal{A}}{argmax}\; Q(s,a) } $ <br />$\quad\quad$ end for <br />$\quad$ until $(\pi \approx \pi’)$ <br />$\quad$ return $\pi$ <br /> end function</table></div><h4 id="convergence-of-control-algorithms-1">Convergence of Control Algorithms</h4><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Algorithm<th style="text-align: center">Table Lookup<th style="text-align: center">Linear<th style="text-align: center">Non-Linear<tbody><tr><td style="text-align: center">Monte-Carlo Control <br /> Sarsa <br /> Q-learning <br /> LSPI<td style="text-align: center">$\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$ <br /> $\checkmark$<td style="text-align: center">($\checkmark$) <br /> ($\checkmark$) <br /> X <br /> ($\checkmark$)<td style="text-align: center">X <br /> X <br /> X <br /> -</table></div><p>($\checkmark$) = chatters around near-optimal value function</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 6 Value Function Approximation - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-6/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 6 Value Function Approximation - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-6/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 6 Value Function Approximation - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-6/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-5/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 5 Model-Control</p></a> <a href="/posts/RL-course-Note-7/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 7 Policy Gradient</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
