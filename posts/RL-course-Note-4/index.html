<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 4 Model-Free Prediction" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-4/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-4/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-09T21:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 4 Model-Free Prediction" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-4/","@type":"BlogPosting","headline":"RLcourse note - Lecture 4 Model-Free Prediction","dateModified":"2021-12-16T22:03:34+09:00","datePublished":"2021-12-09T21:20:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-4/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 4 Model-Free Prediction | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 4 Model-Free Prediction</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 9, 2021, 9:20 PM +0900" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 16, 2021, 10:03 PM +0900" >Dec 16, 2021<i class="unloaded">2021-12-16T22:03:34+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2408 words">13 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/PnHCvfgC_ZA"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec4_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h2 id="introduction-of-model-free-prediction">Introduction of Model-Free Prediction</h2><p>Estimate the Value function of an unknown MDP</p><h3 id="4강-소감">4강 소감</h3><p>Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으로 정리가 가능한 방법이 TD인데 수식이 많이 어렵지는 않지만 강의에서는 생략된 곳도 많은 것 같다. 이 부분은 차후 책으로 증명같은 부분을 검토할 필요가 있을 것도 같다. 사실 직관적으로는 당연하고 이해하기 너무 쉬운 부분이지만 그냥 넘어가면 찝찝하니까… 이해가 잘 되면 필요에 따라 활용할 방법도 늘어나는 법이니까. 슬슬 용어들이 혼용되거나 혼동을 주는 부분들이 있는데 이것도 헷갈리지 않도록 신경써야겠다</p><h3 id="monte-carlo-reinforcement-learning">Monte-Carlo Reinforcement Learning</h3><ul><li>MC methods learn directly from episodes of experience<li>MC is model-free: no knowledge of MDP transitions / rewards<li>MC learns from complete episodes: no booststrapping<li>MC uses the simplest possible idea: value = mean return<li>Caveat: can only apply MC to episodic MDPs<ul><li>All episodes must terminate</ul><li>각 에피소드가 완료된 후에 그 Reward로부터 학습을 진행한다. bootstrapping은 불가능하고 각 에피소드는 종료되어야 한다.<li>Expected return을 계산하는 것이 아닌 실제 반환되는 return으로 계산한다.</ul><h3 id="first-vist-monte-carlo-policy-evaluation">First-Vist Monte-Carlo Policy Evaluation</h3><ul><li>Goal: learn $\mathsf{v_\pi}$ from episodes of experience under policy $\pi$</ul>\[\mathsf{ S_1, A_1, R_2, \dots, S_k \sim \pi }\]<ul><li>Monte-Carlo policy evaluation uses empirical mean return instead of expected return <em>- state s 를 최초로 visit한 time t만 사용한다.</em><li>counter $\mathsf{N(s)}$ state 별로 따로 counter를 가진다는 뜻, 각 state를 update 할 때 쓰기 위함<li>Total return $\mathsf{S(s) \leftarrow S(s) + G_t}$<li>Estimated Value by mean return $\mathsf{V(s) = S(s) /N}$<li>By law of large number, $\mathsf{V(s) \rightarrow v_\pi(s) \;as\; N \rightarrow \infty}$ V는 mean return이고 v는 state return인데 이것도 어이없네…</ul><h3 id="every-visit-monte-carlo-policy-evaluation">Every-Visit Monte-Carlo Policy Evaluation</h3><ul><li>Goal: learn $\mathsf{v_\pi}$ from episodes of experience under policy $\pi$</ul>\[\mathsf{ S_1, A_1, R_2, \dots, S_k \sim \pi }\]<ul><li>Monte-Carlo policy evaluation uses empirical mean return instead of expected return<li><em>state s를 방문하는 모든 step t를 이용한다.</em><li>counter $\mathsf{N(s)}$<li>Total return $\mathsf{S(s) \leftarrow S(s) + G_t}$<li>Estimated Value by mean return $\mathsf{V(s) = S(s) /N}$<li>By law of large number, $\mathsf{V(s) \rightarrow v_\pi(s) \;as\; N \rightarrow \infty}$<li>예시로 Blackjack 이 주어졌는데 사실 이 예시는 MDP로 정의되지 않음 ㅋㅋ</ul><h3 id="incremental-mean">Incremental Mean</h3><p>The mean $\mu_1, \mu_2, \dots$ of a sequence $\mathsf{x_1,x_2,\dots}$ can be computed incrementally, $\mu_k$ 는 sequence $\mathsf{x_1, \dots, x_k}$ 의 평균</p>\[\begin{aligned} \mu _k &amp;=\mathsf{\frac{1}{k} \displaystyle\sum^{k}_{j=1} x_j =\frac{1}{k} \left( x_k + \displaystyle\sum^{k-1}_{j=1} x_j \right)}\\ &amp;=\mathsf{\frac{1}{k} (x_k + (k-1)\mu_{k-1})}\\ &amp;=\mathsf{\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})}\\ \end{aligned}\]<p>별거아닌 식인데 귀찮게 써버렸네</p><h3 id="incremental-monte-carlo-updates">Incremental Monte-Carlo Updates</h3><ul><li>For each state $\mathsf{S_t}$ with return $\mathsf{G_t}$</ul>\[\mathsf{V(S_t) \leftarrow V(S_t) + \frac{1}{N} (G_t - V(S_t))}\]<p>S_t 흠…. 그리고 S total return이랑 겹치네</p><ul><li>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes</ul>\[\mathsf{V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))}\]<ul><li>1/N 이 아니라 $\alpha$로 고정하는 경우 1/N보다 과거 정보 비중이 exponential로 줄어든다. 이것저것 해보는 느낌이군</ul><h2 id="temporal-difference-learning">Temporal-Difference Learning</h2><ul><li>TD methods learn directly from episodes of experience<li>TD is model-free: no knowledge of MDP transitions / rewards<li>TD learns from incomplete episodes, by <em>bootstrapping</em><li>TD updates a guess towards a guess<li>간단하게는 실시간으로 업데이트가 가능한 것도 장점이긴 하다 업데이트 방법을 다양하게 활용할 수 있는게 더 좋은듯</ul><h3 id="mc-and-td">MC and TD</h3><ul><li>Goal: learn $\mathsf{v_\pi}$ online from experience under policy $\pi$<li>Incremental every-visit Monte-Carlo<ul><li>Update value $\mathsf{V(S_t)}$ toward actual return $\mathsf{G_t}$</ul></ul>\[\mathsf{ V(S_t) \leftarrow V(S_t) +\alpha (G_t - V(S_t))}\]<ul><li>Simplest temporal-difference learning algorithm: TD(0)<ul><li>Update value $\mathsf{V(S_t)}$ toward estimated return $\mathsf{R_{t+1} +\gamma V(S_t)}$</ul></ul>\[\mathsf{ V(S_t) \leftarrow V(S_t) +\alpha (R_{t+1} +\gamma V(S_t) - V(S_t))}\]<ul><li>$\mathsf{R_{t+1} +\gamma V(S_{t+1})} $ is called the TD target<li>$\mathsf{ \delta _t = R _{t+1} + \gamma V ( S _{t+1})-V(S _t)}$ is called the TD error</ul><h3 id="advantages-and-disadvantages-of-mc-vs-td">Advantages and Disadvantages of MC vs. TD</h3><ul><li>TD can learn before knowing the final outcome<ul><li>TD can learn online after every step<li>MC must wait until end of episode before reutrn is known</ul><li>TD can learn without the final outcome<ul><li>TD can learn from incomplete sequences<li>MC can only learn from complete sequences<li>TD works in continuing (non-terminating) environmnets<li>MC only works for episodic (terminating) environments</ul></ul><p>장단점이라더니 다 TD 장점이네</p><h3 id="biasvariance-trade-off">Bias/Variance Trade-Off</h3><ul><li>Return $\mathsf{G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma ^{T-1} R_T}$ is unbiased estimate of $\mathsf{v_\pi(S_t)}$<li>$\mathsf{v_\pi}$가 DP일 때랑 다른가? $\mathsf{G_t}$는 단일 episode에서의 return이고 최적 value와는 다른 값이 나올수 있을것같은데… episode마다 다를거같은데<li>True TD target $\mathsf{R_{t+1} + \gamma v-\pi(S_{t+1})}$ is unbiased estimate of $\mathsf{v_\pi(S_t)}$<li>이것도 마찬가지… $v_\pi$는 TD target의 expectation이지 모든 episode에서 같아지는 건 아닐거 같은데… estimate라고 해도 말이 거꾸로 뒤집한거같고.. $v_\pi$가 target의 estimate 아닌가<li>TD target $\mathsf{R_{t+1} + \gamma V(S_{t+1})}$ is biased estimate of $\mathsf{v_\pi(S_t)}$<li>수렴할 때 bias가 생기는 건 흔한 일이긴 한데 그 뜻 맞나<li>TD target is much lower variance than the return:<ul><li>Return depends on many random actions, transitions, rewards<li>TD target depends on one random action, transition, reward</ul><li>return은 최종결과까지 다 고려하니까 variance가 큰 건 당연</ul><h3 id="advantages-and-disadvantages-of-mc-vs-td-2">Advantages and Disadvantages of MC vs. TD 2</h3><ul><li>MC has gigh variance, zero bias<ul><li>Good convergence properties<li>(even with function approximation)<li>Not very sensitive to initial value<li>Very simple to understand and use</ul><li>TD has low variance, some bias<ul><li>Usually more efficient than MC<li>TD(0) converges to $\mathsf{v_\pi(s)}$<li>(but not always with function approximation)<li>More sensitive to initial value</ul><li>MC와 결국 통하는거 같은데 왜 bias가 생기는 걸까</ul><h3 id="batch-mc-and-td">Batch MC and TD</h3><ul><li>MC and TD converge: $\mathsf{V(s) \rightarrow v_\pi(s)}$ as experience $\rightarrow \infty$<li>But what about batch solution for finite experience?</ul>\[\begin{aligned} &amp;\mathsf{s^1_s, a^1_1, r^1_2, \dots , s^1_{T_1}} \\ &amp;\quad\quad\vdots \\ &amp;\mathsf{s^K_s, a^K_1, r^K_2, \dots , s^K_{T_K}} \end{aligned}\]<ul><li>e.g Repeatedly sample episode $\mathsf{k\in[1,K]}$<li>Apply MC or TD(0) to episode $\mathsf{k}$</ul><h3 id="certainty-equivalence">Certainty Equivalence</h3><ul><li>EC converges to solution with minimum mean-squared error<ul><li>Best fit to the observed returns<br /> \(\displaystyle\sum^K_{k=1} \sum^{T_k}_{t=1}(G^k_t - V (s^k_t))^2\)</ul><li>TD(0) converges to solution of max likelihood Markov model<ul><li>Solution to the MDP $\mathcal{\langle S, A, \hat{P}, \hat{R}, \gamma \rangle}$ that best fits the data<br /> \(\begin{aligned} \mathsf{\hat{\mathcal{P}}^a_{s,s'}} &amp;= \mathsf{\frac{1}{N(s,a)} \displaystyle\sum^K_{k=1}\sum^{T_k}_{t=1}1(s^k_t, a^k_t, s^k_{t+1} = s, a, s')} \\ \mathsf{\hat{\mathcal{R}}^a_s} &amp;= \mathsf{\frac{1}{N(s,a)} \displaystyle\sum^K_{k=1}\sum^{T_k}_{t=1}1(s^k_t, a^k_t = s, a)r^k_t} \end{aligned}\)</ul></ul><h3 id="advantages-and-disadvantages-of-mc-vs-td-3">Advantages and Disadvantages of MC vs. TD 3</h3><ul><li>TD exploits Markov property<ul><li>Usually more efficient in Markov environments</ul><li>MC does not exploit Markov property<ul><li>Usually more effective in non-Markov environmnets</ul></ul><h3 id="bootstrapping-and-sampling">Bootstrapping and Sampling</h3><ul><li>Bootstrapping: update involves an estimate<ul><li>MC does not bootstrap<li>DP bootstraps<li>TD bootstraps</ul><li>Sampling: update samples an expectation<ul><li>MC samples<li>DP does not sample<li>TD samples</ul></ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 400px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/Note3_figure1.JPG" alt="thumb1" width="400px" height="400px" /></p><h2 id="td-lambda">TD ($\lambda$)</h2><h3 id="n-step-prediction">n-Step Prediction</h3><ul><li>TD target look n steps into the future<li>$\infty$-step prediction goes to MC</ul><h3 id="n-step-return">n-Step Return</h3><ul><li>Consider the following n-step returns for n=1,2,$\infty$:</ul>\[\begin{aligned} \mathsf{n=1\quad (TD)\quad} &amp; \mathsf{G^{(1)}_t = R_{t+1} + \gamma V(S_{t+1})} \\ \mathsf{n=2\quad \quad\quad\quad\,} &amp; \mathsf{G^{(2)}_t = R_{t+1} + \gamma R_{t+2} + \gamma ^2 V(S_{t+2})} \\ \vdots \quad\quad\quad\quad\quad\; &amp; \;\, \vdots \\ \mathsf{n=\infty \;\, (MC)\quad} &amp; \mathsf{G^{(\infty)}_t = R_{t+1} + \gamma R_{t+2} +\dots + \gamma ^{T-1} R_T} \end{aligned}\]<ul><li>Define the n-step return</ul>\[\mathsf{G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma ^{n-1} R_{t+n} + \gamma ^n V(S_{t+n})}\]<ul><li>n-step temporal-difference learning</ul>\[\mathsf{ V(S_t) \leftarrow V(S_t) +\alpha (G^{(n)}_t - V(S_t))}\]<h3 id="averaging-n-step-returns">Averaging n-Step Returns</h3><ul><li>We can average n-step returns over different n<li>e.g. average the 2-step and 4-step returns</ul>\[\mathsf{\frac{1}{2} G^{(2)} + \frac{1}{2} G^{(4)} }\]<ul><li>Combines information from two different time-steps<li>Can we efficiently combine information from all time-steps?</ul><h3 id="lambda-return">$\lambda$-return</h3><ul><li>The $\lambda$-return $\mathsf{G^\lambda _t }$ combines all n-step reutrns $\mathsf{G^{(n)}_t}$<li>Using weight $\mathsf{(1-\lambda)\lambda ^{n-1}}$</ul>\[\mathsf{G^\lambda_t = (1-\lambda) \displaystyle\sum^\infty_{n=1} \lambda^{n-1} G^{(n)}_t}\]<ul><li><p>discount $\lambda$를 주면서 평균을 구하는 방법임</p><li><p>Forward-view TD($\lambda$)</p></ul>\[\mathsf{ V(S_t) \leftarrow V(S_t) +\alpha (G^\lambda_t - V(S_t))}\]<ul><li>정체 평균 G 에 대하여 update 함</ul><h3 id="tdlambda-weighting-function">TD($\lambda$) Weighting Function</h3>\[\mathsf{G^\lambda_t = (1-\lambda) \displaystyle\sum^\infty_{n=1} \lambda^{n-1} G^{(n)}_t}\]<h3 id="forward-view-tdlambda">Forward-view TD($\lambda$)</h3><ul><li>update value function towards the $\lambda$-return<li>Forward-view looks into the future to compute $mathsf{G^\lambda_t}$<li>Like MC, can only be computed from episodes</ul><h3 id="backward-view-tdlambda">Backward View TD($\lambda$)</h3><ul><li>Forward view provides theory<li>Backward view provides mechanism<li><p>Update online, every step, from incomplete sequences</p><li>Keep an eligibility trace for every state s<li>Update value $\mathsf{V(s)}$ for every state $\mathsf{s}$</ul>\[\begin{aligned} \delta_t &amp;= \mathsf{R_{t+1} + \gamma V(S_{t+1}) - V(S_t)}\\ \mathsf{V(s)} &amp;\leftarrow V(s) + \alpha\delta _t E_t(s) \end{aligned}\]<h3 id="tdlambda-and-td0">TD($\lambda$) and TD(0)</h3><ul><li>When $\lambda = 0$, only current state is updated</ul>\[\begin{aligned} \mathsf{E_t(s)} &amp;= \mathsf{1(S_t = s)}\\ \mathsf{V(s)} &amp;\leftarrow V(s) + \alpha\delta _t E_t(s) \end{aligned}\]<ul><li>This is exactly equivalent to TD(0) update</ul>\[\mathsf{V(s) \leftarrow V(s) + \alpha\delta _t}\]<h3 id="tdlambda-and-mc">TD($\lambda$) and MC</h3><ul><li>When $\lambda =1$, credit is deferred until end of episode<li>Consider episodic environmnets with offline updates<li>Over the course of an episode, total update for TD(1) is the same as total update for MC<li>이유가 생략되었는데 나중에 책봐야할듯</ul><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>The sum of offline updates is identical for forward-view and backward-view TD($\lambda$)<br /><center>$$ \mathsf{\displaystyle\sum ^T_{t=1} \alpha\delta_t E_t(s) = \sum^T_{t=1} \alpha \left( G^\lambda_t - V(S_t)\right)1(S_t=s)} $$</center></table></div><p>강의는 여기까지</p><h3 id="mc-and-td1">MC and TD(1)</h3><ul><li>Consider an episode where $\mathsf{s}$ is visited once at time-step k,<li>TD(1) eligibility trace discounts time since visit,</ul>\[\begin{aligned} \mathsf{E_t(s)} &amp;= \mathsf{ \gamma E_{t-1}(s) + 1(S_t =s)} \\ &amp;= \begin{cases}\; 0 &amp; \mathsf{if\; t&lt; k} \\ \;\mathsf{\gamma ^{t-k}} &amp; \mathsf{if\; t\geq k} \end{cases} \end{aligned}\]<ul><li>TD(1) updates accumulate error online</ul>\[\mathsf{\displaystyle \sum^{T-1}_{t=1} \alpha\delta_t E_t(s) = \alpha \sum^{T-1}_{t=k} \gamma^{t-k}\delta_t = \alpha(G_k - V(S_k))}\]<ul><li>By end of episode it accumulates total error</ul>\[\mathsf{\delta_k + \gamma\delta_{k+1} + \gamma^2\delta_{k+2} + \dots + \gamma^{T-1-k} \delta_{T-1}}\]<h3 id="telescoping-in-td1">Telescoping in TD(1)</h3><p>When $\lambda = 1$, sum of TD errors telescopes into MC error,</p>\[\begin{aligned} &amp; \mathsf{\delta_k + \gamma\delta_{k+1} + \gamma^2\delta_{k+2} + \dots + \gamma^{T-1-k} \delta_{T-1}} \\ =&amp; \mathsf{R_{t+1} + \gamma V(S_{t+1}) - V(S_t)} \\ +&amp; \gamma \mathsf{R_{t+2} + \gamma^2 V(S_{t+2}) - \gamma V(S_{t+1})} \\ +&amp; \gamma^2 \mathsf{R_{t+3} + \gamma^3 V(S_{t+3}) - \gamma^2 V(S_{t+2})} \\ &amp; \quad \vdots \\ +&amp; \gamma^{T-1-t} \mathsf{R_T + \gamma^{T-t} V(S_T) - \gamma^{T-1-t} V(S_{T-1})} \\ =&amp; \mathsf{R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1-t} R_T - V(S_t)} \\ =&amp; \mathsf{G_t - V(S_t)} \end{aligned}\]<h3 id="tdlambda-and-td1">TD($\lambda$) and TD(1)</h3><ul><li>TD(1) is roughly equivalent to every-visit Monte-Carlo<li>Error is accumulated online, step-by-step<li>If value function is only updated offline at end of episode<li>Then total update is exactly the same as MC</ul><h3 id="telescoping-in-tdlambda">Telescoping in TD($\lambda$)</h3><p>For general $\lambda$, TD errors also telescope to $\lambda$-error, $\mathsf{G^\lambda_t - V(S_t)} $</p>\[\begin{aligned} \mathsf{G^\lambda _t - V(S_t) }=&amp; \mathsf{-V(S_t) + (1-\lambda)\lambda^0(R_{t+1} + \gamma V(S_{t+1})) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (1-\lambda)\lambda^1 (R_{t+1}+\gamma R_{t+2} + \gamma^2 V(S_{t+2})) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (1-\lambda)\lambda^2 (R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + \gamma^3 V(S_{t+3})) } \\ &amp; \quad\quad\quad\;\,+\,\dots\\ =&amp; \mathsf{-V(S_t) + (\gamma\lambda)^0(R_{t+1} + \gamma V(S_{t+1}) -\gamma\lambda V(S_{t+1})) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (\gamma\lambda)^1 (R_{t+2}+ \gamma V(S_{t+2}) -\gamma\lambda V(S_{t+2})) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (\gamma\lambda)^2 (R_{t+3}+ \gamma V(S_{t+3}) -\gamma\lambda V(S_{t+3})) } \\ &amp; \quad\quad\quad\;\,+\,\dots\\ =&amp; \quad\quad\quad\quad\;\, \mathsf{ (\gamma\lambda)^0(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (\gamma\lambda)^1 (R_{t+2}+ \gamma V(S_{t+2}) -V(S_{t+1})) } \\ &amp; \quad\quad\quad\;\,\mathsf{+\, (\gamma\lambda)^2 (R_{t+3}+ \gamma V(S_{t+3}) -V(S_{t+2})) } \\ &amp; \quad\quad\quad\;\,+\,\dots\\ =&amp;\;\mathsf{\delta _t + \gamma\lambda\delta _{t+1} + (\gamma\lambda)^2 \delta_{t+2} + \dots} \end{aligned}\]<h3 id="forward-and-backwards-tdlambda">Forward and Backwards TD($\lambda$)</h3><ul><li>Consider an episode where s is visited once at time-step k<li>TD($\lambda$) eligibility trace discounts time since visit,</ul>\[\begin{aligned} \mathsf{E_t(s)} &amp;= \mathsf{ \gamma\lambda E_{t-1}(s) + 1(S_t =s)} \\ &amp;= \begin{cases}\; 0 &amp; \mathsf{if\; t&lt; k} \\ \;\mathsf{(\gamma\lambda)^{t-k}} &amp; \mathsf{if\; t\geq k} \end{cases} \end{aligned}\]<ul><li>Backward TD($\lambda$) updates accumulate error online</ul>\[\mathsf{\displaystyle \sum^T_{t=1} \alpha\delta_t E_t(s) = \alpha \sum^T_{t=k} (\gamma\lambda)^{t-k}\delta_t = \alpha(G^\lambda_k - V(S_k))}\]<ul><li>By end of episode it accumulates total error for $\lambda$-return<li>For multiple visits to $\mathsf{s, E_t(s)}$ accumulates many errors</ul><h3 id="offline-equivalence-of-forward-and-backward-td">Offline Equivalence of Forward and Backward TD</h3><p>Offline updates</p><ul><li>Updates are accumulated within episode<li>but applied in batch at the end of episode</ul><p>Online updates</p><ul><li>TD($\lambda$) updates are applied online at each step within episode<li>Forward and backward-view TD($\lambda$) are slightly different<li>NEW: Exact online TD($\lambda$) achieves perfect equivalene<li>By using a slightly different form of eligibility trace</ul><h3 id="summary-of-forward-and-backward-tdlambda">Summary of Forward and Backward TD($\lambda$)</h3><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Offline updates<th style="text-align: center">$\lambda=0$<th style="text-align: center">$\lambda \in (0,1)$<th style="text-align: center">$\lambda =1$<tbody><tr><td style="text-align: center">Backward view <br /><br />Forward view<td style="text-align: center">TD(0) <br /> $\parallel$ <br /> TD(0)<td style="text-align: center">TD($\lambda$) <br /> $\parallel$ <br /> Forward TD($\lambda$)<td style="text-align: center">TD(1) <br /> $\parallel$ <br /> MC</table></div><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Online updates<th style="text-align: center">$\lambda=0$<th style="text-align: center">$\lambda \in (0,1)$<th style="text-align: center">$\lambda =1$<tbody><tr><td style="text-align: center">Backward view <br /><br />Forward view <br /><br /> Exact Online<td style="text-align: center">TD(0) <br /> $\parallel$ <br /> TD(0) <br /> $\parallel$ <br /> TD(0)<td style="text-align: center">TD($\lambda$) <br /> $\nparallel$ <br /> Forward TD($\lambda$) <br /> $\parallel$ <br /> Exact Online TD($\lambda$)<td style="text-align: center">TD(1) <br /> $\nparallel$ <br /> MC <br /> $\nparallel$ <br /> Exact Online TD(1)</table></div><p>here indicates equivalence in total update at end of episode.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 4 Model-Free Prediction - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-4/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 4 Model-Free Prediction - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-4/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 4 Model-Free Prediction - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-4/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-5/"><div class="card-body"> <span class="timeago small" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 5 Model-Control</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Control Optimise the Value function of an unknown MDP Model-free control can solve Some MDP problems which modelled: MDP model is unknown, but experi...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-6/"><div class="card-body"> <span class="timeago small" >Dec 20, 2021<i class="unloaded">2021-12-20T17:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 6 Value Function Approximation</h3><div class="text-muted small"><p> Video Link : Introduction of Value Function Approximation Reinforcement learning can be used to solve large problems, e.g. 6강 소감 6강은 어느정도 이해했지만 좀 찜찜한 부분들도 있다 슬슬 실전에 사용되는 내용에 많아지는 만큼 세심한 수식들이 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-3/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 3 Planning by Dynamic Programming</p></a> <a href="/posts/RL-course-Note-5/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 5 Model-Control</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
