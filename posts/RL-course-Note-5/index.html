<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="RLcourse note - Lecture 5 Model-Control" /><meta name="author" content="DS Jung" /><meta property="og:locale" content="ko" /><meta name="description" content="Video Link :" /><meta property="og:description" content="Video Link :" /><link rel="canonical" href="http://mathg0811.github.io/posts/RL-course-Note-5/" /><meta property="og:url" content="http://mathg0811.github.io/posts/RL-course-Note-5/" /><meta property="og:site_name" content="DS’s Study Note" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-16T06:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RLcourse note - Lecture 5 Model-Control" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@DS Jung" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"DS Jung"},"description":"Video Link :","url":"http://mathg0811.github.io/posts/RL-course-Note-5/","@type":"BlogPosting","headline":"RLcourse note - Lecture 5 Model-Control","dateModified":"2021-12-20T17:16:21+09:00","datePublished":"2021-12-16T06:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://mathg0811.github.io/posts/RL-course-Note-5/"},"@context":"https://schema.org"}</script><title>RLcourse note - Lecture 5 Model-Control | DS's Study Note</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DS's Study Note"><meta name="application-name" content="DS's Study Note"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/1637822709573.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">DS's Study Note</a></div><div class="site-subtitle font-italic">First Note for study</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/mathg0811" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jds.illusory','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>RLcourse note - Lecture 5 Model-Control</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>RLcourse note - Lecture 5 Model-Control</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> DS Jung </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 16, 2021, 6:00 AM +0900" >Dec 16, 2021<i class="unloaded">2021-12-16T06:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Dec 20, 2021, 5:16 PM +0900" >Dec 20, 2021<i class="unloaded">2021-12-20T17:16:21+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2899 words">16 min read</span></div></div><div class="post-content"><p>Video Link :</p><p class="text-center"><a href="https://youtu.be/0g4j2k_Ggc4"><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400px 200px'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/pic/RL_lec5_thumb.JPG" alt="thumb1" width="400px" height="200px" /></a></p><h2 id="introduction-of-model-free-control">Introduction of Model-Free Control</h2><p>Optimise the Value function of an unknown MDP<br /> Model-free control can solve Some MDP problems which modelled:</p><ul><li>MDP model is unknown, but experience can be sampled<li>MDP model is known, but is too big to use, except by samples</ul><h3 id="5강-소감">5강 소감</h3><p>Q-Learning으로 간단하지만 깔끔한 update 방법으로 정리되는 과정을 하나하나 짚어간다. 이해하고 보니 별거 아니긴 하네. 결국 여기까지 발전하기까지 누군가가 Idea를 냈다는 것 뿐. Q-learning이 유명하고 많이 보이는데 Return이 action에서 오는 경우가 많은가 보다. 그래서 action value를 사용하고 update를 조금 더 효율적으로 하면서 off-policy를 사용하도록 Algorithm이 정리된 것이다. 근데 Off policy는 이전에 학습된 episode에서도 배울수 있다는 게 내가 보기엔 중요한 장점인 것 같은데, 결국 여기까지에선 같은 state와 action을 공유하는 상황에서 확률이 다른 것들을 서로 다른 policy로 본다는 뜻이고 한두번만 개선되어도 다른 policy로 정의한다는 뜻이라서 내 기대와는 조금 다르다. 물론 설계만 잘 된다면 model 자체가 조금 달라지더라도 어느정도 공유하는 부분에 대해서는 off policy를 활용할 방법을 찾을 수도 있을 것이다. 또 다르게 학습된 policy를 사용한다면 여기서 말하는 Q-Learning과는 다른 부분이 되겠지만 이렇게 되면 target policy는 개선되지 않고 고정이 되는 만큼 다른 문제가 발생할 수 있다. 이 부분이 오히려 개선을 방해할 수 있는 만큼 policy 또한 exploration이 가능하도록 target policy를 $\epsilon$ 방법을 활용하는 방법도 체계화할 수도 있을 것이다. Q-Learning에서 Off Policy 중 다른 학습 결과를 활용하는 방법이 나오면 좋겠다.</p><h2 id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</h2><ul><li>On-policy Learning<ul><li>Learn on the job<li>Learn about policy $\pi$ from experience sampled from $\pi$</ul><li>Off-policy Learning<ul><li>Look over someone’s shoulder<li>Learn about poilcy $\pi$ from experience sampled from $\mu$</ul></ul><h3 id="generalised-policy-iteration">Generalised Policy Iteration</h3><h4 id="generalised-policy-iteration-with-monte-carlo-evaluation">Generalised Policy Iteration With Monte-Carlo Evaluation</h4><p>Policy evaluation - Monte-Carlo policy evaluation<br /> Policy improvement - Greedy policy improvement?</p><h4 id="model-free-policy-iteration-using-action-value-function">Model-Free Policy Iteration Using Action-Value Function</h4><ul><li>Greedy policy improvement over $\mathsf{V(s)}$ requires model of MDP</ul>\[\mathsf{\pi'(s) = \underset{a \in \mathcal{A}}{argmax}\;\mathcal{R}^a_s + \mathcal{P}^a_{ss'}V(s') }\]<ul><li>Greedy policy improvement over $\mathsf{Q(s,a)}$ is model-free</ul>\[\mathsf{\pi'(s) = \underset{a \in \mathcal{A}}{argmax}\;Q(s,a) }\]<p>이 부분이 아직 이해가 안된거같기도 하다. 왜 state value로 업데이트 하는건 MDP 모델이 필요하고 Action value로 업데이트하는건 model-free 인가… 왜 Q는 알수 있는데 P V는 알 수 없는가…</p><h3 id="exporation">Exporation</h3><h4 id="epsilon-greedy-exploration">$\epsilon$-Greedy Exploration</h4><ul><li>Simplest idea for ensuring continual exploration<li>All m actions are tried with non-zero probability<li>With probability $1-\epsilon$ choose the greedy action<li>With probability $\epsilon$ choose an action at random</ul>\[\mathsf{\pi(a\vert s)=} \begin{cases} \epsilon/\mathsf{m} +1 - \epsilon &amp; \mathsf{if\;a^*=\underset{a\in \mathcal{A}}{argmax}\;Q(s,a)} \\ \epsilon/\mathsf{m} &amp; \mathsf{otherwise} \end{cases}\]<p>또 또 식 이상하게 쓴다… policy 결과가 왜 숫자가 나와….. 이번엔 $\pi$ output이 action이 아니라 action을 선택할 확률이다 이거지…. 좀 문자 다른거 써라..</p><h4 id="epsilon-greedy-policy-improvement">$\epsilon$-Greedy Policy Improvement</h4><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>For any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi’$ with respect to $\mathsf{q_\pi}$ is an improvement, $\mathsf{v_{\pi’}(s)\geq v_\pi(s)}$</table></div>\[\begin{aligned} \mathsf{q_\pi (s,\pi'(s))} &amp;= \mathsf{\displaystyle\sum_{a\in\mathcal{A}}\pi'(a\vert s)q_\pi(s,a)}\\ &amp;=\mathsf{\epsilon/m \displaystyle\sum_{a\in\mathcal{A}}q_\pi (s,a)+(1-\epsilon)\underset{a\in\mathcal{A}}{max}\;q_\pi(s,a)}\\ &amp;\leq \mathsf{ \epsilon/m \displaystyle\sum_{a\in\mathcal{A}}q_\pi (s,a)+(1-\epsilon) \displaystyle\sum_{a\in\mathcal{A}} \frac{\pi(a\vert s)-\epsilon/m}{1-\epsilon}q_\pi(s,a) } \\ &amp;=\mathsf{\displaystyle\sum_{a\in\mathcal{A}}\pi(a\vert s)q_\pi(s,a)=v_\pi(s)} \end{aligned}\]<p>Therefor from policy improvement theorem, $\mathsf{v_{\pi’}(s)\geq v_\pi(s)}$</p><p>사실 greedy만 해도 improvement가 보장되었었는데 일부만 greedy 나머지는 random 할때도 보장되는 건 당연한거긴 하다. 굳이 이런건 또 증명해주네</p><h4 id="monte-carlo-policy-iteration">Monte-Carlo Policy Iteration</h4><p>Policy evaluation - Monte-Carlo policy evaluation $\mathsf{Q=q_\pi}$<br /> Policy improvement $\epsilon$-greedy policy improvement</p><h4 id="monte-carlo-policy-control">Monte-Carlo Policy Control</h4><p>Every episode:<br /> Policy evaluation - Monte-Carlo policy evaluation $\mathsf{Q\approx q_\pi}$<br /> Policy improvement $\epsilon$-greedy policy improvement</p><p>Monte-Carlo evaluation 할때 iteration 하지 않고 every episode 마다 improvement 진행</p><h3 id="glie">GLIE</h3><div class="table-wrapper"><table><thead><tr><th>Definition<tbody><tr><td>Greedy in the Limit with Infinite Exploration(GLIE)<br />$\quad \bullet\;$ All state-action pairs are explored infinitely many times,<br /><center>$$ \mathsf{ \underset{k\rightarrow\infty}{lim}\;\, N_k(s,a) = \infty } $$</center><br />$\quad \bullet\;$ The policy converges on a greedy policy,<br /><center>$$ \mathsf{ \underset{k\rightarrow\infty}{lim}\;\, \pi_k(a\vert a) = 1(a=\underset{a'\in\mathcal{A}}{argmax}\;Q_k(s,a')) } $$</center></table></div><ul><li>For example, $\epsilon$-greedy is GLIE if $\epsilon$ reduces to zero at $\mathsf{\epsilon_k = \frac{1}{k}}$</ul><h4 id="glie-monte-carlo-control">GLIE Monte-Carlo Control</h4><ul><li>Sample <em>k</em>th episode using $\pi:\mathsf{ \lbrace S_1,A_1,R_2,\dots,S_T \rbrace \sim \pi}$<li>For each state $\mathsf{S_t}$ and action $\mathsf{A_t}$ in the episode,</ul>\[\begin{aligned} &amp;\mathsf{ N(S_t,A_t)\leftarrow N(S_t,A_t)+1 }\\ &amp;\mathsf{ Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t - Q(S_t,A_t)) } \end{aligned}\]<ul><li>Improve policy based on new action-value function</ul>\[\begin{aligned} \epsilon &amp;\leftarrow \mathsf{ 1/k }\\ \pi &amp;\leftarrow \mathsf{ \epsilon-greedy(Q) } \end{aligned}\]<p>$\epsilon$ 값이 episode 마다 점점 작아짐 - exploration 감소</p><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>GLIE Monte-Carlo control converges to the optimal action-value functino, $\mathsf{ Q(s,a)\rightarrow q_*(s,a) }$</table></div><h2 id="on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</h2><h3 id="mc-vs-td-control">MC vs. TD Control</h3><ul><li>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC)<ul><li>Lower variance<li>Online<li>Incomplete sequence</ul><li>Natural idea: use TD instead of MC in our control loop<ul><li>Apply TD to $\mathsf{Q(S,A)}$<li>Use $\epsilon$-greedy policy improvement<li>Update every time-step</ul></ul><h3 id="sarsalambda">Sarsa($\lambda$)</h3><h4 id="updating-action-value-functions-with-sarsa">Updating Action-Value Functions with Sarsa</h4>\[\mathsf{ Q(S,A) \leftarrow Q(S,A) + \alpha (R+ \gamma Q(S',A') - Q(S,A)) }\]<h4 id="on-policy-control-with-sarsa">On-Policy Control With Sarsa</h4><p>Every time-step<br /> Policy evaluation Sarsa, $\mathsf{Q \approx q_\pi}$<br /> Policy improvement $\epsilon$-greedy policy improvement</p><h4 id="sarsa-algorithm-for-on-policy-control">Sarsa Algorithm for On-Policy Control</h4><div class="table-wrapper"><table><tbody><tr><td>Initialize $\mathsf{Q(s,a), \forall s \in S, a \in A(s)}$, arbitrarily,and $\mathsf{Q}(terminal state, \cdot)=0$<br />Repeat (for each episode):<br />$\quad$Initialize S<br />$\quad$Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy)<br />$\quad$Repeat (for each step of episode)<br />$\quad\quad$Take action A, observe R, S’<br />$\quad\quad$Choose A’ from S’ using policy derived from Q (e.g., $\epsilon$-greedy)<br />\(\quad\quad\mathsf{Q(S,A)\leftarrow Q(S,A) + \alpha[R+\gamma Q(S',Q') - Q(S,A)] }\)<br />\(\quad\quad\mathsf{ S\leftarrow S'; A \leftarrow A';}\)<br />$\quad$until S is terminal</table></div><p>다음 state의 action 까지 포함하는 TD의 action 확장 버전</p><h4 id="convergence-of-sarsa">Convergence of Sarsa</h4><div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>Sarsa converges to the optimal action-value function,<br />$\mathsf{Q(s,a)\rightarrow q_\pi(s,a)}$, under the following conditions:<br />$\quad \bullet\;$ GLOE sequence of policies $\mathsf{ \pi_t(a\vert s) }$<br />$\quad \bullet\;$ Robbins-Monro sequence of step-sizes $\alpha_t$<br /><center>$$\begin{aligned} &amp;\mathsf{ \displaystyle\sum^\infty_{t=1} \alpha_t = \infty } \\ &amp;\mathsf{ \displaystyle\sum^\infty_{t=1} \alpha^2_t &lt; \infty } \end{aligned} $$</center></table></div><h4 id="n-step-sarsa">n-Step Sarsa</h4><ul><li>Consider the following n-step returns for $n = 1, 2, \infty$:</ul>\[\begin{aligned} \mathsf{n=1\;\; (Sarsa)\;\;} &amp; \mathsf{q^{(1)}_t = R_{t+1} + \gamma Q(S_{t+1})} \\ \mathsf{n=2\quad \quad\quad\quad\,} &amp; \mathsf{q^{(2)}_t = R_{t+1} + \gamma R_{t+2} + \gamma ^2 Q(S_{t+2})} \\ \vdots \quad\quad\quad\quad\quad\; &amp; \;\, \vdots \\ \mathsf{n=\infty \;\, (MC)\quad} &amp; \mathsf{q^{(\infty)}_t = R_{t+1} + \gamma R_{t+2} +\dots + \gamma ^{T-1} R_T} \end{aligned}\]<ul><li>Define the n-step return</ul>\[\mathsf{q^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma ^{n-1} R_{t+n} + \gamma ^n Q(S_{t+n})}\]<ul><li>n-step Sarsa updates $\mathsf{Q(s,a)}$ towards the n-step Q-return</ul>\[\mathsf{ Q(S_t, A_t) \leftarrow Q(S_t, A_t) +\alpha \left(q^{(n)}_t - Q(S_t,A_t)\right) }\]<h4 id="forward-view-sarsalambda">Forward View Sarsa($\lambda$)</h4><ul><li>The $q^\lambda$ return combines all n-step Q-reutrns $\mathsf{q^{(n)}_t}$<li>Using weight $\mathsf{(1-\lambda)\lambda ^{n-1}}$</ul>\[\mathsf{q^\lambda_t = (1-\lambda) \displaystyle\sum^\infty_{n=1} \lambda^{n-1} q^{(n)}_t}\]<ul><li>Forward-view Sarsa($\lambda$)</ul>\[\mathsf{ Q(S_t,A_t) \leftarrow Q(S_t,A_t) +\alpha \left(q^\lambda_t - Q(S_t,A_t)\right)}\]<h4 id="backward-view-sarsalambda">Backward View Sarsa($\lambda$)</h4><ul><li>Just like TD($\lambda$), we use eligibility traces in an online algorithm<li>But Sarsa($\lambda$) has one eligibility trace for each state-action pair</ul>\[\begin{aligned} &amp;\mathsf{E_0(s,a) =0} \\ &amp;\mathsf{E_t(s,a) =\gamma \lambda E_{t-1}(s,a) + 1(S_t=s, A_t=a)} \end{aligned}\]<p>이거 $\gamma$는 알겠는데 $\lambda$는 왜 곱해진거지. 그리고 식 제발…. 나야 이해하겠지만 진짜 쉽게 갈걸 어렵게가게 만드네, 수식을 개발자마인드로 쓴건가</p><ul><li>$\mathsf{Q(s,a)}$ is updated for every state s and action a<li>In proportion to TD-error $\delta_t$ and eligibility trace $\mathsf{E_t(s,a)}$</ul>\[\begin{aligned} &amp;\delta_t = \mathsf{R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t))}\\ &amp;\mathsf{Q(s,a) \leftarrow Q(s,a) + \alpha\delta _t E_t(s,a)} \end{aligned}\]<h4 id="sarsalambda-algorithm">Sarsa($\lambda$) Algorithm</h4><div class="table-wrapper"><table><tbody><tr><td>Initialize $\mathsf{Q(s,a)}$ aribitrarily, for all $s \in S, a \in A(s)$<br />Repeat (for each episode):<br />$\quad$E(s,a) = 0, for all $s \in S, a \in A(s)$<br />$\quad$Initialize S, A<br />$\quad$Repeat (for each step of episode)<br />$\quad\quad$Take action A, observe R, S’<br />$\quad\quad$Choose A’ from S’ using policy derived from Q (e.g., $\epsilon$-greedy)<br />\(\quad\quad \delta \leftarrow R+ \gamma Q(S', A') - Q(S,A)\)<br /> \(\quad\quad E(S,A) \leftarrow E(S,A) + 1\)<br /> $ \quad\quad$ For all $s \in S, a \in A(s)$:<br />\(\quad\quad\quad Q(s,a)\leftarrow Q(s,a) + \alpha\delta E(s,a)\)<br />\(\quad\quad\quad E(s,a) \leftarrow \gamma\lambda E(s,a)\)<br />\(\quad\quad S\leftarrow S'; A \leftarrow A';\)<br />$\quad$until S is terminal</table></div><p>$\delta$는 S,A 에 대한 error 값인데 이 값을 episode 내에서 지나온 모든 value를 업데이트하는데 사용한다. Eligibility factor에 의해 감소되어 멀수록 점점 영향은 적어지고 최근에 영향을 미친 곳일수록 크게 작용하기는 한다. 이 업데이트는 때로는 방해가 되기도 하고 잘못된 방향을 강화하는 것도 가능하다. 그러나 충분한 양을 한다고 했을 때 결국 최적 값으로 수렴하긴 할것이다. 그러나 Exploration 비율과 개성되는 값에 따라서는 어딘가에 물려서 개선되지 못하는 것도 가능할 것 같다.</p><h2 id="off-policy-learning">Off-Policy Learning</h2><ul><li>Evaluate target policy $\mathsf{ \pi(a \vert s)}$ to compute $\mathsf{v_\pi(s)}$ or $\mathsf{q_\pi(s,a)}$<li>While following behaviour policy $\mathsf{\mu(a\vert s)}$</ul>\[\mathsf{ \lbrace S_1, A_1, R_2, \dots, S_T \rbrace \sim \mu }\]<p>Why is this important</p><ul><li>Learn from observing humans or other agents<li>Re-use experience generated from old policies $\pi_1, \pi_2, \dots, \pi_{t-1}$<li>Learn about optimal policy while following exploratory policy<li>Learn about multiple policies while following one policy<li>Policy가 다르면 Value가 어느정도 다르게 계산될 텐데 그래도 그 결과를 사용할수 있다라.. 매우 중요하고 도움이 될 내용이긴 하다</ul><h3 id="importance-sampling">Importance Sampling</h3><ul><li>Estimate the expectation of a different distribution</ul>\[\begin{aligned} \mathsf{ \mathbb{E}_{X \sim P}[f(X)] } &amp;= \sum \mathsf{P(X)f(X)} \\ &amp;= \sum \mathsf{Q(X) \frac{P(X)}{Q(X)} f(X) } \\ &amp;= \mathsf{ \mathbb{E}_{X \sim Q} \left[ \frac{P(X)}{Q(X)}f(X)\right]} \end{aligned}\]<p>plicy가 정하는 확률에 대해서 Environment에서 Action reward 는 고정이고 policy에 의한 확률만 고정이므로 이 확률 비율만 보정해주면 return을 다른 policy에 대해서도 구할 수 있다는 뜻인데 이는 기본적으로 같은 Action을 따라 갈 때만 가능하다. $\epsilon$-greedy 등으로 exploration을 포함하는 policy를 통해 충분히 많은 action 가짓 수를 확보하고 모든 episode 데이터가 있을 때에만 return을 계산할 수 있으므로 deterministic policy의 data는 학습이 어렵다고 볼 수 있다.</p><h4 id="importance-sampling-for-off-policy-monte-carlo">Importance Sampling for Off-Policy Monte-Carlo</h4><ul><li>Use returns generated from $\mu$ to evaluate $\pi$<li>Weight return $\mathsf{G_t}$ according to similarity between policies<li>Multiply importance sampling corrections along whole episode</ul>\[\mathsf{ G^{\pi/\mu}_t = \frac{\pi(A_t \vert S_t)}{\mu(A_t \vert S_t)} \frac{\pi(A_{t+1} \vert S_{t+1})}{\mu(A_{t+1} \vert S_{t+1})} \dots \frac{\pi(A_T \vert S_T)}{\mu(A_T \vert S_T)} G_t }\]<ul><li>Update value towards corrected return</ul>\[\mathsf{ V(S_t) \leftarrow V(S_T) + \alpha\left(G^{\pi/\mu}_t - V(S_t)\right) }\]<ul><li>Cannot use if $\mu$ is zero when $\pi$ is non-zero<li>Importance sampling can dramatically increase variance</ul><h4 id="importance-sampling-for-off-pollicy-td">Importance Sampling for Off-Pollicy TD</h4><ul><li>Use TD targets generated from $\mu$ to evaluate $\pi$<li>Weight TD target $\mathsf{R+\gamma V(S’)}$ by importance sampling<li>Only need a single importance sampling correction</ul>\[\mathsf{ V(S_t) \leftarrow V(S_t) + \alpha \left( \frac{\pi(A_t\vert S_t)}{\mu(A_t\vert S_t)} (R_{t+1} + \gamma V(S_{t+1})) - V(S_t)\right) }\]<ul><li>Much lower variance than Monte-Carlo importance sampling<li>Policies only need to be similar over a single step</ul><p>즉 $\mu$ 가 0이거나 너무 작으면 사용하기 힘들다고 하는데 이건 내용을 잘못 파악한것 같다. 물론 episode의 다양성이 커질수록 연산이 힘들어지는 문제가 있다고 볼 수도 있지만 조금만 더 접근하면 충분히 가능할것 같은데. TD를 통해서 solution을 구하는 방법과도 엄청나게 다르지 않을것같은데. 제약조건만 만족하면 상상속에서나 가능한 건 아닐듯</p><h3 id="q-learning">Q-Learning</h3><ul><li>We now consider off-policy learning of action-values $\mathsf{Q(s,a)}$<li>No importance sampling is required<li>Next action ischosen using begaviour policy $\mathsf{ A_{t+1} \sim \mu(\cdot \vert S_t) }$<li>But we consider alternative successor action $\mathsf{ A’ \sim \pi(\cdot \vert S_t) }$<li>And update $\mathsf{Q(S_t,A_t)}$ towards value of alternative action</ul>\[\mathsf{ Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \alpha \left(R_{t+1} + \gamma Q(S_{t+1},A') - Q(S_t,A_t)\right) }\]<p>Error를 다른 action 결과값에서 가져와서 update 해도 된다는건데 그럼 결국 그 다른 policy에만 가까워지는거 아닌가 학습하는 policy가 아니라는 말은 그 결과를 가져오는 target policy는 개선이 안된다는거 같은데 그게 optimal하다고 가정하는건가 그럼 의미가 없는데</p><h4 id="off-policy-control-with-q-learning">Off-Policy Control with Q-Learning</h4><ul><li>We now allow both behaviour and target policies to improve<li>The target policy $\pi$ is greedy w.r.t. $\mathsf{Q(s,a)}$</ul>\[\mathsf{ \pi (S_{t+1}) = \underset{a'}{argmax}\;Q(S_{t+1}, a') }\]<ul><li>The behaviour policy $\mu$ is e.g. $\epsilon$-greedy w.r.t. $\mathsf{Q(s,a)}$<li>The Q-learning target then simplifies:</ul>\[\begin{aligned} &amp; \mathsf{ R_{t+1} + \gamma Q(S_{t+1}, A') } \\ =&amp; \mathsf{ R_{t+1} + \gamma Q(S_{t+1}, \underset{a'}{argmax}\; Q(S_{t+1}, a')) } \\ =&amp; \mathsf{ R_{t+1} + \underset{a'}{max}\;\gamma Q(S_{t+1}, a') } \\ \end{aligned}\]<p>target은 greedy, behaviour은 $\epsilon$-greedy라서 target policy도 결국 개선되긴 한다는 말이네 그니까 결국 exploration을 유지하되 update에 사용되는 return은 greedy로 유지해서 error term이 잘못되지 않도록 한다는 말이군. 그럼 이미 축적된 학습 Data는 Behaviour로 사용되는건가</p><h4 id="q-learning-control-algorithm">Q-Learning Control Algorithm</h4>\[\mathsf{ Q(S,A) \leftarrow Q(S,A) + \alpha \left( R + \gamma\; \underset{a'}{max}\; Q(S',a') - Q(S,A)\right) }\]<div class="table-wrapper"><table><thead><tr><th>Theorem<tbody><tr><td>Q-learning control converges to the optimal action-value function, $\mathsf{Q(s,a)\rightarrow q_*(s,a)}$</table></div><p>Q-learning을 Sarsa max라고도 부른다고 함 결국 action value를 improve하는 방법에서 behaviour는 exploration을 포함하고 update는 greedy로 한다는 뜻</p><h4 id="q-learning-algorithm-for-off-policy-control">Q-Learning Algorithm for Off-Policy Control</h4><div class="table-wrapper"><table><tbody><tr><td>Initialize $\mathsf{Q(s,a) \forall s \in S, a \in A(s)}$, arbitrarily, and $\mathsf{Q(terminal\, state,\cdot)=0}$<br />Repeat (for each episode):<br />$\quad$Initialize S<br />$\quad$Repeat (for each step of episode)<br />$\quad\quad$Choose A from S using policy derived from Q (e.g., $\epsilon$-greedy)<br />$\quad\quad$Take action A, observe R, S’<br />\(\quad\quad Q(S,A)\leftarrow Q(S,A) + \alpha [ R + \gamma\; max_a Q(S',a)-Q(S,A)]\)<br />\(\quad\quad S\leftarrow S';\)<br />$\quad$until S is terminal</table></div><h2 id="summary">Summary</h2><div class="table-wrapper"><table><thead><tr><th> <th>Full Backup (DP)<th>Sample Backup (TD)<tbody><tr><td>Bellman Expectation<br /> Equation for $\mathsf{v_\pi(s)}$<td>Iterative Policy Evaluation<br />\(\mathsf{ V(s) \leftarrow \mathbb{E} [R+\gamma V(S') \vert s] }\)<td>TD Learning <br /> \(\mathsf{ V(S)\;\overset{\alpha}{\leftarrow}\; R+\gamma V(S')}\)<tr><td>Bellman Expectation<br /> Equation for $\mathsf{q_\pi(s,a)}$<td>Q-Policy Iteration<br />\(\mathsf{ Q(s,a) \leftarrow \mathbb{E} [R+\gamma Q(S',A') \vert s,a] }\)<td>Sarsa <br /> \(\mathsf{ Q(S,A)\;\overset{\alpha}{\leftarrow}\; R+\gamma Q(S',A')}\)<tr><td>Bellman Optimality<br /> Equation for $\mathsf{q_*(s,a)}$<td>Q-Value Iteration<br />\(\mathsf{ Q(s,a) \leftarrow \mathbb{E} \left[ R+\gamma \;\underset{a'\in\mathcal{A}}{max}\;Q(S',a') \vert s,a \right] }\)<td>Q-Learning <br /> \(\mathsf{ Q(S,A)\;\overset{\alpha}{\leftarrow}\; R+\gamma \;\underset{a'\in\mathcal{A}}{max}\; Q(S',a')}\)</table></div><p>where \(\mathsf{ x \overset{\alpha}{\leftarrow} y \equiv x \leftarrow x + \alpha(y-x) }\)</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rlcourse/'>RLcourse</a>, <a href='/categories/note/'>Note</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >reinforcementlearning</a> <a href="/tags/lecturenote/" class="post-tag no-text-decoration" >lecturenote</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RLcourse note - Lecture 5 Model-Control - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-5/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RLcourse note - Lecture 5 Model-Control - DS's Study Note&u=http://mathg0811.github.io/posts/RL-course-Note-5/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=RLcourse note - Lecture 5 Model-Control - DS's Study Note&url=http://mathg0811.github.io/posts/RL-course-Note-5/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Review-Alphago-Zero/">Review - Article Alphago Zero</a><li><a href="/posts/Movenet-Test/">ML Model Test - Movenet Multiperson</a><li><a href="/posts/RL-course-Note-9/">RLcourse note - Lecture 9 Exploration and Exploitation</a><li><a href="/posts/RL-course-Note-8/">RLcourse note - Lecture 8 Integrating Learning and Planning</a><li><a href="/posts/RL-course-Note-7/">RLcourse note - Lecture 7 Policy Gradient</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-course-Note-3/"><div class="card-body"> <span class="timeago small" >Dec 2, 2021<i class="unloaded">2021-12-02T10:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 3 Planning by Dynamic Programming</h3><div class="text-muted small"><p> Video Link : Introduction of Dynamic Programming Dynamic sequential or temporal component to the problem Programming optimising a “program”, i.e. a policy A method for solving complex prob...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-4/"><div class="card-body"> <span class="timeago small" >Dec 9, 2021<i class="unloaded">2021-12-09T21:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 4 Model-Free Prediction</h3><div class="text-muted small"><p> Video Link : Introduction of Model-Free Prediction Estimate the Value function of an unknown MDP 4강 소감 Model Free Prediction의 시작으로 TD에 대해서 정리했다. 결국 MC나 그 외 여러가지 Prediction 방법 중 많이 쓰이게 되고 범용적으...</p></div></div></a></div><div class="card"> <a href="/posts/RL-course-Note-6/"><div class="card-body"> <span class="timeago small" >Dec 20, 2021<i class="unloaded">2021-12-20T17:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RLcourse note - Lecture 6 Value Function Approximation</h3><div class="text-muted small"><p> Video Link : Introduction of Value Function Approximation Reinforcement learning can be used to solve large problems, e.g. 6강 소감 6강은 어느정도 이해했지만 좀 찜찜한 부분들도 있다 슬슬 실전에 사용되는 내용에 많아지는 만큼 세심한 수식들이 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/RL-course-Note-4/" class="btn btn-outline-primary" prompt="Older"><p>RLcourse note - Lecture 4 Model-Free Prediction</p></a> <a href="/posts/RL-course-Note-6/" class="btn btn-outline-primary" prompt="Newer"><p>RLcourse note - Lecture 6 Value Function Approximation</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/mathg0811">Daeseong Jung</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcementlearning/">reinforcementlearning</a> <a class="post-tag" href="/tags/lecturenote/">lecturenote</a> <a class="post-tag" href="/tags/rl-test/">rl test</a> <a class="post-tag" href="/tags/alphago/">alphago</a> <a class="post-tag" href="/tags/deepfake/">deepfake</a> <a class="post-tag" href="/tags/movenet/">movenet</a> <a class="post-tag" href="/tags/review/">review</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=jds.illusory"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'jds.illusory'); }); </script>
